{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Interview for DataScience - 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrankGangWang/AppliedML_Python_Coursera/blob/master/Pipelined_Interview_DataScience_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqlyZk0Z-kf0"
      },
      "source": [
        "**Interview questions to test data science skills**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aR3yaVg-6FC"
      },
      "source": [
        "# Let's load the data from public GitHub account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMU3MFw0-ZRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5407093-35b6-49db-aea7-4671552c0779"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load data\n",
        "df = pd.read_pickle(\"https://github.com/manjiler/interview_for_datascience/raw/master/interview_storage.pkl\")\n",
        "\n",
        "# add sorted datetime index\n",
        "df['timestamp_seconds'] = df.pop('timestamp')/1000\n",
        "df['Datetime'] = pd.to_datetime(df['timestamp_seconds'], unit='s', origin='unix')\n",
        "               \n",
        "df.set_index(df['Datetime'], inplace=True)\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "df.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['systemId', 'model_type', 'cpu_utilization', 'read_cache_miss',\n",
              "       'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput',\n",
              "       'write_throughput', 'read_iosz', 'write_iosz', 'timestamp_seconds',\n",
              "       'Datetime'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy_BPP7hdQOd"
      },
      "source": [
        "# RUN to add some artificial features maybe useful for classification\n",
        "# add sin/cos for day/hour\n",
        "# To help finding time periodicity (hourly/daily/weekly/monthly/yearly), \n",
        "#    add additional features like \"time of day/week\" by appling cos and sin to timestamp_seconds \n",
        "# time constants in seconds\n",
        "import numpy as np\n",
        "hour = 60*60\n",
        "day = 24*hour\n",
        "week = 7*day\n",
        "\n",
        "df['Hour_Sin'] = np.sin(df['timestamp_seconds'] * (2 * np.pi / hour))\n",
        "df['Hour_Cos'] = np.cos(df['timestamp_seconds'] * (2 * np.pi / hour))\n",
        "df['Day_Sin'] = np.sin(df['timestamp_seconds'] * (2 * np.pi / day))\n",
        "df['Day_Cos'] = np.cos(df['timestamp_seconds'] * (2 * np.pi / day))\n",
        "week_offset = 3*day # as epoch 0 is Thursday 1970-01-01, and leap seconds 23:59:60\n",
        "df['Week_Sin'] = np.sin((df['timestamp_seconds']+week_offset) * (2 * np.pi / week))\n",
        "df['Week_Cos'] = np.cos((df['timestamp_seconds']+week_offset) * (2 * np.pi / week))\n",
        "\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "print(df.shape, df.index.shape, df.columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ARXTz9ceI7",
        "outputId": "a80a783b-3fe1-4a6c-d6b8-cf331bf3655e"
      },
      "source": [
        "# pipeline of preprocessing, model\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "y_column = ['cpu_utilization']\n",
        "X_columns = list(df.columns.values)\n",
        "X_columns.remove(y_column[0])\n",
        "print(type(X_columns), X_columns)\n",
        "\n",
        "SYSTEM_ID_USED = 'sys1'\n",
        "MODEL_SINGLE_SYSTEM = True\n",
        "\n",
        "if MODEL_SINGLE_SYSTEM:\n",
        "  dftmp = df[df['systemId']==SYSTEM_ID_USED]\n",
        "  y = dftmp[y_column].copy()\n",
        "  X = dftmp[X_columns].copy()\n",
        "else: \n",
        "  y = df[y_column].copy()\n",
        "  X = df[X_columns].copy()\n",
        "\n",
        "\n",
        "print(f'X.shape = {X.shape}, y.shape={y.shape}')\n",
        "print(f'X.columns={X.columns}')\n",
        "print(f'X.head =\\n{X.head()}')\n",
        "print(f'y.head =\\n{y.head()}')\n",
        "print(X.describe().T)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> ['systemId', 'model_type', 'read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput', 'write_throughput', 'read_iosz', 'write_iosz', 'timestamp_seconds', 'Datetime', 'Hour_Sin', 'Hour_Cos', 'Day_Sin', 'Day_Cos', 'Week_Sin', 'Week_Cos']\n",
            "X.shape = (7926, 18), y.shape=(7926, 1)\n",
            "X.columns=Index(['systemId', 'model_type', 'read_cache_miss', 'write_cache_miss',\n",
            "       'read_iops', 'write_iops', 'read_throughput', 'write_throughput',\n",
            "       'read_iosz', 'write_iosz', 'timestamp_seconds', 'Datetime', 'Hour_Sin',\n",
            "       'Hour_Cos', 'Day_Sin', 'Day_Cos', 'Week_Sin', 'Week_Cos'],\n",
            "      dtype='object')\n",
            "X.head =\n",
            "                    systemId model_type  ...  Week_Sin  Week_Cos\n",
            "Datetime                                 ...                    \n",
            "2019-11-01 04:25:00     sys1          A  ... -0.576127 -0.817360\n",
            "2019-11-01 04:30:00     sys1          A  ... -0.578671 -0.815561\n",
            "2019-11-01 04:35:00     sys1          A  ... -0.581210 -0.813753\n",
            "2019-11-01 04:40:00     sys1          A  ... -0.583744 -0.811938\n",
            "2019-11-01 04:45:00     sys1          A  ... -0.586271 -0.810115\n",
            "\n",
            "[5 rows x 18 columns]\n",
            "y.head =\n",
            "                     cpu_utilization\n",
            "Datetime                            \n",
            "2019-11-01 04:25:00        23.083557\n",
            "2019-11-01 04:30:00        18.258795\n",
            "2019-11-01 04:35:00        17.899469\n",
            "2019-11-01 04:40:00        20.296371\n",
            "2019-11-01 04:45:00        18.300312\n",
            "                    count          mean  ...           75%           max\n",
            "read_cache_miss    7926.0  4.245299e+01  ...  5.017609e+01  7.910313e+01\n",
            "write_cache_miss   7926.0  7.092074e+01  ...  7.776491e+01  9.765615e+01\n",
            "read_iops          7926.0  2.667149e+04  ...  3.177174e+04  6.453898e+04\n",
            "write_iops         7926.0  1.797165e+04  ...  2.227595e+04  4.238536e+04\n",
            "read_throughput    7926.0  1.128908e+06  ...  1.338059e+06  3.673570e+06\n",
            "write_throughput   7926.0  3.464799e+05  ...  4.323913e+05  3.154118e+06\n",
            "read_iosz          7926.0  4.436458e+01  ...  5.077839e+01  1.667199e+02\n",
            "write_iosz         7926.0  2.001211e+01  ...  2.270673e+01  1.584438e+02\n",
            "timestamp_seconds  7926.0  1.573836e+09  ...  1.574463e+09  1.575158e+09\n",
            "Hour_Sin           7926.0 -3.615980e-04  ...  7.745191e-01  1.000000e+00\n",
            "Hour_Cos           7926.0 -1.554442e-04  ...  5.000000e-01  1.000000e+00\n",
            "Day_Sin            7926.0 -1.170462e-02  ...  7.071068e-01  1.000000e+00\n",
            "Day_Cos            7926.0 -1.368651e-02  ...  6.915131e-01  1.000000e+00\n",
            "Week_Sin           7926.0 -1.097173e-01  ...  5.981992e-01  1.000000e+00\n",
            "Week_Cos           7926.0  1.138736e-02  ...  6.847288e-01  1.000000e+00\n",
            "\n",
            "[15 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RHAd0ydbKmG",
        "outputId": "63ac770d-b3ea-4d7a-bd26-77a3dd52dab7"
      },
      "source": [
        "numeric_features = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', \n",
        "                            'read_throughput', 'write_throughput', 'read_iosz', 'write_iosz']\n",
        "\n",
        "categorical_features = ['systemId', 'model_type']\n",
        "\n",
        "time_features = [ 'timestamp_seconds', 'Hour_Sin', 'Hour_Cos', 'Day_Sin', 'Day_Cos', 'Week_Sin', 'Week_Cos']\n",
        "\n",
        "numeric_transformer = Pipeline(\n",
        "  steps = [\n",
        "    ('power_transformer', PowerTransformer(method='box-cox', standardize=False)), \n",
        "    ('scaler', StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "time_transformer = Pipeline(\n",
        "  steps = [\n",
        "    ('scaler', StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numeric_transformer', numeric_transformer, numeric_features),\n",
        "        ('time_transformer', time_transformer, time_features),\n",
        "        ('categorical_transformer', categorical_transformer, categorical_features)],\n",
        "        remainder='drop', verbose=False\n",
        ")\n",
        "\n",
        "# Append regressor to preprocessing pipeline.\n",
        "# Now we have a full prediction pipeline.\n",
        "linear_regressor = Pipeline(\n",
        "    steps=[\n",
        "      ('preprocessor', preprocessor),\n",
        "      #('ridge_regressor', Ridge())\n",
        "      #('lasso_regressor', Lasso(alpha=0.01))\n",
        "      ('linear_regressor', LinearRegression())\n",
        "    ]\n",
        ")\n",
        "\n",
        "y_transformer = Pipeline(\n",
        "  steps = [\n",
        "    ('power_transformer', PowerTransformer(method='box-cox', standardize=False)), \n",
        "    ('scaler', StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    shuffle=False, random_state=0)\n",
        "\n",
        "# MUST only train with y_train then transform\n",
        "y_train_transformed = y_transformer.fit_transform(y_train)\n",
        "y_test_transformed = y_transformer.transform(y_test)\n",
        "\n",
        "\n",
        "linear_regressor.fit(X_train, y_train_transformed)\n",
        "\n",
        "print(\"model score: %.3f\" % linear_regressor.score(X_test, y_test_transformed))\n",
        "print(f'type(X)={type(X)}, type(y)={type(y)}, type(X_train)={type(X_train)}, type(y_train)={type(y_train)}' )\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model score: 0.927\n",
            "type(X)=<class 'pandas.core.frame.DataFrame'>, type(y)=<class 'pandas.core.frame.DataFrame'>, type(X_train)=<class 'pandas.core.frame.DataFrame'>, type(y_train)=<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaYvVK_Dnbml"
      },
      "source": [
        "1. w/o power transformer: model score: 0.934\n",
        "2. w/ power transformer: model scroe: 0.931\n",
        "3. w/ power transformer and time_transformer: model scroe: 0.933\n",
        "4. w/ target y transformed,: model score: 0.970\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "b6YtzmPfjacY",
        "outputId": "dcfce636-64db-417a-b2e6-12e7ec2df9ad"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_predicted = linear_regressor.predict(X_test)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(y_test_transformed, y_predicted, '+')\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6, 6)\n",
        "plt.grid()\n",
        "\n",
        "y_predicted_inversed = y_transformer.inverse_transform(y_predicted)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(y_test, y_predicted_inversed, '+')\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6, 6)\n",
        "plt.grid()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAFlCAYAAAD292MqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2df4xdaXnfv++69m5qE+2OQQ7ZXXXGzngTtEEgW4ytVtW4w1AnRnGdEm1QlMqqih2pqFOrUWKKVEorhCvUuKOmf5gI5EZFsasQy2mGGoy7Uxq0nmJTmgDLenY9jmCJSuJhA3YAL+zbPzzP9XOf+77nxz3n3vPr+5GsuT/OPec9a+/3PvN9nx/Oew9CCCHN5aGqF0AIIaQYFHJCCGk4FHJCCGk4FHJCCGk4FHJCCGk4FHJCCGk4f6OKi77+9a/3k5OTmY+/e/cutm7dOroFjQHeQz3gPdSHNtzHuO/h+vXrf+m9f4N9vRIhn5ycxLVr1zIfv7y8jNnZ2dEtaAzwHuoB76E+tOE+xn0Pzrk/C71Oa4UQQhoOhZwQQhoOhZwQQhoOhZwQQhoOhZwQQhoOhZwQQhoOhZwQQhoOhZwQQhoOhZwQQhoOhZwQ0hpOX75R9RIqgUJOCGkNi1dWq15CJVDICSGk4VTSNIsQQsri9OUbfZH45MklAMDC3DROzO+ualljhUJOCGk0J+Z39wR78uQSbp06VPGKxg+tFUIIaTgUckJIa1iYm656CZVAISeEtIaueOIWCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhhDQcCjkhZCx0dTDyOKCQE0LGQlcHI48DCjkhhDQczuwkhIyMpMHIb91c1araB4WcEDIykgYjLy9/s6pltQ5aK4QQ0nAo5ISQsdDVwcjjgEJOCBkLXR2MPA4o5IQQ0nAKC7lz7knn3LPOua86577inFsoY2GEEEKyUUZE/kMA/8J7/yYA+wD8U+fcm0o4LyGkJgxblclqzvFQWMi993/uvf/ixuPvAngewONFz0sIqQ/DVmWymnM8OO99eSdzbhLA5wA87b3/jnnvGIBjALBjx449586dy3zeO3fuYNu2baWtswp4D/WA9zAcRy/dxdmDW0v9HP8u8nPgwIHr3vu99vXShNw5tw3A/wTwIe/9HyQdu3fvXn/t2rXM515eXsbs7GyxBVYM76Ee8B6yY6syhYW56cQMlKyf499FfpxzQSEvpbLTObcZwCcBfCJNxAkhzSCpKnMUnyPDU0bWigPwMQDPe+9/q/iSCCFthBufo6OMrJW/DeBXAfw959yXNv78fAnnJYTUBF2VmUeQ9ee48Tk6ysha+WPvvfPev9l7/5aNP58qY3GEkOKUEQlrbzuPILOaczywspOQllNlJHz68g1Mnlzqta+Vx/LlQrulHNjGlhCSSlJf8aSoO62N7eKVVUbtJUAhJ6SFDCu8MUKCfPryjdznGuYzJB0KOSEtZBwpgDaaThPphbnp3pdLmV8yhB45ISQnsb7iaV68iPSJ+d24depQr+Lz1qlDuHXqEEW8AIzICWk5ZQ50EMtGRFtH02mfEeQzh3dxaGdZUMgJaTllRrrWshFCwq4j8JDNs7y8jMnJydLW1mUo5IR0mDI2H0WYY158UkS++B7aKWVAISekwwyT/hcT5hhJETkpBwo5ISQXMWFmcU91UMgJ6Rhl55gLJ+Z3p1o1M1MTQ5+fxKGQE9IxRpljnmbVrKytl3Yt8gAKOSEkF0kRPakGFgQR0gFi/vUw4isFPRLJyzl0CqJtjBVqnHVh9V7ua5MwjMgJ6QAxy6OMHPM0q4ZZK6OHETkhZGhop9QDCjkhDSRLql9aL/Bhz6/fs02z0oSdWSujgUJOSAOxDapCwmu97DzNqfT59bntRqf9TNq5mbUyGijkhLSAUU4B0ufm3M16ws1OQhrCMGl/Ek2nWR5yXKz0PnbtmamJvig7VFyU1GtldjZxWSQjFHJCGoJkfywvL+Popbu9QQ228+DM1ATOH98P4IEwpxX9LF5Z7bNdbP+UUCRuK0FjxUXMWhk9FHJCGkpMIJOaWIVK6GMbm6GuhnLuUUwcIsNDISekgaRZJVbM9fM0y2Nhbjrx/PKe/VLIkorIdMXRzC2lkBPSQKwQzExNJEbioWhdIvq0KFuLr7ZTbJZKFnHiOLfhWgenQSEnpAGkRXHiiWtR1hG3FvCY4E+eXOoJtb5eXrEm44dCTkiNEUEdJopbvLLai6aTSui1gOvPhq73zJnnUrNUyCCjah0sUMgJqTFZBDxUpCNCkVUksh63sraOW6cOcdMzJ6NsHQxQyAmpLZJNokvsgcH869hAZP1cIj+72Xj68o1e2fyoo0YyOijkhNSMWBm8CKrOvw5550mpgvZYfZ2kEW4xjz1ky5BkRpG5wxJ9QmpGqEeKvA6gr4+3FfxRiERsPXl6t5AHjOK/FyNyQhqAFuiLL72KxchxknGSZJEkeeoxC4bUGwo5ITXGZp0IIatDRDjvoAcAfTZK6Hp2PRT6ekFrhZAao6swdW9xjbVeshAry0/rbhjKLSfVQyEnpAHEeovHSOp6KNaKjepD4p5lCAWpHgo5ITUlTURD6YkiyBJZJ83pzDI8uUj/cX4JjA8KOSE1JSaih3dtBvBAkEWIs0TpdvSbJk82ShaR5hCK8cHNTkJKZBSd7UJoERbBLKt8Pkth0CgaP5HhoZATUiJFBS6LiB6Z3oKLL73aVyo/MzWBfTu34/zx/dHyeZutEhtMoT+bt5yc1aHVQCEnpEbk6cmhhXdlbR0ra+sDIhoSUJ1iaPPK06yZNJEedU8REoZCTkhBxhWFZtl8lCg76dq6K6I8TzqfQJGuLxRyQgoyLoGLzezUZN1g1PnpmlBx0bCwaGh8UMgJaRCnL9/AW+8nrfRtPKYR2wi1xCpJY8clQU98fDD9kJAS0QJno90sKXuhFEFdrCOinSaki1dWe585fflGr4+4TTEMNebK8sVAka4XjMgJKZGkKTtpGS2SupjFprFDj6346s9ltVtohTQXCjkhY8TmmctzsT5CQm83OY9eugtcWgoKuBCyTnSqomZhbrp3DZuKyLTBZkAhJ6REYhksgoilCKQcK/61FnoRXDvt/uzBrZidnU20amIiHxJm2wiLGSnNoxQhd859HMA7AXzLe/90GeckpIkkTdmxudrW95bHIvR6c1Lz4ZXv4eil+GalWDixVrWkfZQVkZ8F8NsAfrek8xHSGtKi9DyfBYAXvv0aZqYmokIf8rrz+N/0yptHKVkr3vvPAQj/qyKkg5y+fGMglU83t8qSdaJ/6s8CiIq4rtYUYj53zJqhJ948nPe+nBM5Nwngj2LWinPuGIBjALBjx449586dy3zuO3fuYNu2bSWssjp4D/VgXPdw9NJdHN61GRdfejX3Z88e3No7hzy+sHov8VxyrdBnk9aYdswo4b+n/Bw4cOC6936vfX1sm53e+48C+CgA7N2718/Ozmb+7PLyMvIcX0d4D/VglPfQl5FyaQmL73kHFtHvUetjdOMq2dhcWVtXG5mr+D+v/mRw09JaKyLyRy/d7b2Wep+Xlir9++S/p/Jg1gohJaHT94CwFx7KGLl683Zf10I5l7wvm5/63CLiM1MTOH98f/B6oRRCdidsJxRyQiIM01tct3+1xLoRWr87NFhZn1+/JiIeun4oS4WNr9pJWemHvwdgFsDrnXPfAPAB7/3Hyjg3IVWRpbd4UkZKlp7edlxb0lpCx+mImtkm3aWsrJV3e+/f6L3f7L1/giJOmkDWYcNJhTd2KPLC3HRiVortpRLyv7Xox3qj2OPFokkjrZcLaSa0VkhnCUXcST53FpvFFuPoNMRQL5VQBaaOsoXTl2/g6s3bwbRDOe/K2nrQluHgh/bD7oeEGGJdATWnL9/o/RFCUbj+skhqXiXXefzRRwbEVcQ+lju+eGU1aOlkGaJM2gEjctIpLqze6ytvT2oupR/b6DZpoEMsog4hn5P3X37l+322i53mozc77cZn0j2FoKfeHijkpFMcmd6Cxfe8A0DYWgjlfKdZENaukBzv0AZlSOQB9HLJQ1F3zOrR3QyTLJekdZN2QCEnJELITgnZI1acZYiDvKcjZzu5XhOzTtLYt3N7Xx46fe/uQSEnncVaH2n2SKijIDAonNYusYU+cs4sAx9sXrreHLXXTWqkRdoNNztJ68nSHCqURgj0528npelJWp8+3gqubGRevXk789QeQa/n8K7NwWPOH98/MDSCdANG5KT1ZCnssSSl6aVNnrd2ivzUG5mxjUpL6JiLL73aE2zrgzMi7yaMyAkxJGVziHCGUhQ1MZF+3cObEt8PEYqyV9bWo5496R6MyEkriXneh3dtRlqzOhu9a2FPiu6ziOp3f/Cj1GM0+kvCjnqTaw7TCGuYPjKkvjAiJ60kFDXfOnUIR6a3DHWuEM+ceQ7A4GbmzNREauaI9eKzIMdeWL0XLPOX96VNQJJQM5JvF4zICVGEItW8o9psqXyIyZNLeMilC2ooun7r5m/25cIDD74YJLWR0Xa3YEROWk+eqLfMSDUtg+Q1H+5aGPpNwk66j5Hm7xdpmMXGWvWFETlpPUWj06s3b/dlpGRFMkhCjbH0ufft3N73WpZrpP2WMIqGWYz06wuFnHSemCjK9J2VtfVMaX1pJfahlEM5t/6sCKyOgJ8581zfEIkkUWZXw+5BISedJyaK1nZIs11CIq6j8VikbSP2UFVp2fnhWe0mjoZrBvTISSdJ83slIyWPZ37r1KEBXzzL50PH2DmbSVhRziLSWUU4lv1DEa8XFHLSSbR4aqGcmZrA5MmlaAScZFnoz4mgZ7E4Qv3HT8zvxjNnnhso+588uYQPr3yv7/jQQGfSLSjkpPNoUd+3c3tiRJt1s1MEXR8fO689pxT5nD++f2Ds261Th/C+mR9LvPaoskvYv7y+UMhJZzh9+QaOXro7kH4n7wEPMjPK2izU6YX6CyOLKOr1yfMsIj2qYh9G+vWFQk5aSUjwTszvxtmDW4MibcelAfkj0FDeeKz3eJrY2mpNqRalmJIQzFohrUIqM0UIk4TP9voWdPqh+NVZGlSlZa3Y1/X6bFdE/WWzeGW1L/UwBLNLug2FnLQKO+w4JmILc9N45sxzQfGV/PHJk0sDxTrDrCft9SLVmvocRYp9SLOhtUJaQ2gij/aVtd1yYn53X+Ul8CBC1+KeJbIHksvxF+amB97XKX2hOZx2rYQkwYicNJ4022Pxyip+//rX8fIr3w+2sdVWjBASV4uIc1Ll58LcNK7evD3w/uTJpZ7toUv/9dDnYWF2SfdgRE4aT9KgB3n95Ve+D+D+dJ1QNoj+mYd9O7cnRuMS+YfEVdIMQ69rQr9RJMEIvntQyEkrEeGUoprY+/qx/QJIY2VtHYtXVhPL5+Xa0nhLn182QtOifxF29hAnMWitkFahs0F+//rXoyKrRTHUC2WY6DyJkP1iNyg1oX4rhMRgRE5ayenLN/D5k3MAskXXAAYi5iIMI8DaHrJFRNr+OXrpLnuDkz4YkZNWoW2ItK6Dsc/m+YxGb37mnfxjhT+UTig/l5eXMTtLH5w8gBE5aR0igDqqTpvWUwbnj+/vFe7YjVcb4SdN/qGdQvLCiJzUjqzpd3JcrKpRR9VF+3lv2eRw70c+8ZiyfHV773qoMiEhGJGT2hGzJawvrIt1Qv5yiO2PhF9/3cObEtcUE/FYpG+/TGzZfx6yzusk3YVCThpDmu+sOxgCg0OMAeDfz24FMLix+acfPAggvwWjI/3QAAZ7/fPH91OQSenQWiG1IGvTp6TjbJtYa9HcF+kfYGZqInVjM2lgcgyp1rT3pa/BZlZkFFDISS2INX06fflGn9Bacb116hCeOfPcwOshET5/fD8WfuczWFm7G12HRNh5RVyGJ9tMGV1sxGZWZFTQWiG1xg550HYFcF/oYx0Mgf487LKLfDQra+uZ8sCZ/01GASNyUjvyjFoToZSIWIhlqVx86dUSVngf/VuDjeBDv2EUbYZFSAwKOakdenp8VotDhPvxRx/pNcgaNTG/PrZhShEno4LWCqktsa6GoeZW0oRKyvJHiQi1LTyStdppPsz/JqOGQk5qTyzzwyLFQUDxXikxQkKdBiNxMmoo5KT2SC8SvZF469ShXmSsI15tdRQty7dfBjNTE32bpvYnNzJJVdAjJ41AR7Uyi1N88RPzuwfmb5aRoWLPoQdELF5ZZUohqQ2MyEmjCPnNsRTEspEmVxzwQOoGI3LSCH72A5fw3R/8qPc85JdLhAwMpiOWgU4fZCMrUicYkZPKSfKW5T0t4jGKdju0nroVaT2WzY5oS4P+ORklpQi5c+6gc+4F59yLzrmTZZyTdIckq8L2T7EpiLb5VZENTi3+uheKTS/Uj7NmpNCOIaOksJA75zYB+E8Afg7AmwC82zn3pqLnJd3kmTPP9R7/7AcuAUAwCradDuW4onaKnvkpZPmNIe01QkZJGR752wC86L2/CQDOuXMADgP4agnnJi0kJsL6/aQItgxfOtbdMDQebvHKat81bbqjjcrltawdHQkpivM+eepJ6gmcexeAg977f7Lx/FcBzHjv32uOOwbgGADs2LFjz7lz5zJf486dO9i2bVuhdVYN7+EBRy/dxdmDW4PP7eNR8dRjD+GFb7+W+XUAOLxrM45Mb+l7TdZ7YfVe7z17f7HXhmWYvwe9vrrA/yfyc+DAgeve+7329bEJuWbv3r3+2rVrma9xf9jsbKF1Vg3v4QG2TW1V/nGoL4tkvsQi9pmpCezbuT3zmiX6LjPffJi/hzrmu/P/ifw454JCXsZm58sAnlTPn9h4jZAe0ldcpw1Onlzq2RahnirA6IYmL8xN94m4HsEm4htqn7uytp7YA8b2ftG+vtwzPXRSNmUI+RcATDvnppxzWwD8MoA/LOG8pEWExE8eJ/nFdip9FuyxIU/dRtR6kzTrRPtQDxjtrdt7lrWNyx+PfXnyi6R9FN7s9N7/0Dn3XgCfBrAJwMe9918pvDLSKmwvbi148vjxRx+JvpcnZzvWszyJmamJoMCemN+NqzdvB9e0MDcdtE7qIpSxqUukfZRS2em9/xSAT5VxLtJOdHaHztqIzdwcNytr68HBD6cv3+jrdphFELPOGGXmCikLluiTSkiKmnWp/SjQUbQmJKyh9MLYOUPUJSpmK4F2QyEnIyMpGk2KwEcp4sB9cb5683b0uknRcpJg15m6r48Ug0JORkYsGq0y5fDswa2YnZ2NdkwUoY554kUEkVExGRUUcjJSQht/IoZJYj6K7oXARpHRpaW+VENZh7Y9dGVmWXZIG6JiDpCuJ+x+SEaGCKEue8+S/nbr1CF89Zt/1XsMpOeTa7G104T0+1Jdab9MsqQoEv43qSsUcjIybOQmRTLacgkJ6OTJpV7bWrE10qJz2xtFN9rS73945XsDx4eQLxvaIaQJ0FohpRPzwK242sdlIdH3M2ee65XTi4US66MSW5P8RtHldEGmUNYfRuSkdEIVjbaXeNlpeDZylhmeIkDy8/CuzcHj9TGh0vwuC1asKrfL/03qBoWclErM/w71HCmTpJJ74IH4SAfAE/O7B4ZS2N4osta6VGoSEoNCTkolZJXYDUcRVSnJL5vQl4QI8oXVe33DI7SY2+fC4pVVivkG3DOoJxRykolhhEw+E/oV/PTlG/j8ybnC68rLxZde7VtPSMxprcThf4d6ws1OkonFK6vRwQixzTD7XI9pi21yjqLnihbm0LlD4lR17xdC8kAhJ4WJVXDGGmLZDUjNKMRzmKwLRp6kSVDISRQrgLoqMknoYg2xqopyh21cRTEnTYEeOYli084O79qc6hfbaT+aqqyKUOYJNy9Jm6CQk8xcfOnVTMfJZJqqCJXzS+bJU4891HtOSFugkJNSCRXVjAN9vfPH9w/kiMtvErHKTkKaDIW8pZRhHdiZj8Bg4yv7E8BAUc2w5PkysGuUsnpNbMYmbRbSdLjZ2VKyTrZJwm4SAoPiqjNQstgVjz/6SN/0+jRkg1R+pk0P0tN/ZO2xdEf2CiFtgRE5KUweMXzXnicBZIu2JbIGHnxhnL58I7GlrW0FoK0eXVmad92E1BlG5C1ilF3qFuamcevWrcTrZEELctr1YimD8tnYhB871T52fkLaAoW8RYxy0O+J+d1Y+J1bpZ0vzYbR7WND7y3MTeP88f19XyJZ7le6HzIaJ22C1grpEdq41Ej6YWhw8aiITReSdMK8kbV0PySkTVDIW8ow1kFS6bwW9/PH92c6X5HuhtbPlvsJDX0QsmShXFi9N/SaCKkrtFZaSlnWQVY/PFR+nyc7xWKvk2Vgs2DHu2m//eJLr2Jx6FURUk8o5B0nJtRaBNNS/oBkgZ2ZmkiduRlDd0yU9cSupT3ysvcICKkzFPKOE9oglZ8i8kXL2YcVcSAsznY9aV80nDlJ2g49chJFmmZVkaqXVpVp37PHh4ZFaL+dwyJIm6CQEwD9hTZWFEXwpOHUKLCDmnVVpi7sCSG/NehNziSRZsMs0jYo5B1Hp/OJBRITQWk4VWaEbs81MzXRV52pe5kDgxPu5WfWCFvyyAlpE/TIO84w0WmZEa3NMLF+uvjfJ+Z3D4h6HuiTkzZDIe8otluhIAI3MzWBfTu3j82G0JustpOh/nn68o0+4c0q6rKpu7y8jKOX7jKjhbQKCnkHSRp+rOdtSuGP9C4ZJVawY+uyMJomhB55J0mbt2mn3Vc57UfQv0EU7R/OhlmkbVDIO4DuoWIHRSRRxJNOIhRdJ13DFgEVtXsYxZO2QSHvACJ8Np86JKijHJos6Y2hiDotvTD2OUIIPfJOEcvcEIFdWVsfqY2yb+d27Nu5vRcRi/eetYRfV5ky64SQB1DIW0qSaNtsFBFR3WHQPi4jKtfn0OIrfcXTSu31RiyzTgh5AK2VlmJtFOC+EK6srfdEVFssItahGZxlWiuhST9JGStSTk8IiUMh7yixMnyhDPG0szUlnVHK6fMMqNDrZdYJIf3QWmkxaS1qgfuiGMorL8Mrt7735Mml4ICILOgvGnrihPTDiLxl2MZRoSwV3f3v6s3bQVEtMt0nhETnbFhFSPlQyFtG1gk6Ivj7dm7HrVOHBmwQme4jrxcV9pW19YFraMQ/t54+o29C0qGQdwSxNKy1IT9j6X/y+rv2PFl4DfKlEXuPEDIc9MhbQNbOftb3TvPBdTpgWemHi1dW+/LGY18sen3MFSckGQp5CwiNa8tDTNDLKg6S1EY7tm1maqJvqHLR+yCkqxQScufcLwH41wB+BsDbvPfXylgUGQ5bwq5bvopIanEuMhQ5D7YaU7CVpDoCJ4Rkp2hE/mUAvwjgTAlrIQUQe0WLoO6xEkJEPFZRWbSiUyyRkEViI25G4IQMTyEh994/DwDOuXJWQzJjByzERHvxyiqu3rydGHnHmlFZEd/+CHD7+9nXqKtD5XlWr5ueOCHZcd774idxbhnArydZK865YwCOAcCOHTv2nDt3LvP579y5g23bthVdZqWUdQ8XVu/hyPQWHL10tzd/8uJLr6Z+7qnHHsJPT2zKdGyMn/pxjxe/k/6lnST4h3dtxpHpLb37EOzzUcF/S/WhDfcx7ns4cODAde/9Xvt6qpA75z4L4CcCb73fe39x45hlpAi5Zu/evf7atex2+vLyMmZnZzMfX0fsPdiIOitZmkvFvO88Vsmo/fOqMlHa+G+pqbThPsZ9D865oJCn5pF779/uvX868OfiaJbaDYbxnm2TKUH74rdOHer1NAldM6koRzOsiNu12KrSPBPvCSHZYPphA0iasQk8+FKYmZpITRkcdZaKzQNfmJtmFgohI6ZQZadz7ohz7hsA9gNYcs59upxltRM7ak0ep02+CbWktcxMTeD88f1jzfyw5fR6jSLeduI9p/wQUj5Fs1YuALhQ0lpaT56CF+2hZ5l6n/ReGYU92l+3EXaog6Iez6aLfSZPLtFWIaRk2GulYrKk/p2Y3x21J+Tzz5x5bmCw8uTJJTxz5rlS1qntG/2FJJuW8luDXSfnbRIyeijkFZG3L7cIpSCieWJ+N05fvtFX3KOPiW18DotUY6YJc0jQ81pKhJBscLOzIkL2QpbmV9rikAk7oc/Yx2kpi2mE+qXItW1hkqxXjtc2Eis4CSkfCnkFJAl2muBp0cwz9b6oT64tktiXkO7rkmUeJyGkHCjkFVC0y58WyjQkkh62wEdH4gu/85netUNfRLY5lu61Ivd4+vKNoYuhCCFh6JHXiCzpibKpmZbFUsb0eVsJKiX0sRFyWYp+xHIhhJQHI/KK0ZuC2pqIRer7dm5PjKx1dKzTBSWqTisu0uhMlRAx+8RG3CwIImS0MCKvmFC3Qo21UOR4GwUD6KuiDBUQJUXySYgXf2H13sBadOSvM2nsccMWQxFC0mFEXlN0emKSL63zxGPZK8NaGTbjZHl5eeCYkGiH4PQfQkYHI/IxkBZ1hqJVPekeGPSlRehjNksZQqmtkjRonxBSHYzIx8DilVWcPbg1+r6NVsXTDmWB6M8kDYwoI91vZmoC+3Zuz5RhwiwUQqqDQl5D7HxNOxJN/xwlVsQvrN5DrPVyWkphlmInQshw0FoZEdYuOXrpbm9zL4tVkXaM3eTMStZ+5HrCvWCnC+k1pvnwoZRF9iUnpBwYkY8Ia5ecPbi1N0nEdgC00ayNtkPRd9pmZmwaUNaioH07t6cek2cGJyFkdFDIa4AdUqzRmSPankjqnfK6hzclRshJVZ7a6pCIO2aJ6Of6sfRGj8GNUULKhdbKGFiYm8aF1XuJedShFrBynPyMpRdavvuDH0Xf0x0R5Xo6F9w2wLJdF5O6Nopdkhb1M4onpFwo5GPgxPxuHJne0ucRi1etM1N0tD0q9JeJFWMRaevPJ6VBpk0uIoSMHgr5CNECaCNyiVpDlZhpUTqQfdMyCVuJqQuPkn4TCG3EyrxQVm4SMn4o5CPENpyy0avN2pAqzVCUbsmyaXnr1CE8/ugj0fdDImtbACShv2RkXiizUggZPxTyMSIpiYJErY8/+ggmTy4NiBzMsUoAAA/nSURBVLPtY2IJTQzSTJ5cwsuvfH/gc9rnli8KOyou5sHbCJ4QUj0U8pKJNYe6sHovONfy1qlDeNeeJ4MWirU4hiU2R1OveWVtfSCiDtk3tnVAlusRQkYLhbxkYpuB0stbjhHEk7biOjM1EeytoomNdUsTbktsQLLYJcLZg1sz2SWM1AkZLxTyMWPtlVjPbzvkOORdW9vFWh4xWybmf8tvAHYtjLAJqTcsCBohIQGUCTm28jK0eRkqzglF4Vl/Cjbytu1q9XG6QhX4ZsLdEkKqghH5CEmyGOx7Ie9c0gG1mOo89FikLJkqST63Pi6G/qKhXUJIfaGQj4DQZmCostMiYqkFWuwO+7mVtfWohaIzVazPrZHjsvQdZz44IfWFQj4C9OahCKBsdsZE1VZcTp5c6uWVxyLvPH62jfYt+jeAUNYNByYTUl8o5CVjW7uKAMq8yzyRrfjmSSK6sraO05dv9HLRbRWmfBnIfE153aKPYwk+Ic2CQl4SEsnaqT7yWHp568n2mtAQ4zx8/uRc8PWVtfVesU/aF0KopD7WU50QUh8o5CVhuwTGiG1SJvU3SSLLNfft3B6NspNK6nUfGCB7HjkhZLww/bAEnjnzXOaBDStr670qythACD3mLTYgQoh1MgQGRd5G0tpfD41qk1RJQki9YUReAiLMaXaIfj9Wcq8jc3kOxLsdhop+kgp+bKMrWZcW7NCGJ/DA5yeE1AsKeYlcvXk7+PrrHt4EIL1UPolYxB9LZ4wJv2Sm2Nfs87Q2A4SQ+kBrRZE2Cd4eG5rYY8eoSdXkU489hE//5s/1jouNahPx1Mfp53ZCT+g8Ni88abIQJ9sT0nwYkSvyRMyhqBXoH1qsbYwXvv1apvOKpZEUUYdEVls7uvdKbJ2x92IbnoSQ+sKIvCC2B4r+MrBdDaX3+BOP/c3UtrQS1ceibftc/4Zg7ZaQN581+mZkTkj96XxEHqtkzJorLYU2oQ3GUDQrZfFpo9pCOeWxtEFZR5512uibkTchzaXzEbm2KkIdALMSirCtVRPyv2OReShzJUSoK6K9nsDIm5B20vmIvAzEa5YoO62viZAl6pdjQsONgWz54xZG34S0Cwq5oqjAia8di261EAPZNlflGDm3tlkW5qajk4OSrBtG34S0Cwq5Iq/AhSJqiZyFLLM2k75ArFeu15ilmRYhpP1QyAsgg4h1pC2Rs0TEaZPu5TwxQlkptl956DrsiUJId6CQ50TavWpCQ5JD6YNlRMgi6GlpiYSQ7tD5rJW8SLtXweaKJ0XXum/K+eP7o+Kre5/IY3medH5uYhLSTSjkQ2DTCOW1LFGxfDYU2QuhLwYRdG2h2OsTQroJhTwDtk1tSLCzWhtZxd72WgEGo3ERd0bihHSbQh65c+4jzrmvOef+xDl3wTn3aFkLqxMywHjYCT4Lc9N46rGHeo/TSLJcBGnwZTNZmKlCSPcoutl5GcDT3vs3A7gB4H3Fl1Q/QjZI1olAwP1IWppmxTzu0BeFFX0RbJ3BYjNTOAiCkO5RyFrx3n9GPb0K4F3FllNP7AanbjiVNsFHY1vc6tctVqB1i12KNSFE47z35ZzIuf8G4Lz3/r9E3j8G4BgA7NixY8+5c+cyn/vOnTvYtm1bKevMy4XVe7j40qs4vGszjkxvwdFLd3uP9TFHprf0jo2hP3f00l089dhD+OmJTcFz2edy3dD5n3rsoWCbXLvOolT591AWvIf60Ib7GPc9HDhw4Lr3fq99PVXInXOfBfATgbfe772/uHHM+wHsBfCLPsM3w969e/21a9cyLRwAlpeXMTs7m/n4MkibwxmzVbJWcoYKeWLnS8qIsR55kcZfaVTx91A2vIf60Ib7GPc9OOeCQp7qkXvv3+69fzrwR0T8KIB3AviVLCLeFGSDM0as1W1SjxM73SfpPEByZ0MArN4khAAonrVyEMBvAPgF7/1fl7Ok6rFl9zEWr6wObISeP74/MTMlVqKvxVyub71w+yUR6p3OVERCukfRPPLfBvAwgMvOOQC46r3/tcKrqhjbo1w2KUOWiAipDDUObUTK5+ScJ+Z3B/PE9YamPr9YKzL1Xq4T+o2BEToh3aNQRO69/ynv/ZPe+7ds/Gm8iAs6yhUBjRXf2FRAEViJoPUINhH+pPMwK4UQkgdWdipsip8U3GhOzO/G1Zu3g02r7LHnj+8fiNLzTKi3Hrm+Ji0UQohAIVdo+0MTyiEP5YTHhh+HjhH7xgp96LNirWi7hRYKIUSgkBuyDIXQUbX1utPSBYFBMbczQ2MtamNFSRR1QrpN54U8tkEJxLscakKVndpfj4l6rMQ+dO1bpw712T6jzBUnhDSPzgu5jYiBB+IbEmARaZ2BorHCHjpHLIoW3zvkfzPqJoTE4IQgPMjbFrRtYSf/nJjf3RNqK+pJ0b3G5o0LoS+HkKhzo5MQoul8RA48iMpjud3A4Eam7UCYJOJ24k+e6Dp0LKNzQoim8xF5qIozVC1po+CsOd8i3Id3bQZAESaElE/nhRwYHJ6se5jEUhI1kyeXcPXm7cRjjkxvKWyJcGgEISRE54RcxNBaJkJMtG3VpkVK+GNfCBdW70XPnVWgWfFJCAnROSEPWSJWSK3dYh8D/RuOVrSBwS+EpD7lFGhCSBE6udlpS98Xr6z2iWkoJTGEbGIuXlnF1Zu3gwOaZ6Ymer1ahl1rKJ2RhUCEEKETQm7FMCkCDgllqDjHnkNEXHLQde/xWDSfRaBDlZ+EEKLphJBnKYMXrJDGinNsBC5o713sllBao5xHr4kQQoahcx65RqbVy2Og39vWZfFA/2CHlbX1oPiO0u9mIRAhJETnhNyWwYd85lhGy8raeq4UwLT8dLumNOiJE0JCdMJa0dgy+KSe3yH0xmiopa310+Xx2YNbo0NaKdCEkCK0PiJPG2x8Yn73QEEQMDjVJ0ngbbYKC3cIIeOk9UIe86xjrWdjMzdDWH9dPPcT87v7/HQ7NJkQQsqksUJedtQrY920KOv39HMt2Bb5ItBReii7JQ+M8AkhSTTWI0/qgRIrorHj2WzutnQx1O/FPi/XHke0naXfCyGkuzRWyJPIUkQTe11vhia1tZXHK2vrAxuloY3To5fuApeWMlVk2rRHQghJolHWSqwHShbrIW3TUz9OElFbdak3SkM/5fHZg1sH+rHEkMETw94rIaRbNCoiH6Zc3WahyOf167GWtXkLcMoUWZbmE0Ky0ighB/LbDvpYEeqs58hzHWmgFZq7ef/xNxM/n9QcixBCkmickIsYZxG4UHMr3fHQvqYfay9bhN/+1Mgsz9DczRPzu7G8nCzkjMAJIcPSKI9ckzWiTsoBD3nZ8thG8qGfQDHfPivc+CSEJNGIiPzC6j0cvZQcNcewg5UFXWqfZLWkCfIoImnaKYSQPDQiIj8yvSU1ak7DVmHKYzvhXpBI2/ZVGUcWCSNwQkgeGiHkZRCzWZJmdFrRtz/tlwkjaUJIFTTCWtEUEcurN28HNzbt49DQiLSuiAAjaUJINTQuIi8iljIMQtssNtI+Mb+7d5zuiKg/x8ibEFInGifklmE8apt1Agx64Pq8NqWQkTchpE40zlqxpDWUSmqglXbexSurmJmayFS6TwghVdH4iDwN2bS0dohudgXENzPPH9/fl6pICCF1o5FCPkwRjkTTSZOA5NyEENIkGmmtDFuEo6Py0Dl0YVBSeX+WQiRCCBkXjRTyYbGdD2Pv25+hvuSEEFIXGmmtaGw1ZhZsNM10QkJIk2m8kIeaWxU5R4iFuWmKPSGktnTKWhkW+uGEkDrTeCFPGshAASaEdIHGCzkHMhBCuk7jPXJCCOk6rRJybkgSQrpIq4ScnjghpIsUEnLn3L91zv2Jc+5LzrnPOOd+sqyFEUIIyUbRiPwj3vs3e+/fAuCPAPyrEtZECCEkB4WE3Hv/HfV0KwBfbDmEEELy4rwvpr3OuQ8B+EcA/grAAe/9X0SOOwbgGADs2LFjz7lz5zJf486dO9i2bVuhdVYN76Ee8B7qQxvuY9z3cODAgeve+70Db3jvE/8A+CyALwf+HDbHvQ/AB9PO573Hnj17fB6effbZXMfXEd5DPeA91Ic23Me47wHANR/Q1NSCIO/92zN+WXwCwKcAfCDj8YQQQkqgaNaKTtw+DOBrxZZDCCEkL0VL9E85554C8BqAPwPwa8WXRAghJA+FhNx7/w/LWgghhJDhaFVlJyGEdJHC6YdDXdS5v8B9KyYrrwfwlyNazrjgPdQD3kN9aMN9jPse/pb3/g32xUqEPC/OuWs+lDvZIHgP9YD3UB/acB91uQdaK4QQ0nAo5IQQ0nCaIuQfrXoBJcB7qAe8h/rQhvuoxT00wiMnhBASpykROSGEkAiNEPK2DLBwzn3EOfe1jXu54Jx7tOo15cU590vOua84515zzlW+W58H59xB59wLzrkXnXMnq15PXpxzH3fOfcs59+Wq1zIszrknnXPPOue+uvHvaKHqNeXFOfeIc+5/O+f+78Y9fLDyNTXBWnHO/bjf6H3unPtnAN7kvW9cOwDn3DsA/A/v/Q+dc/8OALz3v1nxsnLhnPsZ3G/JcAbAr3vvr1W8pEw45zYBuAFgHsA3AHwBwLu991+tdGE5cM79XQB3APyu9/7pqtczDM65NwJ4o/f+i8651wG4DuAfNOzvwQHY6r2/45zbDOCPASx4769WtaZGROS+JQMsvPef8d7/cOPpVQBPVLmeYfDeP++9f6HqdQzB2wC86L2/6b2/B+Ac7jd6awze+88BWK96HUXw3v+59/6LG4+/C+B5AI9Xu6p8bHSUvbPxdPPGn0o1qRFCDtwfYOGc+zqAX0E7Rsr9YwD/vepFdIjHAXxdPf8GGiYgbcM5NwngrQBWql1Jfpxzm5xzXwLwLQCXvfeV3kNthNw591nn3JcDfw4DgPf+/d77J3G/7/l7q11tnLT72Djm/QB+iPv3Ujuy3AMhRXDObQPwSQD/3PzG3Qi89z/y92cVPwHgbc65Sq2uom1sS6MtAyzS7sM5dxTAOwHM+ZpuUOT4u2gSLwN4Uj1/YuM1MmY2fOVPAviE9/4Pql5PEbz3rzjnngVwEPcnp1VCbSLyJNoywMI5dxDAbwD4Be/9X1e9no7xBQDTzrkp59wWAL8M4A8rXlPn2Ngo/BiA5733v1X1eobBOfcGyThzzv0Y7m+gV6pJTcla+SSAvgEW3vvGRVPOuRcBPAzg9sZLV5uWfeOcOwLgPwJ4A4BXAHzJe//3q11VNpxzPw/gPwDYBODj3vsPVbykXDjnfg/ALO533Pt/AD7gvf9YpYvKiXPu7wD4XwD+FPf/fwaAf+m9/1R1q8qHc+7NAP4z7v87egjAf/Xe/5tK19QEISeEEBKnEdYKIYSQOBRyQghpOBRyQghpOBRyQghpOBRyQghpOBRyQghpOBRyQghpOBRyQghpOP8fhRbaI03CbmkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O2dZQr8jaii"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUwopRSMjaps"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3GUSHarhU1w"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(y.values, df['cpu_utilization'].values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZyfti_xtCOe"
      },
      "source": [
        "# optional TODO\n",
        "df['Year'] = df['Datetime'].dt.year  #The year of datetime\n",
        "df['Month_Of_Year'] = df['Datetime'].dt.month #The month as January=1, December=12\n",
        "df['Week_Of_Year'] = df['Datetime'].dt.week #The week ordinal of the year\n",
        "df['Day_Of_Datetime'] = df['Datetime'].dt.day #The days of the datetime\n",
        "df['Hours_Of_Datetime'] = df['Datetime'].dt.hour #The hours of the datetime\n",
        "df['Minutes_Of_Datetime'] = df['Datetime'].dt.minute #The minutes of the datetime\n",
        "\n",
        "# The day of the week with Monday=0, Sunday=6\n",
        "# TODO: convert the dayofweek into a categorical variable via one-hot encoding\n",
        "df['Day_Of_Week'] = df['Datetime'].dt.dayofweek  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onnuDSHstDGV"
      },
      "source": [
        "To avoid data leakage, split df into train set and test set;\n",
        "1. train set is used for training and cross validation (TimeSeriesSplit)\n",
        "2. test set is used for final sanity check of model\n",
        "\n",
        "Design a pipeline that includes all data preparations including \n",
        "-- standarization, transformation, normalization\n",
        "-- feature selection\n",
        "-- encoding, feature scaling, dimension reduction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgUhtf1FShce"
      },
      "source": [
        "names = ['Year', 'Month_Of_Year', 'Week_Of_Year', 'Day_Of_Datetime',\n",
        "       'Hours_Of_Datetime', 'Minutes_Of_Datetime', 'Day_Of_Week']\n",
        "for name in names:\n",
        "  num = np.unique(df[name])\n",
        "  print(f'The feature {name} has {len(num)} unique values: {num}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHtPl5RSEzAu"
      },
      "source": [
        "#names = ['year', 'month', 'week', 'day', 'hour', 'minute', 'dayofweek']\n",
        "for name in df.columns.values:\n",
        "  num = np.unique(df[name])\n",
        "  print(f'The feature {name} has {len(num)} unique values: {num}')\n",
        "  if len(num)==1:\n",
        "    print(f'***** Warning: The feature {name} is deletd as it only has a single unique value {num}')\n",
        "    del df[name]\n",
        "df.columns \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs8eiUT6tvcg"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0fapiaktxdz"
      },
      "source": [
        "df[df['systemId']=='sys1'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSHBiAYRteC9"
      },
      "source": [
        "# Define Constants\n",
        "import numpy as np\n",
        "import datetime\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SYSTEM_ID_SELECTED = ['sys1'] # ['sys1'] or ['All']\n",
        "TEST_SIZE = 0.2 # Not yet used\n",
        "USE_TIME_TRANSFORMED = True \n",
        "\n",
        "# We separately process sub-sets of columns and concatenate them in a numpy array called data \n",
        "target_col = ['cpu_utilization']\n",
        "numerical_transform_cols = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops',\n",
        "                            'read_throughput', 'write_throughput', 'read_iosz', 'write_iosz']\n",
        "time_col = ['timestamp_seconds']\n",
        "time_transformed_cols = ['Week_Sin', 'Week_Cos', 'Day_Sin', 'Day_Cos', 'Hour_Sin', 'Hour_Cos']\n",
        "cols_to_encode = ['systemId', 'model_type']\n",
        "\n",
        "#AS timestamp_seconds is appended after numerical_transform_cols are added to data\n",
        "# As feature selection can be RFE thus standarize/normalize data first;\n",
        "# Feature selection also cause time poition dymanic thus need be decided after all feature selection\n",
        "Y_INDEX_DATA = 0 # index of target in data\n",
        "X_FIRST_INDEX_DATA = 1 # first index in data for X\n",
        "\n",
        "data_cols = []\n",
        "df.describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbq-3PQo0XLj"
      },
      "source": [
        "# Basic checking and basic stats\n",
        "# check NA for need of imputation\n",
        "print(f'*** There are {df.isna().sum().sum()} NA data')\n",
        "print(df.info())\n",
        "print(df[df['systemId']==SYSTEM_ID_SELECTED[0]].loc[:, ['systemId', 'model_type', 'cpu_utilization']] .tail(5))\n",
        "print(df[df['systemId']==SYSTEM_ID_SELECTED[0]].loc[:, ['systemId', 'model_type', 'cpu_utilization']] .head(5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUgoar1zmGno"
      },
      "source": [
        "# Run: check time series's time is continuous or not\n",
        "# plot timestamp_seconds against target y cpu_utilization;\n",
        "def plot_time_y(df):\n",
        "  num_figs= 4\n",
        "  dftmp = df[df['systemId']==SYSTEM_ID_SELECTED[0]].loc[:, ['timestamp_seconds', 'cpu_utilization']]\n",
        "  t = df[df['systemId']==SYSTEM_ID_SELECTED[0]].loc[:, ['timestamp_seconds']].values/60\n",
        "  plt.figure()\n",
        "\n",
        "  plt.subplot(num_figs,1,1)\n",
        "  plt.plot( dftmp.index[1:], t[1:], 'g+')\n",
        "  plt.grid()\n",
        "  plt.ylabel('time in minutes')\n",
        "\n",
        "  t = t[1:]- t[0:-1] # time difference\n",
        "  plt.subplot(num_figs,1,2)\n",
        "  plt.plot(dftmp.index[1:], t, 'g-+')\n",
        "  plt.ylabel(f'time interval (minutes)')\n",
        "  ax = plt.gca()\n",
        "  ax.legend([f'min={t.min()}, max={t.max()}, mean={t.mean()}'], fontsize='large')\n",
        "  plt.grid()\n",
        "\n",
        "  num_days = 7\n",
        "  stop_index = int(num_days*24*60/5)\n",
        "  plt.subplot(num_figs,1,3)\n",
        "  plt.plot(dftmp.index, dftmp['cpu_utilization'].values)\n",
        "  plt.plot(dftmp.index[:stop_index], dftmp['cpu_utilization'].values[:stop_index], 'g-+')\n",
        "  plt.ylabel('cpu_utilization')\n",
        "  plt.grid()\n",
        "\n",
        "  plt.subplot(num_figs,1,4)\n",
        "  plt.plot(dftmp.index[:stop_index], dftmp['cpu_utilization'].values[:stop_index], 'g-+')\n",
        "  plt.ylabel('cpu_utilization')\n",
        "  ax = plt.gca()\n",
        "  ax.legend([f'zoomed in above figure'], fontsize='large')\n",
        "  plt.grid()\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18.5, num_figs*3)\n",
        "  fig.suptitle(f'time series check: min interval={t.min()}, max interval={t.max()}')\n",
        "  plt.tight_layout() # 2nd last step in fig setting\n",
        "  fig.subplots_adjust(top=0.88) # last in fig setting\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaEgR5YAMtw4"
      },
      "source": [
        "plot_time_y(df)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGy2QJDDyNag"
      },
      "source": [
        "# plot simple predictor as y[n]=y[n-1] cpu_utilization with decimation by D\n",
        "def plot_baseline_1step_predictor(D, df, systemId):\n",
        "\n",
        "  num_figs = 3\n",
        "  df = df[df['systemId']==systemId[0]].loc[:, ['timestamp_seconds', 'cpu_utilization']]\n",
        "  L = len(df)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot( df['cpu_utilization'][0:L-1:D].values, df['cpu_utilization'][1:L:D].values, 'g+')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM_bFZh_yawm"
      },
      "source": [
        "plot_baseline_1step_predictor(1, df, SYSTEM_ID_SELECTED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yndckIz7w2JE"
      },
      "source": [
        "# plot y cpu_utilization with decimation by D\n",
        "def plot_y_decimated(D, df, systemId):\n",
        "  num_figs = 3\n",
        "  df = df[df['systemId']==systemId[0]].loc[:, ['timestamp_seconds', 'cpu_utilization']]\n",
        "  L = len(df)\n",
        "  plt.figure()\n",
        "  plt.subplot(num_figs,1,1)\n",
        "  y_diff = df['cpu_utilization'][1:L:D].values - df['cpu_utilization'][:L-1:D].values\n",
        "\n",
        "  plt.plot( df.index[1:L:D], df['cpu_utilization'][1:L:D].values, '-+', label='cpu_utilization')\n",
        "  plt.plot( df.index[1:L:D], y_diff, 'g-+', label='delta cpu_utilization')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(num_figs,1,2)\n",
        "  plt.plot( df.index[1:L:D], df['cpu_utilization'][0:L-1:D].values, '-+', label='cpu_utilization')\n",
        "  plt.plot( df.index[1:L:D], df['cpu_utilization'][1:L:D].values, 'g-+', label='lag 1 cpu_utilization')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(num_figs,1,3)\n",
        "  plt.plot( df.index[1:L:D], 100*y_diff/df['cpu_utilization'][1:L:D].values, '-+', label='cpu_utilization delta in percent')\n",
        "  plt.legend()\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18.5, 3*num_figs)\n",
        "  ax = plt.gca()\n",
        "  ax.grid()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAmUnIhL0ID0"
      },
      "source": [
        "D = 1 # decimate by D*5 mins\n",
        "plot_y_decimated(D, df, SYSTEM_ID_SELECTED)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lkYJ3UXxN-P"
      },
      "source": [
        "df.shape, "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoT_jbnGLYHX"
      },
      "source": [
        "It is not a strict time series data as can be shown with time interval between adjacent samples are not always same (though mostly 5 minutes).\n",
        "So, it cannot be modelled with ARIMA-like methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANYv7bdPEdpX"
      },
      "source": [
        "## Now that the data is loaded. let's begin!!!\n",
        "\n",
        "# **About the data**\n",
        "\n",
        "---\n",
        "This is time series data for one month collected for N number of devices\n",
        "\n",
        "12 Columns\n",
        "\n",
        "systemId - Device name\n",
        "\n",
        "timestamp_seconds - epoch time when the sensor data was collected\n",
        "\n",
        "model_type - Different versions/release/model of the device (similar to mobile models)\n",
        "\n",
        "cpu_utilization - this percentage of how much the CPU is used on the device.\n",
        "\n",
        "read_cache_miss - Percentage of read that were not present in the Cache\n",
        "\n",
        "write_cache_miss - Percentage of write that were not present in the Cache\n",
        "\n",
        "read_iops -  Number of read IOs per second (Input/Output)\n",
        "\n",
        "write_iops -  Nummber of write IOs per second (Input/Output)\n",
        "\n",
        "read_throughput - the read bandwidth per second (Units kbps)\n",
        "\n",
        "write_throughput - the write bandwidth per second (Units kbps)\n",
        "\n",
        "read_iosz - the block size for read Input/Output operations\n",
        "\n",
        "write_iosz - the block size for write Input/Output operations\n",
        "\n",
        "\n",
        "y -> cpu_utilization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q8r1IWOAKIQ"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw8r7vKGEklC"
      },
      "source": [
        "## Q1. Do an EDA on the data, correlation plots, features that might be important for the modeling. Share your observations. Comment on how the data looks from modeling perspective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y479tMw7kg3v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8qb3jEoy3Ow"
      },
      "source": [
        "# plot utilities\n",
        "def plot_corr(df_tmp, systemId_selected):\n",
        "  import seaborn as sns\n",
        "  if systemId_selected[0] != 'All':\n",
        "    df_tmp = df_tmp[df_tmp['systemId']==systemId_selected[0]]\n",
        "  sns.set_theme(style=\"white\")\n",
        "  # Compute the correlation matrix\n",
        "  corr = df_tmp.corr()\n",
        "  corr_y = corr['cpu_utilization']\n",
        "  corr_y = corr_y.sort_values(ascending=False)\n",
        "  f, ax = plt.subplots(figsize=(15, 5))\n",
        "  corr_y.plot.bar(rot=60)\n",
        "  print(corr_y)\n",
        "  \n",
        "  # Generate a mask for the upper triangle\n",
        "  mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "  # Set up the matplotlib figure\n",
        "  f, ax = plt.subplots(figsize=(15, 9))\n",
        "  # Generate a custom diverging colormap\n",
        "  cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "  # Draw the heatmap with the mask and correct aspect ratio\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.9, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "  f.suptitle(f'systemId {systemId_selected[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRThy7dPzSt9"
      },
      "source": [
        "# RUN: plot hist\n",
        "# Conclusion: a few columns are far from Gaussian distribution\n",
        "plot_corr(df, SYSTEM_ID_SELECTED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJA_mJCcySNb"
      },
      "source": [
        "# plot histogram of systemId_selected\n",
        "def plot_df_hist(df_tmp, systemId_selected, title_str, cols, bins):\n",
        "  if systemId_selected[0] != 'All':\n",
        "    df_tmp = df_tmp[df_tmp['systemId']==systemId_selected[0]]\n",
        "  df_tmp = df_tmp[cols]\n",
        "  df_tmp.hist(bins=bins)\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18, 8)\n",
        "  fig.suptitle(f'{title_str}, systemId {systemId_selected[0]}')\n",
        "  plt.tight_layout()\n",
        "  fig.subplots_adjust(top=0.9)\n",
        "\n",
        "# utility hist2d plot xcols against target ycol\n",
        "def hist2d_plot(df_temp, systemId, xcols, ycol, xbins, ybins, vmax):\n",
        "  print(f\"hist2d_plot 1 Initial address of df_temp: {id(df_temp)}, shape={df_temp.shape}\")\n",
        "  if systemId != 'All': \n",
        "    df_temp = df[df['systemId']==systemId[0]]\n",
        "  print(f\"hist2d_plot 2 Initial address of df_temp: {id(df_temp)}, shape={df_temp.shape}\")\n",
        "  print(df_temp.shape)\n",
        "  len_x = len(xcols)\n",
        "  num_rows = math.ceil(math.sqrt(len_x))\n",
        "  num_cols = math.ceil(len_x/num_rows)\n",
        "  x_index = 0\n",
        "  for row in range(num_rows):\n",
        "    for col in range(num_cols):\n",
        "      if x_index == len_x:\n",
        "        break\n",
        "      plt.subplot(num_rows, num_cols, x_index+1)\n",
        "      plt.hist2d(df_temp[xcols[x_index]], df_temp[ycol], bins=[xbins, ybins], vmax=vmax) #\n",
        "      plt.colorbar()\n",
        "      plt.xlabel(xcols[x_index])\n",
        "      plt.ylabel(ycol)\n",
        "      x_index = x_index + 1\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18.5, 3*num_rows)\n",
        "  fig.suptitle(f'systemId={systemId}')\n",
        "  plt.tight_layout() # 2nd last step in fig setting\n",
        "  fig.subplots_adjust(top=0.88) # last in fig setting\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edyDh42I8B-T"
      },
      "source": [
        "\n",
        "title_str = 'Before transformation!'\n",
        "plot_df_hist(df, SYSTEM_ID_SELECTED, title_str, df.columns, 30)\n",
        "#NOTE: Hour_Sin AND Hour_Cos have only 7 distinct bins at [0, 30, 60, 90, -30, -60, -90] degress as \n",
        "# as sampling perid=5 minutes, thus 2pi*5min/60min=pi/6=30degree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOwZ76uV-RDA"
      },
      "source": [
        "The histograms show that we need apply normalization and power transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtyLQmcWebEX"
      },
      "source": [
        "#Plot hist2d: xcols against target ycol\n",
        "vmax = 100; xbins = 50; ybins = 50\n",
        "ycol = 'cpu_utilization'\n",
        "xcols_group = [\n",
        "  ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops'],\n",
        "  ['read_throughput', 'write_throughput', 'read_iosz', 'write_iosz'],\n",
        "  ['Week_Sin', 'Week_Cos', 'Day_Sin', 'Day_Cos', 'Hour_Sin', 'Hour_Cos']]\n",
        "for xcols in xcols_group:\n",
        "  hist2d_plot(df, SYSTEM_ID_SELECTED, xcols, ycol, xbins, ybins, vmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX2yFbs00w8M"
      },
      "source": [
        "# Start preprocessing data: transform to Gaussian, scale, label ategorical featurures and one-hot encode;\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vMT39fHGAh9"
      },
      "source": [
        "# All pre-processing functions\n",
        "# Transform to make data more Gaussian-like \n",
        "# Standarize to make close Norml(0, 1)\n",
        "# Only to numerical_transform_cols, not for timestamp, categoricals\n",
        "def get_target_for_transform(df, systemId_selected, numerical_transform_cols, print_flag=False):\n",
        "  from sklearn.preprocessing import PowerTransformer\n",
        "  #from sklearn.preprocessing import QuantileTransformer\n",
        "  if systemId_selected[0] == 'All':\n",
        "    df_tmp = df[numerical_transform_cols]\n",
        "  else:  \n",
        "    df_tmp = df[df['systemId']==systemId_selected[0]][numerical_transform_cols]\n",
        "  if print_flag:\n",
        "    print(f'*** Inside get_target_for_transform(): df_tmp.shape={df_tmp.shape}')\n",
        "  # Start Transformation\n",
        "  data = df_tmp.values\n",
        "  return data\n",
        "\n",
        "# Transform to make data more Gaussian-like \n",
        "# Standarize to make close Norml(0, 1)\n",
        "# Only to numerical_transform_cols, not for timestamp, categoricals\n",
        "def transform_features(df, systemId_selected, numerical_transform_cols, print_flag=False):\n",
        "  from sklearn.preprocessing import PowerTransformer\n",
        "  #from sklearn.preprocessing import QuantileTransformer\n",
        "  if systemId_selected[0] == 'All':\n",
        "    df_tmp = df[numerical_transform_cols]\n",
        "  else:  \n",
        "    df_tmp = df[df['systemId']==systemId_selected[0]][numerical_transform_cols]\n",
        "  # Start Transformation\n",
        "  data = df_tmp.values\n",
        "  power = PowerTransformer(method='box-cox', standardize=True)\n",
        "  data = power.fit_transform(data)\n",
        "  if print_flag:\n",
        "    print(f'*** Inside transform_features(): df_tmp.shape={df_tmp.shape}')\n",
        "    print(f'  transform_features: {power.get_params(deep=True)}')\n",
        "    print(f'  power.lambdas_: {power.lambdas_}')\n",
        "  return data, power\n",
        "\n",
        "# RUN: LabelEncoder + OneHotEncoder for categorical columns\n",
        "# return np.array oflabel encoded data and encoders\n",
        "def label_encode(df_in, systemId_selected, cols_to_encode, print_flag=False):\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  from sklearn.preprocessing import OneHotEncoder\n",
        "  if systemId_selected[0] == 'All':\n",
        "    df = df_in\n",
        "  else:  \n",
        "    df = df_in[df_in['systemId']==systemId_selected[0]]\n",
        "  print(f'inside label_encode: systemId={systemId_selected}, df shape={df.shape}')\n",
        "  first_col = True\n",
        "  label_encoders = []\n",
        "  my_onehot_encoders = []\n",
        "  for col in cols_to_encode:\n",
        "    label_encoder = LabelEncoder()\n",
        "    X = df[col].values\n",
        "    X = label_encoder.fit_transform(X)\n",
        "    label_encoders.append({col:label_encoder})\n",
        "    X = X.reshape(X.shape[0], 1)\n",
        "    onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "    X = onehot_encoder.fit_transform(X)\n",
        "    if print_flag:\n",
        "      print(f'******Inside label_encode: just after my_OneHotEncoder: col={col}, X.shape={X.shape}')\n",
        "      print(f'******Inside label_encode: just after my_OneHotEncoder: col={col}, onehot_encoder={onehot_encoder}')\n",
        "    if first_col:\n",
        "      data = X\n",
        "      #print(f'first_col={first_col}, col={col}, X.shape={X.shape}, data.shape={data.shape}')\n",
        "      first_col = False\n",
        "    else:        \n",
        "      data = np.concatenate((data, X), axis=1)\n",
        "      #print(f'first_col={first_col}, col={col}, X.shape={X.shape}, data.shape={data.shape}')\n",
        "  return data, label_encoders, my_onehot_encoders \n",
        "\n",
        "def minmax_scaler_time(df, systemId_selected, cols):\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  if systemId_selected[0] == 'All':\n",
        "    df_tmp = df[cols].values\n",
        "  else:  \n",
        "    df_tmp = df[df['systemId']==systemId_selected[0]][cols].values\n",
        "  minmax_scaler = MinMaxScaler()\n",
        "  df_tmp = minmax_scaler.fit_transform(df_tmp)\n",
        "  return df_tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geDYFuM9cMJB"
      },
      "source": [
        "Compare hists of each single systemId ('sys1' vs 'sys2', etc) and single systemId with hist of all systemIds 'All' case:\n",
        "1. each systemId has different hist patterns\n",
        "2. Hour_Sin and Hour_Cos show a pattern at certain times per hour\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fImgk3-41x9T"
      },
      "source": [
        "# Apply transforms and extract X, Y, x_cols;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4htquO_NTH9p"
      },
      "source": [
        "# RUN: apply transforms to df and extract X, Y, x_cols;\n",
        "\n",
        "# Step 1: transform AND standarization FOR numerical_transform_cols\n",
        "# Step 1.1: transform target Y\n",
        "# MUST first use numerical_transform_cols THEN use cols_to_encode\n",
        "# MUST NOT transform timestamp_seconds !!! \n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "data_cols = []\n",
        "data = get_target_for_transform(df, SYSTEM_ID_SELECTED, target_col)\n",
        "target_transformer = PowerTransformer(method='box-cox', standardize=True)\n",
        "data = target_transformer.fit_transform(data)\n",
        "data_cols = data_cols + target_col\n",
        "\n",
        "print(f'After Y PowerTransformer, data shape={data.shape}, target_col len = {len(target_col)}, data_cols={len(data_cols)}')\n",
        "print(f'  target_transformer.get_params = : {target_transformer.get_params(deep=True)}')\n",
        "print(f'  target_transformer.lambdas_= {target_transformer.lambdas_}')\n",
        "\n",
        "# Step 1.2: transform AND standarization FOR numerical_transform_cols\n",
        "# MUST first use numerical_transform_cols THEN use cols_to_encode\n",
        "# MUST NOT transform timestamp_seconds !!! \n",
        "\n",
        "print(f'Before X numericals transformation:')\n",
        "print(f'  data shape = {data.shape}')\n",
        "print(f'  len of numerical_transform_cols = {len(numerical_transform_cols)}')\n",
        "data_new, numerical_X_transformer = transform_features(df, SYSTEM_ID_SELECTED, numerical_transform_cols)\n",
        "data_cols = data_cols + numerical_transform_cols\n",
        "data = np.concatenate((data, data_new), axis=1)\n",
        "print(f'After X numericals transformation:')\n",
        "print(f'  data shape={data.shape}')\n",
        "print(f'  data_cols len={len(data_cols)},\\n  data_cols={data_cols}')\n",
        "\n",
        "\n",
        "# process timestamp_seconds\n",
        "print(f'Before add new column {time_col}:\\n  data shape={data.shape},\\n  len time_col={len(time_col)}')\n",
        "data_new = minmax_scaler_time(df, SYSTEM_ID_SELECTED, time_col)\n",
        "data_cols = data_cols + time_col\n",
        "data = np.concatenate((data, data_new), axis=1)\n",
        "print(f'After add {time_col}:\\n  data shape={data.shape},\\n  len data_cols={len(data_cols)}')\n",
        "\n",
        "# RUN: MinMax timestamp_seconds, not StandardScaler\n",
        "# timestamp_seconds will be mapped to data[:,9] AND X[:,8]\n",
        "if USE_TIME_TRANSFORMED:\n",
        "  data_cols = data_cols + time_transformed_cols\n",
        "  data_new = minmax_scaler_time(df, SYSTEM_ID_SELECTED, time_transformed_cols)\n",
        "  print(f'Before add {len(time_transformed_cols)} columns {time_transformed_cols}:\\n  data shape={data.shape},\\n  data_new shape={data_new.shape}')\n",
        "  data = np.concatenate((data, data_new), axis=1)\n",
        "  print(f'After add time_transformed_cols:\\n  data shape={data.shape},\\n  data_cols ={data_cols}')\n",
        "\n",
        "\n",
        "# Add label encoder categorical columns (model_type only) (sysemId)\n",
        "# Note: This is only useful is model all systemIds together;\n",
        "# WARNING: only run once to avoid add the same columns more than 1 times\n",
        "\n",
        "data_new, label_encoders, my_onehot_encoders = label_encode(df, SYSTEM_ID_SELECTED, cols_to_encode)\n",
        "print(f'Before add {len(cols_to_encode)} columns {cols_to_encode}:\\n  data shape={data.shape},\\n  data_new shape={data_new.shape}')\n",
        "data = np.concatenate((data, data_new), axis=1)\n",
        "data_cols = data_cols + cols_to_encode\n",
        "\n",
        "print(f'After concatenate:\\n  data shape={data.shape}')\n",
        "print(f'  data_cols len={len(data_cols)},\\n  data_cols={data_cols}')\n",
        "\n",
        "X = data[:, X_FIRST_INDEX_DATA:]\n",
        "Y = data[:, Y_INDEX_DATA]\n",
        "x_cols = data_cols[1:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAWSSzMMvNHb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SoypBGi0658"
      },
      "source": [
        "# Run to check results in good shape \n",
        "def check_data_dims():\n",
        "  dftemp = df[df['systemId']==SYSTEM_ID_SELECTED[0]]\n",
        "  print(f' df with {SYSTEM_ID_SELECTED} shape = {dftemp.shape}\\n')\n",
        "  del dftemp\n",
        "  print(f' data.shape = {data.shape}, len(data_cols) = {len(data_cols)}\\n')\n",
        "  print(f' len(x_cols)={len(x_cols)}, X.shape = {X.shape}, Y.shape = {Y.shape}\\n')\n",
        "  print(f' data_cols = {data_cols}, \\n x_cols = {x_cols}\\n' )\n",
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpsSA9rArmM3"
      },
      "source": [
        "Plot ACF and PACF of target cpu_utilization and all numerical features to check correlation in time lags and decide the need of TimeSeriesSplit for strict time series or KFold for iid samples.\n",
        "\n",
        "Note: the time steps are not always 5 minutes thus the ACF and PACF results shall be carefully read.\n",
        "May be need imputation???\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saXEF7d7m2Me"
      },
      "source": [
        "def plot_y_lag(df, systemId_selected, numerical_transform_cols):\n",
        "  if systemId_selected[0] != 'All':\n",
        "    df = df[df['systemId']==systemId_selected[0]][numerical_transform_cols]\n",
        "  for col in numerical_transform_cols:\n",
        "    y = df[col]\n",
        "\n",
        "    values = pd.DataFrame(y.values)\n",
        "    dataframe = pd.concat([values.shift(2), values.shift(1), values], axis=1)\n",
        "    dataframe.columns = ['t-2', 't-1', 't']\n",
        "    result = dataframe.corr()\n",
        "    print(result)\n",
        "\n",
        "\n",
        "    plt.plot(y[1:].values, y[0:-1].values, '.')\n",
        "    plt.figure()\n",
        "    plt.plot(y[2:].values, y[0:-2].values, '.')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r5YW83YrxUl"
      },
      "source": [
        "plot_y_lag(df, ['All'], target_col)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsMLRP8xolQu"
      },
      "source": [
        "plot_y_lag(df, ['sys19'], target_col)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZFso_MfpIiW"
      },
      "source": [
        "By scatter plot of y[n] and y[n-1] for the case of all systemIds, we can see no correlation there.\n",
        "The y[n] and y[n-1] relationship varies for individual single systemId, e.g. sys11 has strong correlation, but sys12 has weak correlation.\n",
        "This justifies the need of separate model for prediction for each sytemId separately.\n",
        "We can use labellled and one-hot encoded systemId as a model feature but this is a cost and add complexity.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSmVuQfTs3Vg"
      },
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "def plot_acf_mine(df, systemId_selected, numerical_transform_cols):\n",
        "  if systemId_selected[0] != 'All':\n",
        "    df = df[df['systemId']==systemId_selected[0]][numerical_transform_cols]\n",
        "\n",
        "  for col in numerical_transform_cols:\n",
        "    plot_acf(df[col])\n",
        "    plt.title(f'acf of {col} of systemId {systemId_selected[0]}')\n",
        "\n",
        "    plot_pacf(df[col])\n",
        "    plt.title(f'pacf of {col} of systemId {systemId_selected[0]}')\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur5lxCnYulpa"
      },
      "source": [
        "plot_acf_mine(df, SYSTEM_ID_SELECTED, target_col)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yneVyya5u2LY"
      },
      "source": [
        "plot_acf_mine(df, ['All'], target_col)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urKjWJbKsBSJ"
      },
      "source": [
        "plot_acf_mine(df, SYSTEM_ID_SELECTED, numerical_transform_cols)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34FY2tj8jKxS"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa0vKBm5vRT5"
      },
      "source": [
        "plot_acf_mine(df, ['All'], numerical_transform_cols)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kel3t2PBAkpN"
      },
      "source": [
        "target_col, data_cols.index(target_col[0])\n",
        "#data_cols.index(target_col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lr_2nEAkkP5"
      },
      "source": [
        "print(f'After add {time_col}:\\n  data shape={data.shape},\\n  len data_cols={len(data_cols)}')\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(data[:, data_cols.index('timestamp_seconds')])\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(data[:, data_cols.index('timestamp_seconds')], data[:, data_cols.index(target_col[0])])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-tXQCqCxF4b"
      },
      "source": [
        "# check time transformations are all right\n",
        "plt.figure()\n",
        "index_to_plots = [1,3,5,7,8]\n",
        "L = len(index_to_plots)\n",
        "for k in range(L):\n",
        "  plt.subplot(L,1,k+1)\n",
        "  plt.plot(data[:, data_cols.index('timestamp_seconds')], data[:, data_cols.index('timestamp_seconds') + index_to_plots[k]],)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeNDWMD6jyqv"
      },
      "source": [
        "# Compare hist before/after transformation/scaling \n",
        "# Before transformation: hist\n",
        "title_str = 'Before transformation!'\n",
        "plot_df_hist(df, SYSTEM_ID_SELECTED, title_str, df.columns, 30)\n",
        "#NOTE: Hour_Sin AND Hour_Cos have only 7 distinct bins at [0, 30, 60, 90, -30, -60, -90] degress as \n",
        "# as sampling perid=5 minutes, thus 2pi*5min/60min=pi/6=30degree\n",
        "\n",
        "# After transformation: hist\n",
        "title_str = 'After transformation!'\n",
        "data_df = pd.DataFrame(data, columns=data_cols)\n",
        "plot_df_hist(data_df, ['All'], title_str, data_df.columns, 30)\n",
        "del data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayriUf62sfFV"
      },
      "source": [
        "Feature selection:\n",
        "1. correlation based\n",
        "2. L1 norm\n",
        "3. p-value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daCEstTKvKjS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71mXegL8seAG"
      },
      "source": [
        "# RUN: feature selection based on correlation  \n",
        "# NOTE: this is done separately for each of systemId, not for all systemIds altogether\n",
        "#label encoder for model\n",
        "def select_feature_by_correlation(data, data_cols, target_col, corr_threshold):\n",
        "  df = pd.DataFrame(data, columns=data_cols)\n",
        "  corr = df.corr()\n",
        "  columns = np.full((corr.shape[0],), True, dtype=bool)\n",
        "  for i in range(corr.shape[0]):\n",
        "    for j in range(i+1, corr.shape[0]):\n",
        "      if corr.iloc[i,j] >= corr_threshold:\n",
        "        columns[j] = False\n",
        "  selected_columns = df.columns[columns]\n",
        "  removed_columns = [x for x in df.columns if x not in selected_columns]\n",
        "\n",
        "  print(f'df shape={df.shape}; corr shape={corr.shape};')\n",
        "  print(f'df.columns={df.columns}')\n",
        "  print(f'columns={columns}')\n",
        "  print(f'selected_columns={selected_columns}')\n",
        "  print(f'removed_columns={removed_columns}')\n",
        "  return selected_columns, removed_columns, corr\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YeRXt7Ewd0c"
      },
      "source": [
        "# RUN: feature selection based on correlation  \n",
        "CORR_THRESHOLD_FEATURE_SELECTION = 0.8;\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "#corr of X\n",
        "selected_columns, removed_columns, corr_X = select_feature_by_correlation(X, x_cols, target_col, CORR_THRESHOLD_FEATURE_SELECTION)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e2vpiv_xEW7"
      },
      "source": [
        "# corr of data\n",
        "_, _, corr_data = select_feature_by_correlation(data, data_cols, target_col, CORR_THRESHOLD_FEATURE_SELECTION)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T93dvZn7a0u"
      },
      "source": [
        "# plot 'read_throughput', 'read_cache_miss'\n",
        "cols_correlated = ['read_throughput', 'read_cache_miss', 'read_iops', 'write_iops']\n",
        "cols_correlated_index = [data_cols.index(x) for x in cols_correlated]\n",
        "\n",
        "print(len(data_cols), data.shape)\n",
        "print(cols_correlated_index, data_cols)\n",
        "plt.figure()\n",
        "plt.plot(data[:, cols_correlated_index[0]], data[:, cols_correlated_index[1]], '+')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(data[:, cols_correlated_index[0]], data[:, cols_correlated_index[2]], '+')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(data[:, cols_correlated_index[0]], data[:, cols_correlated_index[3]], '+')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(data[:, cols_correlated_index[2]], data[:, cols_correlated_index[3]], '+')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HygDlXOKAXK2"
      },
      "source": [
        "# make a regression prediction with an RFE pipeline\n",
        "# Observation: Features are selected almost according to their correlation with target\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "# create pipeline\n",
        "for num_features in range(2,17):\n",
        "  rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=num_features)\n",
        "  #model = DecisionTreeRegressor()\n",
        "  #pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "  # fit the model on all available data\n",
        "  #pipeline.fit(X, Y)\n",
        "  rfe.fit(X, Y)\n",
        "  scores = rfe.score(X, Y)\n",
        "  selected_features = rfe.get_support(indices=True)\n",
        "  print(num_features, selected_features, scores)\n",
        "  selected_feature_str = [x_cols[x] for x in selected_features]\n",
        "  print(f'  {selected_feature_str}')\n",
        "  #print(f\"  {corr_data.loc[selected_feature_str, target_col[0]]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kimaSZmqwCgN"
      },
      "source": [
        "# Start RFE feature selection; \n",
        "# Input: X, Y, num_features_to_select\n",
        "NUM_SPLITS = 10\n",
        "\n",
        "num_features_to_select = [12,13]\n",
        "# evaluate RFE for regression\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(num_features_to_select[0], num_features_to_select[1]):\n",
        "\t\trfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=i)\n",
        "\t\tmodel = DecisionTreeRegressor()\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        "\n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "  #cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "  kfold = KFold(n_splits = NUM_SPLITS) # shuffle=False\n",
        "  scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', \n",
        "                           cv=kfold, n_jobs=-1, error_score='raise')\n",
        "  return scores\n",
        "\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "  scores = evaluate_model(model, X, Y)\n",
        "  results.append(scores)\n",
        "  names.append(name)\n",
        "  print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "  #print(model.support_, model.ranking_)\n",
        "  print(model)\n",
        "# plot model performance for comparison\n",
        "plt.boxplot(results, labels=names, showmeans=True)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmwqmTSlmWkp"
      },
      "source": [
        "Observation:\n",
        "1. read_throughput and read_cache_miss are highly correlated (0.862791);\n",
        "2. model_type and systemId are unrelated in case of single case systemId\n",
        "3. timestamps not contribute much\n",
        "4. transfomred times have some correlation with a few columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfSUUplsnQ1Y"
      },
      "source": [
        "removed_columns = removed_columns + ['model_type', 'systemId']\n",
        "removed_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv55QQPZrs-m"
      },
      "source": [
        "def remove_data_columns(data, data_cols, removed_columns):\n",
        "  data_index_remove = [data_cols.index(x) for x in removed_columns]\n",
        "  print(f'data_index_remove = {data_index_remove}')\n",
        "  data = np.delete(data, data_index_remove, axis=1)\n",
        "  data_cols_updated = [ x for x in data_cols if x not in removed_columns]\n",
        "  print(data.shape, len(data_cols_updated), data_cols_updated)\n",
        "  return data_cols_updated, data  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETtdxZbwoRG2"
      },
      "source": [
        "# WARNING: only run this ONCE\n",
        "data_cols, data = remove_data_columns(data, data_cols, removed_columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QquxwQdyOfVe"
      },
      "source": [
        "**Observation** \n",
        "After comparison Correlation plots of two cases:\n",
        "-- Case_SINGLE: Only extract a single systemId (here 'sys1') case; \n",
        "-- Case_ALL:  Use all systemIds as a whole;\n",
        "\n",
        "Conclusion:\n",
        "1. timestamp_seconds has low correlation with all the rest features; and is less useful directly;\n",
        "2. Day_Cos and Day_Sin have good correlation with a few columns and shall help prediction;\n",
        "\n",
        "\n",
        "Case_SINGLE clearly exposes a larger nuber of columns having high correlations with the target 'cpu_utilization' than  that of Case_ALL. \n",
        "\n",
        "1.1 columns having good correlation with 'cpu_utilization':\n",
        "Case_SINGLE: ['write_throughput', 'write_iops', 'read_cache_miss', 'read_throughput', 'read_iops', 'write_cache_miss']\n",
        "\n",
        "Case_ALL: ['read_throughput', 'read_cache_miss'] \n",
        "['write_cache_miss',  'read_iops', 'write_throughput', 'write_iosz' ]\n",
        "\n",
        "1.1 columns having good correlation with Day_sin:  \n",
        "Case_SINGLE ['write_cache_miss', 'read_iops', 'write_iosz' ]\n",
        "Case_ALL: None\n",
        "\n",
        "1.2 Day_Cos has good correlation with 2 columns:\n",
        "Case_SINGLE: ['write_iops', 'read_iosz']\n",
        "Case_ALL: None\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p0hETXoVx8X"
      },
      "source": [
        "# check outliers; conclusion: no apparent outliers\n",
        "df_stat = df.describe().T\n",
        "df_stat['50% to 75%'] = (df_stat['50%']/df_stat['75%'])\n",
        "df_stat['max to 75%'] = (df_stat['max']/df_stat['75%'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlFAoZhAFnpP"
      },
      "source": [
        "# RUN: backwardElimination\n",
        "import statsmodels.api as sm\n",
        "def backwardElimination(x, Y, sl, columns):\n",
        "    print(f'x={x[:2,:]}')\n",
        "    print(f'Y={Y[:2]}')\n",
        "    print(f'len(x[0]) = {len(x[0])}')\n",
        "    print(f'x shape = {x.shape}')\n",
        "\n",
        "    numVars = len(x[0])\n",
        "    for i in range(0, numVars):\n",
        "        regressor_OLS = sm.OLS(Y, x).fit()\n",
        "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
        "        if maxVar > sl:\n",
        "            for j in range(0, numVars - i):\n",
        "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
        "                    x = np.delete(x, j, 1)\n",
        "                    columns = np.delete(columns, j)\n",
        "        print(f'**** backwardElimination i={i}, p-values={regressor_OLS.pvalues}')\n",
        "        print(f'inside loop: x shape = {x.shape}')\n",
        "                    \n",
        "    print(regressor_OLS.summary())\n",
        "    return x, columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TAeNfiAFJ_t"
      },
      "source": [
        "SL = 0.05\n",
        "data_modeled, selected_columns = backwardElimination(X, Y, SL, selected_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF5qqPmeErjG"
      },
      "source": [
        "## Q2. Group on systemId and one week's duration and then calculate following custom metrics\n",
        "- std/median on columns read_iops, read_cache_miss\n",
        "- rolling mean on columns write_throughput and write_iosz\n",
        "- exponential moving average on column write_cache_miss and write_iops\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbWfBocsso4I"
      },
      "source": [
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZx1kgeW-WMD"
      },
      "source": [
        "features = [['systemId', 'read_iops', 'read_cache_miss'],\n",
        "            ['systemId', 'write_throughput', 'write_iosz'],\n",
        "            ['systemId', 'write_cache_miss', 'write_iops']]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_ZRtJ9aEkSJ"
      },
      "source": [
        "dftemp = df[['systemId', 'read_iops', 'read_cache_miss']]\n",
        "metrics = []\n",
        "metrics_sum = dftemp.groupby('systemId').resample('W').sum()\n",
        "metrics_std = dftemp.groupby('systemId').resample('W').std()\n",
        "metrics_std = dftemp.groupby('systemId').resample('W').median()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zriHZDhU5aK"
      },
      "source": [
        "type(metrics_sum.index.values)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUwZyG-KWAmh"
      },
      "source": [
        "print(metrics_sum.index.values.shape)\n",
        "print(type(metrics_sum.index.values))\n",
        "print(type(metrics_sum.index.values[0]))\n",
        "print(type(metrics_sum.index.values[0][0]))\n",
        "\n",
        "print(metrics_sum.index.values.shape)\n",
        "print(metrics_sum.index.values[0])\n",
        "print(metrics_sum.index.values[:10][0])\n",
        "\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viu_0x0EYT-R"
      },
      "source": [
        "print(metrics_sum.index.values[:10][:][:][0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RWQa6oLGHhw"
      },
      "source": [
        "dftemp = df[['systemId', 'read_iops', 'read_cache_miss']]\n",
        "metrics = []\n",
        "metrics.append(dftemp.groupby('systemId').resample('W').sum())\n",
        "metrics.append(dftemp.groupby('systemId').resample('W').std())\n",
        "metrics.append(dftemp.groupby('systemId').resample('W').median())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKp7q1UuGWHF"
      },
      "source": [
        "metrics[2]['read_iops'].plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm5MN7DfSrfk"
      },
      "source": [
        "metrics[2].index.names\n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wfIH2u_F2gy"
      },
      "source": [
        "metrics_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN4wTbrjOy0p"
      },
      "source": [
        "\n",
        "dftemp = df[['systemId', 'read_iops', 'read_cache_miss']]\n",
        "df_grouped = dftemp.groupby('systemId')\n",
        "\n",
        "ct = 0\n",
        "for group_name, df_group in df_grouped:\n",
        "  ct = ct + 1\n",
        "  if ct>5: \n",
        "    break\n",
        "\n",
        "  print(f'\\n\\n ****** ct={ct}, {group_name} , shape={df_group.shape}****') \n",
        "  #print(df_group.index)\n",
        "  #print(df_group.head())\n",
        "\n",
        "  stat = df_group.resample('W', origin='start').median()\n",
        "  # rename columns\n",
        "  stat.columns = [group_name+'_read_iops_median', group_name+'_read_cache_miss_median']\n",
        "  #reset index to get grouped columns back\n",
        "  #stat = stat.reset_index()\n",
        "\n",
        "  print(f'\\n ****** {group_name} mean, shape={stat.shape}, stat type ={type(stat)}****') \n",
        "  print(stat)\n",
        "\n",
        "  if ct==1:\n",
        "    df_merged = stat\n",
        "  else:        \n",
        "    df_merged = pd.concat([df_merged, stat], axis=1) #ignore_index=True, \n",
        "\n",
        "  print(f'\\n ****** {group_name} mean, df_merged shape={df_merged.shape}, df_merged columns ={df_merged.columns}****') \n",
        "  print(f'**************** df_merged=\\n{df_merged}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8GLpaQ3DlLY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsaaUUMHOgxD"
      },
      "source": [
        "features = ['systemId','read_iops', 'read_cache_miss'] #, 'write_throughput', 'write_iosz', 'write_cache_miss', 'write_iops']\n",
        "dftemp = df[features]\n",
        "df_grouped = dftemp.groupby('systemId')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF6LAISRPB_U"
      },
      "source": [
        "df_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj5AGYs4P2-p"
      },
      "source": [
        "type(df_grouped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QloeuNMmDaDE"
      },
      "source": [
        "df_grouped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5nvj3fVQPXd"
      },
      "source": [
        "len(df_grouped)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_16duswP64S"
      },
      "source": [
        "for x in df_grouped:\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDA9j-X7P67a"
      },
      "source": [
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHHa5u4APCeC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl3dKXHTK0ua"
      },
      "source": [
        "def make_metrics_std_median(X, x_cols, std_median_cols, print_flag=False):\n",
        "  print(f' *** Inside make_metrics_std_median(): systemId={systemId_selected}, df shape={df.shape}')\n",
        "\n",
        "\n",
        "\n",
        "  df1[f] = df1[f].rolling(window=Window_Length).mean()\n",
        "\n",
        "# Run to add metrics\n",
        "std_median_cols = ['read_iops', 'read_cache_miss']\n",
        "make_metrics_std_median(X, x_cols, std_median_cols, True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGPnrBE7OJbm"
      },
      "source": [
        "features = [['read_iops', 'read_cache_miss'],\n",
        "            ['write_throughput', 'write_iosz'],\n",
        "            ['write_cache_miss', 'write_iops']]\n",
        "\n",
        "for k in range(len(features)):\n",
        "  print(features[k])\n",
        "  for x in features[k]:\n",
        "    print(x)\n",
        "\n",
        "\n",
        "dftemp = df[['systemId', 'read_iops', 'read_cache_miss']]\n",
        "df_grouped = dftemp.groupby('systemId')\n",
        "\n",
        "ct = 0\n",
        "for group_name, df_group in df_grouped:\n",
        "  ct = ct + 1\n",
        "  if ct>2: \n",
        "    break\n",
        "  \n",
        "  df_group = df_group.sort_index()\n",
        "\n",
        "  print(f'\\n\\n ****** {group_name} , shape={df_group.shape}****') \n",
        "  print(df_group.head())\n",
        "\n",
        "  stat = df_group.resample('W', origin='start').sum()\n",
        "  print(f'\\n ****** {group_name} sum, shape={stat.shape}****') \n",
        "  print(stat)\n",
        "\n",
        "  stat = df_group.resample('W', origin='start').median()\n",
        "  print(f'\\n ****** {group_name} median, shape={stat.shape}****') \n",
        "  print(stat)\n",
        "    \n",
        "  stat = df_group.resample('W', origin='start').std()\n",
        "  print(f'\\n ****** {group_name} std, shape={stat.shape}****') \n",
        "  print(stat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isadwiEsLeu_"
      },
      "source": [
        "print(X.shape, len(x_cols))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwZYLRwrJtNM"
      },
      "source": [
        "\n",
        "# normalize, average, and plot\n",
        "def plot_columns_average(df, AVERAGE_METHOD, Window_Length):\n",
        "  df1 = df[df['systemId']=='sys1']\n",
        "\n",
        "  start_hour = 24*1+15;\n",
        "  interval_hour= 24*20 + 10\n",
        "  start = int(start_hour*hour/(5*60))\n",
        "  stop = int((start_hour + interval_hour)*hour/(5*60))\n",
        "\n",
        "  for f in cols_to_avg:\n",
        "    if AVERAGE_METHOD == 'rolling_moving_average':\n",
        "      df1[f] = df1[f].rolling(window=Window_Length).mean()\n",
        "    elif AVERAGE_METHOD == 'exponential_moving_average':\n",
        "      df1[f] = df1[f].ewm(span=Window_Length, adjust=False).mean()\n",
        "    elif AVERAGE_METHOD == 'cumulative_average': \n",
        "      df1[f] = df1[f].expanding(min_periods=Window_Length, adjust=False).mean()\n",
        "    else:\n",
        "      raise ValueError(f'AVERAGE_METHOD ={AVERAGE_METHOD} is not implemented!')\n",
        "        \n",
        "Window_Length = int(2*60/5) \n",
        "\n",
        "AVERAGE_METHOD_LIST = ['rolling_moving_average', 'exponential_moving_average', 'cumulative_average']\n",
        "AVERAGE_METHOD = AVERAGE_METHOD_LIST[1] \n",
        "plot_columns_average(df, AVERAGE_METHOD, Window_Length)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33R0k37jKLxc"
      },
      "source": [
        "group = group.sort_values(by='timestamp_seconds')\n",
        "group.head(8)tmp = group.rolling(2).mean()\n",
        "tmp.head(8)\n",
        "\n",
        "group['rmean_write_throughput'] = tmp['write_throughput']\n",
        "group['rmean_write_iosz'] = tmp['write_iosz']\n",
        "group.head(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V74KW2qUp1Bm"
      },
      "source": [
        "# fft plot TODO: for time ordered and single systemId \n",
        "def plot_fft(df_in, col_choice):\n",
        "  import tensorflow as tf\n",
        "  df = df_in[df_in['systemId']==SYSTEM_ID_SELECTED[0]]\n",
        "  fft = tf.signal.rfft(df[col_choice])\n",
        "  f_per_dataset = np.arange(0, len(fft))\n",
        "  print(f'df.shape={df.shape}, fft.shape={fft.shape}')\n",
        "  n_samples_h = len(df[col_choice])\n",
        "  plt.plot(f_per_dataset, np.abs(fft), '-+')\n",
        "  #plt.xscale('log')\n",
        "  plt.ylim(0, np.mean(np.abs(fft))*10)\n",
        "  plt.xlim([0.1, max(plt.xlim())])\n",
        "  _ = plt.xlabel('Frequency (log scale)')\n",
        "  plt.grid()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXWbL1MKUHUK"
      },
      "source": [
        "plot_fft(df, 'cpu_utilization')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zkw_1g-NEAx"
      },
      "source": [
        "plot_fft(df, 'read_throughput')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnFVO6ylYnIl"
      },
      "source": [
        "plot_fft(df, 'write_throughput')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk984iciULm4"
      },
      "source": [
        "plot_fft(df, 'Hour_Cos')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw8C45N6VCMj"
      },
      "source": [
        "plot_fft(df, 'Day_Cos')\n",
        "7966/24/12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix-gkEm8VOnH"
      },
      "source": [
        "plot_fft(df, 'Week_Cos')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMvlqIcDwVoC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRDklfCQwcGf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tkAAvJ2wKDP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVK4DMG6E5DQ"
      },
      "source": [
        "\n",
        "if 0:\n",
        "  summary = []\n",
        "  for name, group in group_by_systemId:\n",
        "    summary.append(\n",
        "      {\n",
        "        name : \n",
        "          {\n",
        "            'length': len(group),\n",
        "            'min': group['read_iops'].min(),\n",
        "            'max': group['read_iops'].max(),\n",
        "            'mean': group['read_iops'].mean(),\n",
        "            'std': group['read_iops'].std(),\n",
        "            'median': group['read_iops'].median()\n",
        "          }\n",
        "      }\n",
        "    )\n",
        "  if 0:\n",
        "    print(f'name={name}, len = {len(group)}\\n')\n",
        "    print('systemId uniques=', np.unique(group['systemId']))\n",
        "    #print(f'group={group.head(2)}\\n')\n",
        "    print(f'summary={summary}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpcmd8lhQwDh"
      },
      "source": [
        "## Q3. Generate a random distribution of samples from data such that each day should contain 12 continous samples and start of the sample should be random with that day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyzNxQmp3OkJ"
      },
      "source": [
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kX575JMFcdg"
      },
      "source": [
        "## Q4. Fit a linear regression to this data with y as \"cpu_utilization\" column. Comment on the fit of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSYOL0BkSrcC"
      },
      "source": [
        "TODO: \n",
        "1. reserve a common test set for comparison scores like mse\n",
        "(time continuity ???)\n",
        "2. define a baseline model simply y[n]=y[n-1]\n",
        "3. Linear Regression, Ridge, Lasso with Grid Search\n",
        "4. GradientBoost, XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZeVOVe9FGPg"
      },
      "source": [
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqYE2sDGM2a-"
      },
      "source": [
        "# First define a baseline prediction model as y[n] = y[n-1];\n",
        "# There are a few discontinuity points which can be ignored.\n",
        "\n",
        "# persistence model\n",
        "def model_persistence(x):\n",
        "\treturn x\n",
        "\n",
        "def baseline_timeseries_predictor_y(Y):\n",
        "  values = pd.DataFrame(Y)\n",
        "  from pandas import concat\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "\n",
        "  # Create lagged dataset\n",
        "  dataframe = concat([values.shift(1), values], axis=1)\n",
        "  dataframe.columns = ['t-1', 't']\n",
        "  print(dataframe.head(5))\n",
        "\n",
        "  # time series is split into train and test sets\n",
        "  X = dataframe.values\n",
        "  train_size = int(len(X) * 0.66)\n",
        "  train, test = X[1:train_size], X[train_size:]\n",
        "  train_X, train_y = train[:,0], train[:,1]\n",
        "  test_X, test_y = test[:,0], test[:,1]\n",
        "\n",
        "  # walk-forward validation\n",
        "  predictions = list()\n",
        "  for x in test_X:\n",
        "    yhat = model_persistence(x)\n",
        "    predictions.append(yhat)\n",
        "\n",
        "  test_score = mean_squared_error(test_y, predictions)\n",
        "\n",
        "  print('Test MSE: %.3f' % test_score)\n",
        "\n",
        "  # plot predictions and expected results\n",
        "  stop_idx = 100 #len(test_y)\n",
        "  plt.figure()\n",
        "  plt.plot(test_y[:stop_idx], 'r-+')\n",
        "  plt.plot(predictions[:stop_idx], 'g-o')\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(22.5, 3)\n",
        "\n",
        "  stop_idx = 100 #len(test_y)\n",
        "  plt.figure()\n",
        "  plt.plot(test_y[:stop_idx], 'r-+')\n",
        "  plt.plot(test_y[:stop_idx] - predictions[:stop_idx], 'g-o')\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(22.5, 3)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(test_y, predictions, 'r+')\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(10, 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DSqQ1RqNWgH"
      },
      "source": [
        "baseline_timeseries_predictor_y(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt78-HKTG_IR"
      },
      "source": [
        "def RFE_timelagged_y(dataframe):\n",
        "  from sklearn.feature_selection import RFE\n",
        "  from sklearn.ensemble import RandomForestRegressor\n",
        "  from matplotlib import pyplot\n",
        "  array = dataframe.values\n",
        "  X = array[:,0:-1]\n",
        "  y = array[:,-1]\n",
        "  # perform feature selection\n",
        "  rfe = RFE(RandomForestRegressor(n_estimators=500, random_state=1), n_features_to_select=4)\n",
        "  fit = rfe.fit(X, y)\n",
        "  # report selected features\n",
        "  print('Selected Features:')\n",
        "  names = dataframe.columns.values[0:-1]\n",
        "  for i in range(len(fit.support_)):\n",
        "    if fit.support_[i]:\n",
        "      print(names[i])\n",
        "  # plot feature rank\n",
        "  names = dataframe.columns.values[0:-1]\n",
        "  ticks = [i for i in range(len(names))]\n",
        "  pyplot.bar(ticks, fit.ranking_)\n",
        "  pyplot.xticks(ticks, names)\n",
        "  pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLOOBhX3HCWo"
      },
      "source": [
        "Selected Features:\n",
        "t-6\n",
        "t-3\n",
        "t-2\n",
        "t-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8KFnDZNB8j3"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCR7KVUWB8xk"
      },
      "source": [
        "# create num_lags number of new columns of shifted observations\n",
        "def baseline_timeseries_multilags_y(Y, num_lags, rolling_window_widths, ewm_alphas):\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  values = pd.DataFrame(Y)\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "\n",
        "  # Create lagged dataset\n",
        "  dataframe = pd.DataFrame()\n",
        "  for i in range(num_lags+1):\n",
        "    dataframe['t-'+str(i)] = values.shift(i).values[:,0]\n",
        "\n",
        "  if len(rolling_window_widths)>0:\n",
        "    for rolling_window in rolling_window_widths:\n",
        "      # temp solution: add rolling before target y as rolling mean of t-1 as we extract y as last column later\n",
        "      dataframe['rolling std '+str(rolling_window)] = dataframe['t-1'].rolling(window=rolling_window).std()\n",
        "      dataframe['rolling mean '+str(rolling_window)] = dataframe['t-1'].rolling(window=rolling_window).mean()\n",
        "  if len(ewm_alphas)>0:\n",
        "    for alpha in ewm_alphas: \n",
        "      dataframe['ewm_mean '+str(alpha)] = dataframe['t-1'].ewm(alpha=alpha).mean()\n",
        "      dataframe['ewm_std '+str(alpha)] = dataframe['t-1'].ewm(alpha=alpha).std()\n",
        "\n",
        "  #print(dataframe.head(num_lags+1))\n",
        "  ignored_nans = max(num_lags, len(rolling_window_widths), )\n",
        "  dataframe = dataframe[ignored_nans+1:]\n",
        "  dataframe.head(10)\n",
        "  return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhhnILYFKLXh"
      },
      "source": [
        "def LinearRegression_mse(dataframe):\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "  from sklearn.linear_model import LinearRegression \n",
        "\n",
        "  #X = dataframe.values\n",
        "  train_size = int(len(dataframe) * 0.66)\n",
        "  train_y = dataframe.iloc[:train_size, 0].values\n",
        "  train_X = dataframe.iloc[:train_size, 1:].values\n",
        "\n",
        "  test_y = dataframe.iloc[train_size:, 0].values\n",
        "  test_X = dataframe.iloc[train_size:, 1:].values\n",
        "\n",
        "  train_y = train_y.reshape(train_y.shape[0], 1)\n",
        "  test_y = test_y.reshape(test_y.shape[0], 1)\n",
        "\n",
        "  print(f'**** train_X shape = {train_X.shape}, test_X shape = {test_X.shape}')\n",
        "  print(f'**** train_y shape = {train_y.shape}, test_y shape = {test_y.shape}')\n",
        "\n",
        "  model = LinearRegression()\n",
        "  model.fit(train_X, train_y)\n",
        "  predictions = model.predict(test_X)\n",
        "  test_score = mean_squared_error(test_y, predictions)\n",
        "\n",
        "  if 0:\n",
        "    print(type(train_X), type(train_y))\n",
        "    print(train_X.shape, train_y.shape)\n",
        "    print(test_X.shape, test_y.shape)\n",
        "    print(f'type model.coef_={type(model.coef_)}')\n",
        "  \n",
        "  print(f'model.coef_={model.coef_}')\n",
        "  print('Test MSE: %.3f' % test_score)  \n",
        "  print(f'model.coef_ shape={model.coef_.shape}, columns = {dataframe.columns[1:]}')\n",
        "\n",
        "  plt.figure()\n",
        "  model.coef_ = model.coef_.reshape(model.coef_.shape[1],)\n",
        "  plt.plot(dataframe.columns[1:], model.coef_, 'r-o')\n",
        "  plt.title('model coef')\n",
        "  plt.xticks(rotation = 60)\n",
        "  plt.grid()\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(22.5, 3)\n",
        "\n",
        "  # plot predictions and expected results\n",
        "  stop_idx = 100 #len(test_y)\n",
        "  plt.figure()\n",
        "  plt.plot(test_y[:stop_idx], 'r-+')\n",
        "  plt.plot(predictions[:stop_idx], 'g-o')\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(22.5, 3)\n",
        "\n",
        "  stop_idx = 100 #len(test_y)\n",
        "  plt.figure()\n",
        "  plt.plot(test_y[:stop_idx], 'r-+')\n",
        "  plt.plot(test_y[:stop_idx] - predictions[:stop_idx], 'g-o')\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(22.5, 3)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(test_y, predictions, 'r+')\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(5, 5)\n",
        "\n",
        "  return test_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I9NCjdeLhkU"
      },
      "source": [
        "num_lags = 1\n",
        "rolling_window = []\n",
        "ewm_alphas = []\n",
        "y_lagged_df = baseline_timeseries_multilags_y(Y, num_lags, rolling_window, ewm_alphas)\n",
        "#y_lagged_df = y_lagged_df[['t-6', 't-3', 't-2', 't-1', 't']]\n",
        "(y_lagged_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JuwiKJOB8_p"
      },
      "source": [
        "num_lags = 5\n",
        "rolling_window = [6, 12]\n",
        "ewm_alphas = [0.75, 0.5, 0.1]\n",
        "y_lagged_df = baseline_timeseries_multilags_y(Y, num_lags, rolling_window, ewm_alphas)\n",
        "#y_lagged_df = y_lagged_df[['t-6', 't-3', 't-2', 't-1', 't']]\n",
        "(y_lagged_df.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDF7BXb9H-HP"
      },
      "source": [
        "mse = LinearRegression_mse(y_lagged_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W72KDRFPoFD"
      },
      "source": [
        "Adding 12 lags of target y yields MSE of 0.5 which is better than mse of 1.0 of naive model of single lag case but still worse;\n",
        "TODO:\n",
        "add lags to the feature sets inckuding read_tput etc;\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJITjdxZHa2z"
      },
      "source": [
        "print(y_lagged_df.shape, type(y_lagged_df))\n",
        "y_lagged_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2dVi8Ibeiuc"
      },
      "source": [
        "Gradient Boost Regressor mse is 0.04 which is much smaller than mse of baseline mse of 1.02."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyuR2EIzH-Y3"
      },
      "source": [
        "y_lagged_df.iloc[:3,:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ehMGxorIyvi"
      },
      "source": [
        "y_lagged_df.iloc[:3,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_3Xm8z7eovv"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)\n",
        "reg = GradientBoostingRegressor(random_state=0)\n",
        "reg.fit(X_train, y_train)\n",
        "reg.score(X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVSrXgoHfrIX"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "y_hat = reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_hat)\n",
        "print(f'mse={mse}')\n",
        "plt.plot(y_test, y_hat, '+')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4_jA_iqFHL-"
      },
      "source": [
        "The effect of time in this data set is not clear as shown be correlation of time with target.\n",
        "Thus we can use KFold for cross-validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGLIPvv-PhR-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIB0EOqZYNOT"
      },
      "source": [
        "# Use LinearRegression for regression, DecisionTreeRegressor for feature selection\n",
        "# explore the number of selected features for RFE\n",
        "# Try Ridge, Lasso, ElasticNet for regularization and feature selection\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        "\n",
        "n_features_to_select = [2, len(x_cols)]\n",
        "ridge_alpha = 0.1 # larger alpha stronger regularisation\n",
        "lasso_alpha = 0.1 # larger alpha stronger regularisation\n",
        "\n",
        "from enum import Enum\n",
        "class LRModel(Enum):\n",
        "   LR = 1\n",
        "   RIDGE = 2\n",
        "   LASSO = 3\n",
        "   ELASTICNET = 4\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset(X, Y):\n",
        "\tX, y = X, Y\n",
        "\treturn X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models(model_choice):\n",
        "  models = dict()\n",
        "  for i in range(n_features_to_select[0], n_features_to_select[1]):\n",
        "    #rfe = RFE(estimator=LinearRegression(), n_features_to_select=i)\n",
        "    rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=i)\n",
        "    if model_choice == LRModel.LR:\n",
        "      model = LinearRegression()\n",
        "    elif model_choice == LRModel.RIDGE:\n",
        "      model = Ridge()\n",
        "    elif model_choice == LRModel.LASSO:\n",
        "      model = Lasso()\n",
        "    elif model_choice == LRModel.ELASTICNET:\n",
        "      model = ElasticNet()\n",
        "    else:\n",
        "      print(\"Unexpected arg for LR model\")\n",
        "      raise\n",
        "  models[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "  return models\n",
        "\n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "\tcv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
        "\tscores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        "\n",
        "\n",
        "# define dataset\n",
        "X, y = get_dataset(X, Y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhl4lvM1F2Sp"
      },
      "source": [
        "# RFE + LinearRegression/Ridge/Lasso/ElasticNet + evaluate_model\n",
        "# get the models to evaluate\n",
        "models = get_models(LRModel.RIDGE)\n",
        "\n",
        "# KFOLD Cross-validation\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlOcaazcGfeq"
      },
      "source": [
        "# Time-series split with time continuity kept\n",
        "# evaluate the models and store results\n",
        "\n",
        "TRAIN_FRACTION = 0.7\n",
        "L_train = int(len(Y)*TRAIN_FRACTION)\n",
        "\n",
        "X_train = X[:L_train, :]\n",
        "Y_train = Y[:L_train]\n",
        "X_test = X[L_train:, :]\n",
        "Y_test = Y[L_train:]\n",
        "\n",
        "results, names = list(), list()\n",
        "model.fit(X_train, Y_train)\n",
        "predicted = model.predict(X_test, Y_test)\n",
        "\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNwjNOPrGzl1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5vgA3c2vtXx"
      },
      "source": [
        "# DNN regression\n",
        "NUM_SPLITS = 5\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "# Regression\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "Y = data[:, Y_INDEX_DATA]\n",
        "X = data[:, X_FIRST_INDEX_DATA:]\n",
        "\n",
        "# define base model\n",
        "def baseline_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(18, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "  \n",
        "# Step: evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, verbose=2)))\n",
        "pipeline = Pipeline(estimators)\n",
        "\n",
        "kfold = KFold(n_splits = NUM_SPLITS, shuffle=False)\n",
        "print(f'X shape={X.shape}, Y shape = {Y.shape}')\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(f'results of cross_val_score = {results}')\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSLnCrYavtk7"
      },
      "source": [
        "# DNN/MLP: get predicted values\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# TRY this also\n",
        "#from sklearn import linear_model\n",
        "\n",
        "# cross_val_predict returns an array of the same size as `y` where each entry\n",
        "# is a prediction obtained by cross validation:\n",
        "#results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "predicted = cross_val_predict(pipeline, X, Y, cv=kfold)\n",
        "# method='predict', \n",
        "# X: array-like of shape (n_samples, n_features)\n",
        "# Returns: When method is predict: (n_samples,)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(Y, predicted)\n",
        "ax.plot([Y.min(), Y.max()], [Y.min(), Y.max()], 'k--', lw=4)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "ax.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCTuPWtTwzAW"
      },
      "source": [
        "mse = mean_squared_error(Y, predicted)\n",
        "print(f'mse={mse}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K1bRffESCD0"
      },
      "source": [
        "# DNN mse 0.06 is worse than GradientBoost 0.04"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxCsofhfhijW"
      },
      "source": [
        "# plot against time IN transformed/scaled version\n",
        "def plot_prediction_time(start, stop, X, Y, predicted):\n",
        "  num_cols = 3\n",
        "  plt.subplot(num_cols, 1, 1)\n",
        "  plt.plot(X[start:stop, x_cols.index('timestamp_seconds')], Y[start:stop], 'b-+', label='rea target')\n",
        "  #plt.plot(X[start:stop, x_cols.index('timestamp_seconds')], ( predicted[start:stop]), 'g-o', label='predicted target')\n",
        "  plt.plot(X[start:stop, x_cols.index('timestamp_seconds')], (Y[start:stop] - predicted[start:stop]), 'g-o', label='error')\n",
        "  plt.ylabel('scaled target')\n",
        "  plt.xlabel('scaled time')\n",
        "  plt.grid(True)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(num_cols, 1, 2)\n",
        "  plt.plot(data[start:stop, data_cols.index('timestamp_seconds')], (Y[start:stop] - predicted[start:stop])/(Y[start:stop]), 'r-*', label='error/real')\n",
        "  plt.ylim(0, 1)\n",
        "  plt.ylabel('scaled target')\n",
        "  plt.xlabel('scaled time')\n",
        "  plt.grid(True)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(num_cols, 1, 3)\n",
        "  #plt.plot(X[start:stop, x_cols.index('timestamp_seconds')])\n",
        "  plt.hist((Y[start:stop] - predicted[start:stop])/(Y[start:stop]), bins=100)\n",
        "  plt.ylabel('scaled time')\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(22.5, num_cols*3)\n",
        "\n",
        "\n",
        "# Get orignal target w/o transformation from df\n",
        "def get_target(df, systemId_selected, numerical_transform_cols):\n",
        "  if systemId_selected[0] == 'All':\n",
        "    df_tmp = df[numerical_transform_cols]\n",
        "  else:  \n",
        "    df_tmp = df[df['systemId']==systemId_selected[0]][numerical_transform_cols]\n",
        "  return df_tmp.values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBkHs8Gdxp6Y"
      },
      "source": [
        "start= 1000 #int(0*24*60/5)\n",
        "stop = 1111 #len(Y) #int(1*24*60/5)\n",
        "plot_prediction_time(start, stop, X, Y, predicted)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmJANDYhGtX8"
      },
      "source": [
        "start=100; stop=110\n",
        "print(Y[start:stop], '\\n',predicted[start:stop], '\\n',\n",
        "      Y[start:stop]- predicted[start:stop], '\\n', (Y[start:stop]- predicted[start:stop])/Y[start:stop])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGHpeYz6GIA3"
      },
      "source": [
        "plt.plot(Y, predicted, '+')\n",
        "plt.grid()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(22.5, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7edDXw2TxBV1"
      },
      "source": [
        "y_real = get_target(df, SYSTEM_ID_SELECTED, 'cpu_utilization' )\n",
        "\n",
        "# RUN: inverse_transform of predicted target \n",
        "y_hat = target_transformer.inverse_transform( predicted.reshape(predicted.shape[0], 1) )\n",
        "print(predicted.shape, y_hat.shape, type(y_hat))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDkxtKHexa_C"
      },
      "source": [
        "# RUN: inverse_transform TRUE target transformed\n",
        "y_real_inverse_transfomred = target_transformer.inverse_transform( Y.reshape(Y.shape[0], 1) )\n",
        "print(Y.shape, y_real_inverse_transfomred.shape, type(y_real_inverse_transfomred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdxBPl5XzBQ4"
      },
      "source": [
        "plt.plot(y_real, y_real_inverse_transfomred, '+')\n",
        "plt.grid()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(22.5, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkFDk0ASyHea"
      },
      "source": [
        "plt.plot(y_real_inverse_transfomred, y_hat, '+')\n",
        "plt.grid()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(22.5, 5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iKuQORI0GEQ"
      },
      "source": [
        "# TODO: get real time in timedate\n",
        "#plot inverse transformed y_hat against time\n",
        "fig, ax = plt.subplots()\n",
        "start= int(0*24*60/5)\n",
        "stop = int(0.2*24*60/5)\n",
        "\n",
        "ax.plot(X[:, x_cols.index('timestamp_seconds')][start:stop], y_real[start:stop], 'b-+', label='real')\n",
        "ax.plot(X[:, x_cols.index('timestamp_seconds')][start:stop], y_hat[:, 0][start:stop], 'g-o', label='predicted')\n",
        "ax.set_xlabel('scaled time')\n",
        "ax.set_ylabel('Predicted')\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(22.5, 3)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqOQt7XBLb-s"
      },
      "source": [
        "# Run this after Standardarization\n",
        "# Add nonlinear transformation of features\n",
        "df['rw_cache_miss_ratio'] = df['read_cache_miss'] / df['write_cache_miss']\n",
        "df['rw_iops_ratio'] = df['read_iops'] / df['write_iops']\n",
        "df['rw_throughput_ratio'] = df['read_throughput'] / df['write_throughput']\n",
        "df['rw_iosz_ratio'] = df['read_iosz'] / df['write_iosz']\n",
        "\n",
        "df['rw_cache_miss_diff'] = df['read_cache_miss'] - df['write_cache_miss']\n",
        "df['rw_iops_diff'] = df['read_iops'] - df['write_iops']\n",
        "df['rw_throughput_diff'] = df['read_throughput'] - df['write_throughput']\n",
        "df['rw_iosz_diff'] = df['read_iosz'] - df['write_iosz']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vjiyYZ1U7x7"
      },
      "source": [
        "# remove cols_remove from cols;\n",
        "# divide cols into array of lists of size W \n",
        "def split_columns(cols, cols_remove, W):\n",
        "  for col in cols_remove:\n",
        "    cols.remove(col)\n",
        "  L = len(cols)\n",
        "  R = int(L/W)\n",
        "  x1 = list(np.array(cols[:R*W]).reshape(R,W))\n",
        "  x2 = cols[R*W:]\n",
        "  x1.append(x2)\n",
        "  return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM0rK8fcJAC4"
      },
      "source": [
        "# RUN: pairplotr VERY SLOW for larger number of features! !!\n",
        "def pairplot(df, systemId_selected, cols, title_str):\n",
        "  if len(systemId_selected)>0:\n",
        "    df = df[df['systemId']==systemId_selected]\n",
        "  sns.pairplot(df[cols], corner=True)\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(15.5, 3*2)\n",
        "  fig.suptitle(f'{title_str}, systemId {systemId_selected[0]}')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHKHE7h5Z91t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKyG7xdKHobv"
      },
      "source": [
        "systemId = 'sys1'\n",
        "cols = list(df.columns.values)\n",
        "cols_remove = ['timestamp_seconds', 'systemId', 'model_type']\n",
        "W = 4\n",
        "cols_groups = split_columns(cols, cols_remove, W)\n",
        "print(f'cols_groups={cols_groups}')\n",
        "title_str = 'Before Transformation'\n",
        "\n",
        "for col_subset in cols_groups:\n",
        "  pairplot(df, systemId, col_subset, title_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmqsItbZ_bCR"
      },
      "source": [
        "print(f' data.shape={data.shape}, data_cols len={len(data_cols)}\\n data_cols={data_cols}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_jGhaNqH-FO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgtNPUf0xp-x"
      },
      "source": [
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_real, y_hat[:, 0])\n",
        "ax.plot([y_real.min(), y_real.max()], [y_real.min(), y_real.max()], 'k--', lw=4)\n",
        "ax.set_xlabel('Measured')\n",
        "ax.set_ylabel('Predicted')\n",
        "ax.grid()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18.5, 5)\n",
        "plt.show()\n",
        "predicted.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcgEv1n7xqD-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyVV3Z8pvqgK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMZkqYdVz_Gu"
      },
      "source": [
        "Finished regression here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eL3dMJyUnxm"
      },
      "source": [
        "# Use LinearRegression for regression, LinearRegression for feature selection\n",
        "# DecisionTreeRegressor for feature selection ??? TODO\n",
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        "\n",
        "n_features_to_select = [2, len(x_cols)]\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset(X, Y):\n",
        "\tX, y = X, Y\n",
        "\treturn X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(n_features_to_select[0], n_features_to_select[1]):\n",
        "\t\t#rfe = RFE(estimator=LinearRegression(), n_features_to_select=i)\n",
        "\t\trfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=i)\n",
        "\t\tmodel = LinearRegression()\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        "\n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "\tcv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
        "\tscores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        "\n",
        "\n",
        "# define search space\n",
        "param_space = dict()\n",
        "param_space['n_estimators'] = [10, 100, 500]\n",
        "param_space['max_features'] = [2, 4, 6]\n",
        "\n",
        "def grid_search_regression(model, param_space):\n",
        "  cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "  # define search\n",
        "  search = GridSearchCV(model, space, scoring=neg_mean_squared_error, n_jobs=1, cv=cv_inner, refit=True)\n",
        "  # configure the cross-validation procedure\n",
        "  cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "  # execute the nested cross-validation\n",
        "  scores = cross_val_score(search, X, y, scoring='accuracy', cv=cv_outer, n_jobs=-1)\n",
        "  # report performance\n",
        "  print('mse: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGgXG65RwIAn"
      },
      "source": [
        "# grid search\n",
        "# define dataset\n",
        "X, y = get_dataset(X, Y)\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()\n",
        "pyplot.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCQb4VZIVVFd"
      },
      "source": [
        "# non grid search:\n",
        "# define dataset\n",
        "X, y = get_dataset(X, Y)\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()\n",
        "pyplot.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zREitEanNuH"
      },
      "source": [
        "## Q5. Create a column where cpu_utilization < 20 is 0 and cpu_utilization >= 20 as 1. Using this newly created column build a logistic regression. Commment on the evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twy9oe8xnNRM"
      },
      "source": [
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG8Kz56q2VPx"
      },
      "source": [
        "target_threshold = target_transformer.transform([[20]])\n",
        "target_threshold = target_threshold[0]\n",
        "f = lambda x: x>target_threshold\n",
        "Y_cat = f(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsE8yLmWwqhp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS8EonF0wsTz"
      },
      "source": [
        "permutatation importance for regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zhphLf4wzm0"
      },
      "source": [
        "# permutation feature importance with knn for regression\n",
        "# define the model\n",
        "def permutation_importance_regression(model, X, Y, x_cols, scoring, title):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.inspection import permutation_importance\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      X, Y, random_state=0)\n",
        "  # fit the model\n",
        "  model.fit(X_train, y_train)\n",
        "  # perform permutation importance\n",
        "  results = permutation_importance(model, X_train, y_train, scoring=scoring)\n",
        "  # get importance\n",
        "  importance = results.importances_mean\n",
        "  # summarize feature importance\n",
        "  for i,v in enumerate(importance):\n",
        "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "  # plot feature importance\n",
        "  plt.bar([x for x in x_cols], importance)\n",
        "  plt.title(title)\n",
        "  plt.xticks(rotation = 60) # Rotates X-Axis Ticks by 45-degrees\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18, 7)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLZkbebq0hay"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "models = [GradientBoostingRegressor(), KNeighborsRegressor()]\n",
        "scoring='neg_mean_squared_error'\n",
        "title = 'Feature importance of GradientBoostingRegressor' +'scoring ' + scoring\n",
        "permutation_importance_regression(models[0], X, Y, x_cols, scoring, title)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysRLyqmf4Zwi"
      },
      "source": [
        "Base on regression model's permutation importance, the top 5 featurs are:\n",
        "1. write_throughput, \n",
        "2. write_iops, \n",
        "3. read_throughput, \n",
        "4. read_cache_miss,\n",
        "5. read_iops\n",
        "\n",
        "Base on calssification model's permutation importance, the top 5 featurs are:\n",
        " 1. write_iops,   \n",
        " 2. write_throughput, \n",
        " 3. read_throughput, \n",
        " 4. read_cache_miss,\n",
        " 5. read_iops\n",
        "\n",
        "Based on correlation with target cpu_utilization:\n",
        "(run: plot_corr(df, SYSTEM_ID_SELECTED))\n",
        "1. write_throughput     0.774564\n",
        "2. write_iops           0.758773\n",
        "3. read_cache_miss      0.620105\n",
        "4. read_throughput      0.483752\n",
        "5. write_cache_miss     0.453977\n",
        "6. read_iops            0.356857\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OaEhH2m5V1o"
      },
      "source": [
        "print(type(Y_cat), Y_cat.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imhvnfYXc7ea"
      },
      "source": [
        "Permutation Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn3qR1GBc5IX"
      },
      "source": [
        "# permutation importance for classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.model_selection import train_test_split\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y_cat, stratify=Y_cat, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"RF train accuracy: %0.3f\" % rf.score(X_train, y_train))\n",
        "print(\"RF test accuracy: %0.3f\" % rf.score(X_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LMYKjaWkohN"
      },
      "source": [
        "def plot_permutation_importance(X, Y, title):\n",
        "  result = permutation_importance(rf, X, Y, n_repeats=10,\n",
        "                                  random_state=42, n_jobs=2)\n",
        "  sorted_idx = result.importances_mean.argsort()\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.boxplot(result.importances[sorted_idx].T,\n",
        "            vert=False, labels=[x_cols[k] for k in sorted_idx])\n",
        "  ax.set_title(title)\n",
        "  fig.tight_layout()\n",
        "  plt.grid()\n",
        "  fig.set_size_inches(15, 5)\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAXww-FSnJ1J"
      },
      "source": [
        "title = \"Permutation Importances (test set)\"\n",
        "plot_permutation_importance(X_test, y_test, title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szL7NKl_nJ7q"
      },
      "source": [
        "title = \"Permutation Importances (train set)\"\n",
        "plot_permutation_importance(X_train, y_train, title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3ic2XIPYvDY"
      },
      "source": [
        "Trees Feature Importance from Mean Decrease in Impurity (MDI)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFvl_HPrYsAi"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y_cat, stratify=Y_cat, random_state=42)\n",
        "feature_names = x_cols\n",
        "forest = RandomForestClassifier(random_state=0)\n",
        "\n",
        "forest.fit(X_train, y_train)\n",
        "import time\n",
        "start_time = time.time()\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([\n",
        "    tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Elapsed time to compute the importances: \"\n",
        "      f\"{elapsed_time:.3f} seconds\")\n",
        "import pandas as pd\n",
        "forest_importances = pd.Series(importances, index=feature_names)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=std, ax=ax)\n",
        "ax.set_title(\"Feature importances using MDI\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdkH8I0faCK8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY1SniiAaGwQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GLu3TcwaxSw"
      },
      "source": [
        "[forest_importances.values, std]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzzNQiOavOf6"
      },
      "source": [
        "# Use LDA to reduce dimensitionality and visualise\n",
        "# Use permutation to decide feature importance "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bjufSXFnND8"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v13ctAru_D86"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hBbzJ8G3Tht"
      },
      "source": [
        "# Get orignal target w/o transformation from df\n",
        "def get_target(df, systemId_selected, numerical_transform_cols):\n",
        "  if systemId_selected[0] == 'All':\n",
        "    df_tmp = df[numerical_transform_cols]\n",
        "  else:  \n",
        "    df_tmp = df[df['systemId']==systemId_selected[0]][numerical_transform_cols]\n",
        "  return df_tmp.values\n",
        "\n",
        "y_real = get_target(df, SYSTEM_ID_SELECTED, 'cpu_utilization' )\n",
        "_= plt.hist(y_real)\n",
        "\n",
        "\n",
        "_=plt.hist(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd5FUhHC2JCb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu0fFZWt_MB1"
      },
      "source": [
        "f1 = lambda x: x>20\n",
        "y_real_cat = f1(y_real)*1\n",
        "_= plt.hist(y_real_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Bd9EmonM2_"
      },
      "source": [
        "_= plt.hist(Y_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f4_54yf88xW"
      },
      "source": [
        "# Use LogisticRegression for classification, DecisionTreeClassifier for feature selection\n",
        "\n",
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        "\n",
        "n_features_to_select = [2, len(x_cols)]\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset(X, Y):\n",
        "\tX, y = X, Y\n",
        "\treturn X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(n_features_to_select[0], n_features_to_select[1]):\n",
        "\t\trfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
        "\t\tmodel = LogisticRegression()\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        "\n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        "\n",
        "# define dataset\n",
        "X, y = get_dataset(X, Y_cat)\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWPq13WoFkwb"
      },
      "source": [
        "## Q6. Fit a simple decision tree regressor to this data. Comment about the fit of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlFfSEKWp2lM"
      },
      "source": [
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6TorxADsQHj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm-XHcuoFzD9"
      },
      "source": [
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        "\n",
        "n_features_to_select = [2, len(x_cols)]\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset(X, Y):\n",
        "\tX, y = X, Y\n",
        "\treturn X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(n_features_to_select[0], n_features_to_select[1]):\n",
        "\t\trfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
        "\t\tmodel = DecisionTreeClassifier()\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        "\n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        "\n",
        "# define dataset\n",
        "X, y = get_dataset(X, Y_cat)\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISHSD3FTQV2H"
      },
      "source": [
        "## Q7. Fit a Random forest regressor. Compare this with simple dicision tree. If Random forest is better then why"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvhtU37asU1f"
      },
      "source": [
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dErQUbu48RXx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deaH7RfM8ScG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GExRlPXA8RjC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq5mWNf3QkUD"
      },
      "source": [
        "# Use RandomForestClassifier for classification, DecisionTreeClassifier for feature selection\n",
        "# TODO: add f1-score for biased classification\n",
        "# explore the number of selected features for RFE\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from matplotlib import pyplot\n",
        "\n",
        "n_features_to_select = [2, len(x_cols)]\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset(X, Y):\n",
        "\tX, y = X, Y\n",
        "\treturn X, y\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor i in range(n_features_to_select[0], n_features_to_select[1]):\n",
        "\t\trfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
        "\t\tmodel = RandomForestClassifier()\n",
        "\t\tmodels[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "\treturn models\n",
        "\n",
        "# evaluate a give model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\treturn scores\n",
        "\n",
        "# define dataset\n",
        "X, y = get_dataset(X, Y_cat)\n",
        "# get the models to evaluate\n",
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\tscores = evaluate_model(model, X, y)\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0iYXqC-8WF2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXGqNRcp8VIo"
      },
      "source": [
        "#GridSerachCV for hyperparameters tuning\n",
        "\n",
        "# manual nested cross-validation for random forest on a classification dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
        "\n",
        "\n",
        "def gridsearch_CV_run(X, Y, scoring, space):\n",
        "  # configure the cross-validation procedure\n",
        "  cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "  # enumerate splits\n",
        "  outer_recall, outer_precision, outer_f1, outer_accuracy = list(),list(),list(),list()\n",
        "  for train_ix, test_ix in cv_outer.split(X):\n",
        "    # split data\n",
        "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "    y_train, y_test = Y[train_ix], Y[test_ix]\n",
        "    # configure the cross-validation procedure\n",
        "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "    # define the model\n",
        "    model = RandomForestClassifier(random_state=1)\n",
        "    # define search\n",
        "    search = GridSearchCV(model, space, scoring=scoring, cv=cv_inner, refit=True)\n",
        "    # execute search\n",
        "    result = search.fit(X_train, y_train)\n",
        "    # get the best performing model fit on the whole training set\n",
        "    best_model = result.best_estimator_\n",
        "    # evaluate model on the hold out dataset\n",
        "    yhat = best_model.predict(X_test)\n",
        "\n",
        "    # evaluate the model\n",
        "    acc = accuracy_score(y_test, yhat)\n",
        "    outer_accuracy.append(acc)\n",
        "\n",
        "    precision = precision_score(y_test, yhat)\n",
        "    outer_precision.append(precision)\n",
        "\n",
        "    f1 = f1_score(y_test, yhat)\n",
        "    outer_f1.append(f1)\n",
        "\n",
        "    recall = recall_score(y_test, yhat)\n",
        "    outer_recall.append(recall)\n",
        "\n",
        "    # report progress\n",
        "    print('est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
        "    print('acc=%.3f, precision=%.3f, recall=%.3f, f1=%.3f, ' % (acc, precision, recall, f1 ))\n",
        "\n",
        "  # summarize the estimated performance of the model\n",
        "  print(f'outer_accuracy: %.3f (%.3f)' % (mean(outer_accuracy), std(outer_accuracy)))\n",
        "  print(f'outer_precision: %.3f (%.3f)' % (mean(outer_precision), std(outer_precision)))\n",
        "  print(f'outer_recall: %.3f (%.3f)' % (mean(outer_recall), std(outer_recall)))\n",
        "  print(f'outer_f1: %.3f (%.3f)' % (mean(outer_f1), std(outer_f1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAQNxb2a834h"
      },
      "source": [
        "scoring = 'f1' #'accuracy'\n",
        "# define search space\n",
        "space = dict()\n",
        "space['n_estimators'] = [10, 100, 500]\n",
        "space['max_features'] = [1, 2, 4, 6]\n",
        "\n",
        "gridsearch_CV_run(X, Y_cat, scoring, space)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EmvM82G8VT1"
      },
      "source": [
        "X.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_oc59AaQl_2"
      },
      "source": [
        "## Q8. How do improve the accuracy of Random forest regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPz1m2MCQs5C"
      },
      "source": [
        "GradientBoost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el_i1OMb9Gmh"
      },
      "source": [
        "#GridSerachCV for hyperparameters tuning\n",
        "\n",
        "# manual nested cross-validation for random forest on a classification dataset\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
        "\n",
        "\n",
        "def gridsearch_CV_GBoost(X, Y, scoring, space):\n",
        "  # configure the cross-validation procedure\n",
        "  cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "  # enumerate splits\n",
        "  outer_recall, outer_precision, outer_f1, outer_accuracy = list(),list(),list(),list()\n",
        "  for train_ix, test_ix in cv_outer.split(X):\n",
        "    # split data\n",
        "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "    y_train, y_test = Y[train_ix], Y[test_ix]\n",
        "    # configure the cross-validation procedure\n",
        "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "    # define the model\n",
        "    model = GradientBoostingClassifier(random_state=1)\n",
        "    # define search\n",
        "    search = GridSearchCV(model, space, scoring=scoring, cv=cv_inner, refit=True)\n",
        "    # execute search\n",
        "    result = search.fit(X_train, y_train)\n",
        "    # get the best performing model fit on the whole training set\n",
        "    best_model = result.best_estimator_\n",
        "    # evaluate model on the hold out dataset\n",
        "    yhat = best_model.predict(X_test)\n",
        "\n",
        "    # evaluate the model\n",
        "    acc = accuracy_score(y_test, yhat)\n",
        "    outer_accuracy.append(acc)\n",
        "\n",
        "    precision = precision_score(y_test, yhat)\n",
        "    outer_precision.append(precision)\n",
        "\n",
        "    f1 = f1_score(y_test, yhat)\n",
        "    outer_f1.append(f1)\n",
        "\n",
        "    recall = recall_score(y_test, yhat)\n",
        "    outer_recall.append(recall)\n",
        "\n",
        "    # report progress\n",
        "    print('est=%.3f, cfg=%s' % (result.best_score_, result.best_params_))\n",
        "    print('acc=%.3f, precision=%.3f, recall=%.3f, f1=%.3f, ' % (acc, precision, recall, f1 ))\n",
        "\n",
        "  # summarize the estimated performance of the model\n",
        "  print(f'outer_accuracy: %.3f (%.3f)' % (mean(outer_accuracy), std(outer_accuracy)))\n",
        "  print(f'outer_precision: %.3f (%.3f)' % (mean(outer_precision), std(outer_precision)))\n",
        "  print(f'outer_recall: %.3f (%.3f)' % (mean(outer_recall), std(outer_recall)))\n",
        "  print(f'outer_f1: %.3f (%.3f)' % (mean(outer_f1), std(outer_f1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzF3EJj2RQK6"
      },
      "source": [
        "\n",
        "scoring = 'f1' #'accuracy'\n",
        "# define search space\n",
        "space = dict()\n",
        "space['n_estimators'] = [10, 100, 500]\n",
        "space['max_features'] = [1, 2, 4, 6]\n",
        "parameters = {'learning_rate': [0.1],\n",
        "              'subsample'    : [0.9, 0.5],\n",
        "              'n_estimators' : [100, 500],\n",
        "              'max_depth'    : [2, 4, 6, 8],\n",
        "              'max_features' : [2, 4, 6]\n",
        "              }\n",
        "gridsearch_CV_GBoost(X, Y_cat, scoring, parameters)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqn9vA-QdH8L"
      },
      "source": [
        "# 2nd time run\n",
        "# TODO: start with faster HalvingGridSearchCV, Try XGBoost iso GradientBoost\n",
        "scoring = 'f1' #'accuracy'\n",
        "# define search space\n",
        "space = dict()\n",
        "space['n_estimators'] = [10, 100, 500]\n",
        "space['max_features'] = [1, 2, 4, 6]\n",
        "parameters = {'learning_rate': [0.1],\n",
        "              'n_estimators' : [100, 500],\n",
        "              'max_depth'    : [2, 4, 6, 8],\n",
        "              'max_features' : [2, 4, 6]\n",
        "              }\n",
        "gridsearch_CV_GBoost(X, Y_cat, scoring, parameters)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhUdpuvqi0QC"
      },
      "source": [
        "# 3rd time run\n",
        "# TODO: start with faster HalvingGridSearchCV, Try XGBoost iso GradientBoost\n",
        "scoring = 'f1' #'accuracy'\n",
        "# define search space\n",
        "parameters = {'learning_rate': [0.1],\n",
        "              'n_estimators' : [1000],\n",
        "              'max_depth'    : [2, 4],\n",
        "              'max_features' : [2, 4, 6]\n",
        "              }\n",
        "gridsearch_CV_GBoost(X, Y_cat, scoring, parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1BGrdNA8gSE"
      },
      "source": [
        "## Q9. Cluster the input variables using KMeans and GMM.\n",
        "       \n",
        "1.   Draw the contour plots\n",
        "2.   Explain the hyper-parameters you choose and why?\n",
        "\n",
        "use t-SNE for visualization\n",
        "\n",
        "PCA  + K-means OR DBSCAN\n",
        "1. PCA + cluster\n",
        "2. DBSCAN\n",
        "2. Clustering on the output of t-SNE with 80 perplexity; beware, t-SNE can produce \"fake\" patterns!\n",
        "If we would instead further increase the perplexity, the uniformity would increase, and the separation would reduce again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPsuDzmHsXJ8"
      },
      "source": [
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auoYiJQXTNNW"
      },
      "source": [
        "#First, decide how many features wed like to keep based on the cumulative variance plot.\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "print(len(pca.explained_variance_), pca.explained_variance_) \n",
        "print(len(pca.explained_variance_ratio_), pca.explained_variance_ratio_)\n",
        "print(pca.explained_variance_ratio_.cumsum())\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1, X.shape[1]+1), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
        "plt.title('explained variance by components')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KZwtbRgX1Pc"
      },
      "source": [
        "# We keep the first 4 components to get a cumulative variance of 86 per cent;\n",
        "from sklearn.decomposition import PCA\n",
        "num_components_pca = 4\n",
        "pca = PCA(n_components = num_components_pca)\n",
        "pca.fit(X)\n",
        "pca_out = pca.transform(X)\n",
        "score_pca = pca.transform(X)\n",
        "print(pca_out.shape, X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y--UyOYJrjbS"
      },
      "source": [
        "visualize clusters with PCA output as input to KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wc9-QorriIb"
      },
      "source": [
        "def plot_cluster_mesh(data, n_clusters, use_PCA, title):\n",
        "  if use_PCA:\n",
        "    reduced_data = PCA(n_components=2).fit_transform(data)\n",
        "  else:\n",
        "    reduced_data = data  \n",
        "  kmeans = KMeans(init=\"k-means++\", n_clusters=n_clusters, n_init=10)\n",
        "  kmeans.fit(reduced_data)\n",
        "\n",
        "  # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
        "  h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
        "  y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "  # Obtain labels for each point in mesh. Use last trained model.\n",
        "  Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "  # Put the result into a color plot\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  plt.figure(1)\n",
        "  plt.clf()\n",
        "  plt.imshow(Z, interpolation=\"nearest\",\n",
        "            extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
        "            cmap=plt.cm.Paired, aspect=\"auto\", origin=\"lower\")\n",
        "\n",
        "  plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
        "  # Plot the centroids as a white X\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", s=169, linewidths=3,\n",
        "              color=\"w\", zorder=10)\n",
        "  plt.title(title+ \" K-means clustering\"\n",
        "            \"Centroids are marked with white cross\")\n",
        "  plt.xlim(x_min, x_max)\n",
        "  plt.ylim(y_min, y_max)\n",
        "  plt.xticks(())\n",
        "  plt.yticks(())\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxTE69T1smwV"
      },
      "source": [
        "n_clusters = 4\n",
        "use_PCA = False\n",
        "title = 'Not use PCA output;'\n",
        "plot_cluster_mesh(X[:,:2], n_clusters, use_PCA, title)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf9LIQgcsm5u"
      },
      "source": [
        "n_clusters = 4\n",
        "use_PCA = True\n",
        "title = 'use PCA output;'\n",
        "# TODO: add x_cols[:2]\n",
        "plot_cluster_mesh(X[:,:2], n_clusters, use_PCA, title)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CDc4s4Wt_dg"
      },
      "source": [
        "x_cols[:2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_3WcMeo9FaA"
      },
      "source": [
        "# determine the optimal number of clusters;\n",
        "# apply Elbow method by plotting WCSS vs K \n",
        "def plot_kmeans_inertia(title, wcss):\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.plot(range(1,num_clusters), wcss, marker='o', linestyle='--')\n",
        "  plt.xlabel('Number of Clusters')\n",
        "  plt.ylabel('WCSS')\n",
        "  plt.title(title)\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "def plot_elbow(data, title, num_clusters, random_state):\n",
        "  wcss = [] \n",
        "  for i in range(1, num_clusters):\n",
        "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state=random_state)\n",
        "    kmeans.fit(data)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "  plot_kmeans_inertia(title, wcss)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct6LA6O6Zka6"
      },
      "source": [
        "title = 'K-Means without PCA'\n",
        "num_clusters = 27\n",
        "random_state_kmeans = 0\n",
        "plot_elbow(X, title, num_clusters, random_state_kmeans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF16jNOxb973"
      },
      "source": [
        "title = f'K-Means using top {num_components_pca} components after PCA transform'\n",
        "num_clusters = 27\n",
        "random_state_kmeans = 0\n",
        "plot_elbow(pca_out, title, num_clusters, random_state_kmeans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW9qQ7qVBY5r"
      },
      "source": [
        "From Elbow plot we can set final number of clusters as 4 where the WCSS starts decrease slower."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDzhKQzmBlV4"
      },
      "source": [
        "# plot 3D of clusters \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def plot_3d(X, opt_num_clusters, index_3d, axes_labels, title):\n",
        "\n",
        "  estimators = [('kmeans_opt_num_clusters', KMeans(n_clusters=opt_num_clusters))]\n",
        "\n",
        "  fignum = 1\n",
        "  for name, est in estimators:\n",
        "      fig = plt.figure(fignum, figsize=[8, 8])\n",
        "      ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
        "      est.fit(X)\n",
        "      labels = est.labels_\n",
        "\n",
        "      ax.scatter(X[:, index_3d[0]], X[:, index_3d[1]], X[:, index_3d[2]],\n",
        "                c=labels.astype(float), edgecolor='k')\n",
        "\n",
        "      ax.w_xaxis.set_ticklabels([])\n",
        "      ax.w_yaxis.set_ticklabels([])\n",
        "      ax.w_zaxis.set_ticklabels([])\n",
        "      ax.set_xlabel(axes_labels[0])\n",
        "      ax.set_ylabel(axes_labels[1])\n",
        "      ax.set_zlabel(axes_labels[2])\n",
        "      ax.set_title(title)\n",
        "      ax.dist = 12\n",
        "      fignum = fignum + 1\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Aav-WqoCdDt"
      },
      "source": [
        "# K-Means not using PCA outputs\n",
        "opt_num_clusters = 3\n",
        "title = f'{opt_num_clusters} cluster K-Means without using PCA outputs'\n",
        "index_3d = [0, 1, 2]\n",
        "plot_3d(X, opt_num_clusters, index_3d, x_cols, title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGcHqVpJKrs9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZIJfzsiMDPH"
      },
      "source": [
        "# plot 2d of clusters \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def plot_2d(X, x_index_col, opt_num_clusters, axes_labels, title):\n",
        "  kmeans = KMeans(n_clusters = opt_num_clusters, init = \"k-means++\", random_state = 0)\n",
        "  y_kmeans = kmeans.fit_predict(X)\n",
        "  color_list = ['red', 'blue', 'green', 'yellow', 'violet']\n",
        "  plt.figure(figsize=(10,8))\n",
        "  for label in range(opt_num_clusters):\n",
        "    color = color_list[label]\n",
        "    x = X[y_kmeans == label, x_index_col[0]]\n",
        "    y = X[y_kmeans == label, x_index_col[1]]\n",
        "    plt.scatter(x, y, s = 60, c=color, label = f'Cluster{label}')\n",
        "  plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = 'black', label = 'Centroids')\n",
        "  plt.xlabel(axes_labels[0]) \n",
        "  plt.ylabel(axes_labels[1]) \n",
        "  plt.legend() \n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2GHeblSNSX4"
      },
      "source": [
        "# 2-d plot of K-Means using PCA outputs\n",
        "index_col = [0, 1]\n",
        "opt_num_clusters = 3\n",
        "title = f'{opt_num_clusters} cluster K-Means using top {num_components_pca} components after PCA transform'\n",
        "axes_labels = [f'pca component {k}' for k in [1,2,3]]\n",
        "plot_2d(pca_out, index_col, opt_num_clusters, axes_labels, title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsGi5PwkNYwL"
      },
      "source": [
        "# 2-d plot of K-Means not using PCA outputs\n",
        "index_col = [0, 1]\n",
        "opt_num_clusters = 3\n",
        "title = f'{opt_num_clusters} cluster K-Means not using PCA transform'\n",
        "plot_2d(pca_out, index_col, opt_num_clusters, x_cols, title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MZTqB2GcvN3"
      },
      "source": [
        "DBSCAN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgXgk9-duKV"
      },
      "source": [
        "check_data_dims()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnxp-FHkd55G"
      },
      "source": [
        "# Use DBSCAN to estimate number of clusters\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "\n",
        "# Compute DBSCAN\n",
        "db = DBSCAN(eps=0.7, min_samples=20).fit(pca_out)\n",
        "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True\n",
        "labels = db.labels_\n",
        "\n",
        "# Number of clusters in labels, ignoring noise if present.\n",
        "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise_ = list(labels).count(-1)\n",
        "\n",
        "print('Estimated number of clusters: %d' % n_clusters_)\n",
        "print('Estimated number of noise points: %d' % n_noise_)\n",
        "print(\"Silhouette Coefficient: %0.3f\"\n",
        "      % metrics.silhouette_score(X, labels))\n",
        "\n",
        "# Black removed and is used for noise instead.\n",
        "unique_labels = set(labels)\n",
        "for k in unique_labels:\n",
        "  print(k,np.sum(labels==k))\n",
        "\n",
        "print(f'unique_labels={unique_labels}, labels.shape={labels.shape}')\n",
        "colors = [plt.cm.Spectral(each)\n",
        "          for each in np.linspace(0, 1, len(unique_labels))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf2Lih1GNY2H"
      },
      "source": [
        "\n",
        "for k, col in zip(unique_labels, colors):\n",
        "    if k == -1:\n",
        "        # Black used for noise.\n",
        "        col = [0, 0, 0, 1]\n",
        "\n",
        "    class_member_mask = (labels == k)\n",
        "\n",
        "    xy = X[class_member_mask & core_samples_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "             markeredgecolor='k', markersize=14)\n",
        "\n",
        "    xy = X[class_member_mask & ~core_samples_mask]\n",
        "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
        "             markeredgecolor='k', markersize=6)\n",
        "\n",
        "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z16snsTXfhFa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUlXEDtrWSWB"
      },
      "source": [
        "Decide number of clusters using Silhouette coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIH_ToL3WcsH"
      },
      "source": [
        "def silhouette_cluster_number(X, title):\n",
        "  from sklearn.cluster import KMeans\n",
        "  from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  import matplotlib.cm as cm\n",
        "  import numpy as np\n",
        "\n",
        "  print(__doc__)\n",
        "\n",
        "  range_n_clusters = [2, 3, 4, 5, 6]\n",
        "\n",
        "  for n_clusters in range_n_clusters:\n",
        "      # Create a subplot with 1 row and 2 columns\n",
        "      fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "      fig.set_size_inches(18, 7)\n",
        "\n",
        "      # The 1st subplot is the silhouette plot\n",
        "      # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "      # lie within [-0.1, 1]\n",
        "      ax1.set_xlim([-0.1, 1])\n",
        "      # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "      # plots of individual clusters, to demarcate them clearly.\n",
        "      ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "      # Initialize the clusterer with n_clusters value and a random generator\n",
        "      # seed of 10 for reproducibility.\n",
        "      clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "      cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "      # The silhouette_score gives the average value for all the samples.\n",
        "      # This gives a perspective into the density and separation of the formed\n",
        "      # clusters\n",
        "      silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "      print(\"For n_clusters =\", n_clusters,\n",
        "            \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "      # Compute the silhouette scores for each sample\n",
        "      sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "      y_lower = 10\n",
        "      for i in range(n_clusters):\n",
        "          # Aggregate the silhouette scores for samples belonging to\n",
        "          # cluster i, and sort them\n",
        "          ith_cluster_silhouette_values = \\\n",
        "              sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "          ith_cluster_silhouette_values.sort()\n",
        "\n",
        "          size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "          y_upper = y_lower + size_cluster_i\n",
        "\n",
        "          color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "          ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                            0, ith_cluster_silhouette_values,\n",
        "                            facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "          # Label the silhouette plots with their cluster numbers at the middle\n",
        "          ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "          # Compute the new y_lower for next plot\n",
        "          y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "      ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "      ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "      ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "      # The vertical line for average silhouette score of all the values\n",
        "      ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "      ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "      ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "      # 2nd Plot showing the actual clusters formed\n",
        "      colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "      ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                  c=colors, edgecolor='k')\n",
        "\n",
        "      # Labeling the clusters\n",
        "      centers = clusterer.cluster_centers_\n",
        "      # Draw white circles at cluster centers\n",
        "      ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                  c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "      for i, c in enumerate(centers):\n",
        "          ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                      s=50, edgecolor='k')\n",
        "\n",
        "      ax2.set_title(\"The visualization of the clustered data.\")\n",
        "      ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "      ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "      plt.suptitle((title + \" Silhouette analysis for KMeans \"\n",
        "                    \", n_clusters = %d\" % n_clusters),\n",
        "                  fontsize=14, fontweight='bold')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3yzNBGiYbt6"
      },
      "source": [
        "title = 'PCA output is used as input;'\n",
        "silhouette_cluster_number(pca_out, title)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EyDPSKSZEee"
      },
      "source": [
        "title = 'PCA not used;'\n",
        "silhouette_cluster_number(X, title)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4CFIQef3Jmo"
      },
      "source": [
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwZTsJzB3334"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}