{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Interview for DataScience - 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrankGangWang/AppliedML_Python_Coursera/blob/master/Copy_of_Copy_of_Interview_for_DataScience_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqlyZk0Z-kf0"
      },
      "source": [
        "**Interview questions to test data science skills**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aR3yaVg-6FC"
      },
      "source": [
        "# Let's load the data from public GitHub account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMU3MFw0-ZRS"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import math\n",
        "import statsmodels.api as sm\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDSqgzAExHrS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VEsPDVv-jp-"
      },
      "source": [
        "# load data and check condation of NA\n",
        "df = pd.read_pickle(\"https://github.com/manjiler/interview_for_datascience/raw/master/interview_storage.pkl\")\n",
        "df['timestamp'] = df['timestamp']/1e3\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcKfvKIUxHXP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qogfkFckDERq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb313f9a-fce2-4481-9e12-c5e5d113776d"
      },
      "source": [
        "# check missing data; result: no missing data\n",
        "print(f'There are {df.isna().sum().sum()} NA data')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 0 NA data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANYv7bdPEdpX"
      },
      "source": [
        "## Now that the data is loaded. let's begin!!!\n",
        "\n",
        "# **About the data**\n",
        "\n",
        "---\n",
        "This is time series data for one month collected for N number of devices\n",
        "\n",
        "Columns\n",
        "\n",
        "systemId - Device name\n",
        "\n",
        "timestamp - epoch time when the sensor data was collected\n",
        "\n",
        "model_type - Different versions/release/model of the device (similar to mobile models)\n",
        "\n",
        "cpu_utilization - this percentage of how much the CPU is used on the device.\n",
        "\n",
        "read_cache_miss - Percentage of read that were not present in the Cache\n",
        "\n",
        "write_cache_miss - Percentage of write that were not present in the Cache\n",
        "\n",
        "read_iops -  Number of read IOs per second (Input/Output)\n",
        "\n",
        "write_iops -  Nummber of write IOs per second (Input/Output)\n",
        "\n",
        "read_throughput - the read bandwidth per second (Units kbps)\n",
        "\n",
        "write_throughput - the write bandwidth per second (Units kbps)\n",
        "\n",
        "read_iosz - the block size for read Input/Output operations\n",
        "\n",
        "write_iosz - the block size for write Input/Output operations\n",
        "\n",
        "\n",
        "y -> cpu_utilization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw8r7vKGEklC"
      },
      "source": [
        "## Q1. Do an EDA on the data, correlation plots, features that might be important for the modeling. Share your observations. Comment on how the data looks from modeling perspective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y479tMw7kg3v"
      },
      "source": [
        "# RUN this cell\n",
        "# 1. Add datetime and set it as index\n",
        "# 2. Sort by datetime\n",
        "# 3. To help finding time periodicity (hourly/daily/weekly/monthly/yearly), \n",
        "#    add additional features like \"time of day/week\" by appling cos and sin to timestamp \n",
        "df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', origin='unix')\n",
        "df.set_index(df['datetime'], inplace=True)\n",
        "\n",
        "# IMPORTANT step: sort by time index\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "# add sin/cos for day/hour\n",
        "hour = 60*60\n",
        "day = 24*hour\n",
        "week = 7*day\n",
        "week_offset = 3*day # as epoch 0 is Thursday 1970-01-01, and leap seconds 23:59:60\n",
        "df['Week_Sin'] = np.sin((df['timestamp']+week_offset) * (2 * np.pi / week))\n",
        "df['Week_Cos'] = np.cos((df['timestamp']+week_offset) * (2 * np.pi / week))\n",
        "df['Day_Sin'] = np.sin(df['timestamp'] * (2 * np.pi / day))\n",
        "df['Day_Cos'] = np.cos(df['timestamp'] * (2 * np.pi / day))\n",
        "df['Hour_Sin'] = np.sin(df['timestamp'] * (2 * np.pi / hour))\n",
        "df['Hour_Cos'] = np.cos(df['timestamp'] * (2 * np.pi / hour))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAW5jv6hd9LC"
      },
      "source": [
        "# Constants: \n",
        "# size of data for train, cross validation, test\n",
        "TEST_SIZE = 0.2\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxKVbmg5dj36"
      },
      "source": [
        "# apply transformation to make data more Gaussian-like \n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "#from sklearn.preprocessing import QuantileTransformer\n",
        "def transform_features(df, systemId_selected, numerical_transform_cols):\n",
        "  if systemId_selected[0] == 'All':\n",
        "    df_tmp = df[numerical_transform_cols]\n",
        "  else:  \n",
        "    df_tmp = df[df['systemId']==systemId_selected[0]][numerical_transform_cols]\n",
        "  print(f'*** Inside transform_features(): df_tmp.shape={df_tmp.shape}, columns={df_tmp.columns}')\n",
        "\n",
        "  data = df_tmp.values\n",
        "  power = PowerTransformer(method='box-cox', standardize=True)\n",
        "  data = power.fit_transform(data)\n",
        "  # convert the array back to a dataframe\n",
        "  dataset = pd.DataFrame(data)\n",
        "  # histograms of the variables\n",
        "  dataset.hist()\n",
        "  plt.show()\n",
        "\n",
        "  # histogram of the transformed data\n",
        "  print(f'*** After transform_features(): dataset.shape={dataset.shape}, columns={dataset.columns}')\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yWITbVSdyc_"
      },
      "source": [
        "# compare correlation before AND after transformation to normalize distribution\n",
        "systemId_selected = ['sys1']\n",
        "numerical_transform_cols = ['cpu_utilization',\n",
        "       'read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops',\n",
        "       'read_throughput', 'write_throughput', 'read_iosz', 'write_iosz']\n",
        "transform_features(df, systemId_selected, numerical_transform_cols)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXH3BgBGPFx1"
      },
      "source": [
        "# compare correlation before AND after transformation to normalize distribution\n",
        "systemId_selected = ['All']\n",
        "numerical_transform_cols = ['cpu_utilization',\n",
        "       'read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops',\n",
        "       'read_throughput', 'write_throughput', 'read_iosz', 'write_iosz']\n",
        "transform_features(df, systemId_selected, numerical_transform_cols)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzbhKHUBe0FY"
      },
      "source": [
        "# plot histogram\n",
        "def plot_df_hist(df, systemId_selected, title_str):\n",
        "  if systemId_selected[0] == 'All':\n",
        "    df_tmp = df\n",
        "  else:  \n",
        "    df_tmp = df[df['systemId']==systemId_selected[0]]\n",
        "  df_tmp.hist()\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18.5, 8.5)\n",
        "  fig.suptitle(f'{title_str}, systemId {systemId_selected[0]}')\n",
        "  plt.show()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpPqphCiCdoB"
      },
      "source": [
        "# plot hist of all systemIds: a few columns are not close to Gaussian distribution\n",
        "systemId_selected = ['All']\n",
        "title_str = 'Before transformation!'\n",
        "plot_df_hist(df, systemId_selected, title_str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9hSsRNmVlGN"
      },
      "source": [
        "# plot hist of a single systemIds: a few columns are not close to Gaussian distribution\n",
        "# write_iops is more Gaussian than all systemId case;\n",
        "# Conclusion: need transform_features()\n",
        "systemId_selected = ['sys1']\n",
        "title_str = 'Before transformation!'\n",
        "plot_df_hist(df, systemId_selected, title_str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lswKwUMUEDcd"
      },
      "source": [
        "transform_features(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_PLsg9bhyfx"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_corr(df, systemId_selected):\n",
        "  if systemId_selected[0] == 'All':\n",
        "    df_tmp = df\n",
        "  else:  \n",
        "    df_tmp = df[df['systemId']==systemId_selected[0]]\n",
        "  sns.set_theme(style=\"white\")\n",
        "  # Compute the correlation matrix\n",
        "  corr = df_tmp.corr()\n",
        "  # Generate a mask for the upper triangle\n",
        "  mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "  # Set up the matplotlib figure\n",
        "  f, ax = plt.subplots(figsize=(15, 9))\n",
        "\n",
        "  # Generate a custom diverging colormap\n",
        "  cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "  # Draw the heatmap with the mask and correct aspect ratio\n",
        "  sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.9, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "  f.suptitle(f'systemId {systemId_selected[0]}')\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhezEvcWd8ci"
      },
      "source": [
        "# compare a single selected systemId AGAINST whole systemIds\n",
        "systemId_selected = ['sys1']\n",
        "plot_corr(df, systemId_selected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycSgHqE5LXMR"
      },
      "source": [
        "systemId_selected = ['sys1']\n",
        "plot_corr(df, systemId_selected)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QquxwQdyOfVe"
      },
      "source": [
        "**Observation** \n",
        "After comparison Correlation plots of two cases:\n",
        "-- Case_SINGLE: Only extract a single systemId (here 'sys1') case; \n",
        "-- Case_ALL:  Use all systemIds as a whole;\n",
        "\n",
        "Conclusion:\n",
        "1. timestamp has low correlation with all the rest features; and is less useful directly;\n",
        "2. Day_Cos and Day_Sin have good correlation with a few columns and shall help prediction;\n",
        "\n",
        "\n",
        "Case_SINGLE clearly exposes a larger nuber of columns having high correlations with the target 'cpu_utilization' than  that of Case_ALL. \n",
        "\n",
        "1.1 columns having good correlation with 'cpu_utilization':\n",
        "Case_SINGLE: ['write_throughput', 'write_iops', 'read_cache_miss', 'read_throughput', 'read_iops', 'write_cache_miss']\n",
        "\n",
        "Case_ALL: ['read_throughput', 'read_cache_miss'] \n",
        "['write_cache_miss',  'read_iops', 'write_throughput', 'write_iosz' ]\n",
        "\n",
        "1.1 columns having good correlation with Day_sin:  \n",
        "Case_SINGLE ['write_cache_miss', 'read_iops', 'write_iosz' ]\n",
        "Case_ALL: None\n",
        "\n",
        "1.2 Day_Cos has good correlation with 2 columns:\n",
        "Case_SINGLE: ['write_iops', 'read_iosz']\n",
        "Case_ALL: None\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3HYKew1UHfa",
        "outputId": "4af862a6-4b90-4bb2-e87e-da61086cdd57"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['systemId', 'timestamp', 'model_type', 'cpu_utilization',\n",
              "       'read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops',\n",
              "       'read_throughput', 'write_throughput', 'read_iosz', 'write_iosz',\n",
              "       'datetime', 'Week_Sin', 'Week_Cos', 'Day_Sin', 'Day_Cos', 'Hour_Sin',\n",
              "       'Hour_Cos'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArbVaWPQL-Dm"
      },
      "source": [
        "systemId_selected = ['All']\n",
        "plot_corr(df, systemId_selected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFCeFr9XoPbA"
      },
      "source": [
        "# normalize, average, and plot\n",
        "\n",
        "def plot_columns_average(df, AVERAGE_METHOD, Window_Length):\n",
        "  df1 = df[df['systemId']=='sys1']\n",
        "\n",
        "  start_hour = 24*1+15;\n",
        "  interval_hour= 24*20 + 10\n",
        "  start = int(start_hour*hour/(5*60))\n",
        "  stop = int((start_hour + interval_hour)*hour/(5*60))\n",
        "\n",
        "  #x = df1['timestamp']\n",
        "  df1['iops_rw_diff'] = df1['read_iops'] - df1['write_iops']\n",
        "  df1['iops_rw_ratio'] = df1['read_iops']/df1['write_iops']\n",
        "\n",
        "  df1['throughput_rw_diff'] = df1['read_throughput'] - df1['write_throughput']\n",
        "  df1['throughput_rw_ratio'] = df1['read_throughput'] / df1['write_throughput']\n",
        "\n",
        "  cols = ['write_cache_miss',  'read_iops', 'write_iosz', 'cpu_utilization', 'read_cache_miss', \n",
        "          'read_iosz', 'read_throughput', 'write_throughput', 'throughput_rw_diff', 'throughput_rw_ratio',\n",
        "          'write_iops', 'iops_rw_diff', 'iops_rw_ratio', 'Day_Sin', 'Week_Sin']#, 'Hour_Sin']\n",
        "\n",
        "  ### moving average and standarization\n",
        "  cols_to_avg = ['cpu_utilization', 'read_cache_miss', 'write_cache_miss',\n",
        "                'read_iosz', 'write_iosz', 'read_throughput', 'write_throughput', 'read_iops',\n",
        "          'write_iops','throughput_rw_diff', 'throughput_rw_ratio', 'iops_rw_diff', 'iops_rw_ratio']\n",
        "\n",
        "  for f in cols_to_avg:\n",
        "    if AVERAGE_METHOD == 'rolling_moving_average':\n",
        "      df1[f] = df1[f].rolling(window=Window_Length).mean()\n",
        "    elif AVERAGE_METHOD == 'exponential_moving_average':\n",
        "      df1[f] = df1[f].ewm(span=Window_Length, adjust=False).mean()\n",
        "    elif AVERAGE_METHOD == 'cumulative_average': \n",
        "      df1[f] = df1[f].expanding(min_periods=Window_Length, adjust=False).mean()\n",
        "    else:\n",
        "      raise ValueError(f'AVERAGE_METHOD ={AVERAGE_METHOD} is not implemented!')\n",
        "        \n",
        "  cols_normalization = ['cpu_utilization', 'read_cache_miss', 'write_cache_miss', 'read_iosz', 'write_iosz', \n",
        "                        'read_throughput', 'write_throughput', 'read_iops',\n",
        "          'write_iops', 'throughput_rw_diff', 'throughput_rw_ratio', 'iops_rw_diff', 'iops_rw_ratio']\n",
        "\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  autoscaler = StandardScaler()\n",
        "  df1.loc[:, cols_normalization] = autoscaler.fit_transform(df1.loc[:, cols_normalization])\n",
        "\n",
        "  x = df1['datetime']\n",
        "  linepattern = '-+'\n",
        "  fig = 1\n",
        "  num_cols = len(cols)\n",
        "  for f in cols:\n",
        "    y = df1[f]\n",
        "    plt.subplot(num_cols, 1, fig)\n",
        "    plt.plot(x[start:stop], y[start:stop], linepattern)\n",
        "    if f == 'write_iops' or f=='read_iosz':\n",
        "      plt.plot(x[start:stop], df1['Day_Cos'][start-60:stop-60], 'g-o')\n",
        "    else:\n",
        "      plt.plot(x[start:stop], df1['Day_Sin'][start-60:stop-60], 'g-o')\n",
        "\n",
        "    #plt.plot(x[start:stop], df1['Day_Cos'][start:stop], 'y-')\n",
        "    plt.ylabel(f)\n",
        "    fig= fig+1\n",
        "    plt.grid(True)\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(22.5, num_cols*3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKmVk9usLmrc"
      },
      "source": [
        "# most sampling periods are 5 min, so window in mins is Window_Length*5 mins\n",
        "# when Window_Length = int(2*60/5) in samples, it is in time interval of 120min=2hr\n",
        "Window_Length = int(2*60/5) \n",
        "\n",
        "AVERAGE_METHOD_LIST = ['rolling_moving_average', 'exponential_moving_average', 'cumulative_average']\n",
        "AVERAGE_METHOD = AVERAGE_METHOD_LIST[1] \n",
        "plot_columns_average(df, AVERAGE_METHOD, Window_Length)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p0hETXoVx8X"
      },
      "source": [
        "# check outliers; conclusion: no apparent outliers\n",
        "df_stat = df.describe().T\n",
        "df_stat['50% to 75%'] = (df_stat['50%']/df_stat['75%'])\n",
        "df_stat['max to 75%'] = (df_stat['max']/df_stat['75%'])\n",
        "df_stat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oJIeiO45BHR"
      },
      "source": [
        "print(f\" df.shape={df.shape},\\n df.columns={df.columns}\")\n",
        "sysIDs = np.unique(df['systemId'])\n",
        "print(f\" sysIDs shape={sysIDs.shape},\\n sysIDs={sysIDs}\")\n",
        "model_types = np.unique(df['model_type'])\n",
        "print(f\" model_types shape={model_types.shape}, model_types ={model_types}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_jGhaNqH-FO"
      },
      "source": [
        "# RUN:\n",
        "# utility function for feature selection based on correlation  \n",
        "# NOTE: this is done separately for each of systemId, not for all systemIds altogether\n",
        "\n",
        "#label encoder for model\n",
        "def select_feature_by_correlation(df_in, systemId, corr_threshold):\n",
        "  df = df_in.copy()\n",
        "  if len(systemId)>0:\n",
        "    df = df[df['systemId']==systemId]\n",
        "\n",
        "  del df['datetime'] # can not correlate datetime\n",
        "  # StandardScaler Gaussian like features\n",
        "  # LabelEncoder\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  # add labelencoder to systemId and model_type\n",
        "  label_encoder1 = LabelEncoder()\n",
        "  df['systemId'] = label_encoder1.fit_transform(df['systemId'])\n",
        "  label_encoder2 = LabelEncoder()\n",
        "  df['model_type'] = label_encoder2.fit_transform(df['model_type'])\n",
        "  corr = df.corr()\n",
        "  columns = np.full((corr.shape[0],), True, dtype=bool)\n",
        "  for i in range(corr.shape[0]):\n",
        "      for j in range(i+1, corr.shape[0]):\n",
        "          if corr.iloc[i,j] >= corr_threshold:\n",
        "              if columns[j]:\n",
        "                  columns[j] = False\n",
        "  selected_columns = df.columns[columns]\n",
        "  removed_columns = [x for x in df.columns if x in selected_columns]\n",
        "\n",
        "  removed_columns = [x for x in df.columns if x not in selected_columns]\n",
        "\n",
        "  print(f'*** type corr_threshold={type(corr)}')\n",
        "  print(f'df shape={df.shape}; corr shape={corr.shape}; systemId={systemId}')\n",
        "  print(f'corr={corr}\\n')\n",
        "  print(f'systemId={systemId};')\n",
        "  print(f'selected_columns={selected_columns}')\n",
        "  print(f'removed_columns={removed_columns}')\n",
        "\n",
        "  return selected_columns\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkrv845LJCK4"
      },
      "source": [
        "# RUN:\n",
        "# feature selection based on correlation  \n",
        "\n",
        "#systemId = ['sys1']; corr_threshold=0.5;\n",
        "systemId = []; corr_threshold=0.8;\n",
        "pd.set_option('display.max_columns', None)\n",
        "selected_columns = select_feature_by_correlation(df, systemId, corr_threshold)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbCZIVV2HgBW"
      },
      "source": [
        "print((np.unique(df['model_type'])))\n",
        "for sysId in np.unique(df['systemId']):\n",
        "  data = df[df['systemId']==sysId]\n",
        "  x = np.unique(data['model_type'])\n",
        "  print(f'sysId={sysId}, {x}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLTZZghjAi3Y"
      },
      "source": [
        "# GANG WANG\n",
        "# RUN this: select feature based on p-value\n",
        "\n",
        "systemId = 'sys1'\n",
        "data = df[df['systemId']==systemId]\n",
        "print(np.unique(data['model_type'])\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "columns_to_scale=['timestamp', 'cpu_utilization', 'read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput', 'write_throughput', 'read_iosz', 'write_iosz']\n",
        "autoscaler = StandardScaler()\n",
        "data[columns_to_scale] = autoscaler.fit_transform(data[columns_to_scale])\n",
        "\n",
        "y_column = ['cpu_utilization']; \n",
        "x_columns = ['timestamp', 'read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput', 'write_throughput', 'read_iosz','write_iosz'] #, 'datetime']\n",
        "\n",
        "x_in = data[x_columns].values\n",
        "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "label_encoder2 = LabelEncoder()\n",
        "data['model_type'] = label_encoder2.fit_transform(data['model_type'])\n",
        "feature = data['model_type'].values\n",
        "print(\"feature shape BEFORE reshape:\", feature.shape)\n",
        "feature = feature.reshape(feature.shape[0], 1)\n",
        "print(\"feature shape AFTER reshape:\", feature.shape)\n",
        "\n",
        "print(\"x_in shape BEFORE one-hot-encoder:\", x_in.shape)\n",
        "\n",
        "feature = onehot_encoder.fit_transform(feature)\n",
        "print(f'feature shape={feature.shape}')\n",
        "x_in = np.concatenate((x_in, feature), axis=1)\n",
        "print(\"x_in shape AFTER one-hot-encoder: : \", x_in.shape)\n",
        "\n",
        "\n",
        "y_in = data[y_column].values\n",
        "selected_columns = ['timestamp', 'read_cache_miss', 'write_cache_miss', 'read_iops',\n",
        "                    'write_iops', 'read_throughput', 'write_throughput', 'read_iosz',\n",
        "                    'write_iosz', 'model_type'] #, 'datetime']\n",
        "\n",
        "\n",
        "\n",
        "def backwardElimination(x, Y, sl, columns):\n",
        "    print(f'x={x[:2,:]}')\n",
        "    print(f'Y={Y[:2]}')\n",
        "    print(f'len(x[0]) = {len(x[0])}')\n",
        "    print(f'x shape = {x.shape}')\n",
        "\n",
        "    numVars = len(x[0])\n",
        "    for i in range(0, numVars):\n",
        "        regressor_OLS = sm.OLS(Y, x).fit()\n",
        "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
        "        if maxVar > sl:\n",
        "            for j in range(0, numVars - i):\n",
        "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
        "                    x = np.delete(x, j, 1)\n",
        "                    columns = np.delete(columns, j)\n",
        "        print(f'**** backwardElimination i={i}, p-values={regressor_OLS.pvalues}')\n",
        "        print(f'inside loop: x shape = {x.shape}')\n",
        "                    \n",
        "    print(regressor_OLS.summary())\n",
        "    return x, columns\n",
        "\n",
        "SL = 0.05\n",
        "data_modeled, selected_columns = backwardElimination(x_in, y_in, SL, selected_columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxyNL1tpAl7w"
      },
      "source": [
        "df.columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1aqd14xoFIU"
      },
      "source": [
        "# RUN this: utility function for plotting x vs y for a selected systemId  \n",
        "# x MUST be a list with a SINGLE string;  x=['cpu_utilization'];\n",
        "# y can be a list of any number of numerical features;\n",
        "def plot_multx_y(df, systemId, x, y, start_fraction, stop_fraction, linepattern, plot_1_column):\n",
        "  if len(x)>1:\n",
        "    raise ValueError(f'x-axis MUST be a list with a SINGLE string !')\n",
        "  df_temp = df[df['systemId']==systemId]\n",
        "  start = int(len(df_temp)*start_fraction)\n",
        "  stop = int(len(df_temp)*stop_fraction)\n",
        "  len_y = len(y)\n",
        "\n",
        "  if plot_1_column:\n",
        "    num_rows = len_y\n",
        "    num_cols = 1\n",
        "  else:\n",
        "    num_rows = math.ceil(math.sqrt(len_y))\n",
        "    num_cols = math.ceil(len_y/num_rows)\n",
        "\n",
        "  print(f'{len_y} figures, {num_rows} rows, {num_cols} columns')\n",
        "  y_index = 0\n",
        "  for row in range(num_rows):\n",
        "    for col in range(num_cols):\n",
        "      if y_index == (len_y):\n",
        "        break\n",
        "      plt.subplot(num_rows, num_cols, y_index+1)\n",
        "      plt.plot(df_temp[x[0]][start:stop], df_temp[y[y_index]][start:stop], linepattern)\n",
        "      plt.ylabel(f'{y[y_index]}')\n",
        "\n",
        "      plt.grid(True)\n",
        "      y_index = y_index + 1  \n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18.5, 3*num_rows)\n",
        "  fig.suptitle(f'systemId={systemId}, all the X-axes are from the column of {x[0]}')\n",
        "  plt.tight_layout() # 2nd last step in fig setting\n",
        "  fig.subplots_adjust(top=0.88) # last in fig setting\n",
        "  plt.show()\n",
        "  del df_temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQEmSQhlbYDE"
      },
      "source": [
        "# RUN this: plot the 'cpu_utilization' against each of the rest columns\n",
        "systemId_selected = 'sys1'\n",
        "plot_1_column=False; x = ['cpu_utilization']; y = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "#x = ['datetime']; y = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "#x = ['read_iops']; y =['cpu_utilization']\n",
        "plot_multx_y(df, systemId_selected, x ,y, 0.2, 0.3, '+', plot_1_column)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOz1Vvge_UjI"
      },
      "source": [
        "systemId_selected = 'sys1'\n",
        "#x = ['cpu_utilization']; y = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "plot_1_column=True; x = ['datetime']; y = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "#x = ['read_iops']; y =['cpu_utilization']\n",
        "plot_multx_y(df, systemId_selected, x ,y, 0.2, 0.23, '+', plot_1_column)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJKivAI7y2Mm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyKrqN2PBzjT"
      },
      "source": [
        "systemId_selected = 'sys1'\n",
        "#x = ['cpu_utilization']; y = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "plot_1_column=True; x = ['datetime']; y = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops', 'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "#x = ['read_iops']; y =['cpu_utilization']\n",
        "plot_multx_y(df, systemId_selected, x ,y, 0.2, 0.21, '-+', plot_1_column)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gja2Lth0Q_zO"
      },
      "source": [
        "def plot_all_timestamp(df_in, systemId, start_fraction, stop_fraction, linepattern ):\n",
        "  df = df_in\n",
        "  if (systemId[0] != 'all') & len(systemId)==1:\n",
        "    df = df_in[df_in['systemId']==systemId[0]]\n",
        "  start = int(len(df)*start_fraction)\n",
        "  stop = int(len(df)*stop_fraction)\n",
        "\n",
        "  print(f'len df={len(df)}')\n",
        "  fig, axs = plt.subplots(2)\n",
        "  fig.suptitle('timestamp')\n",
        "\n",
        "  axs[0].plot( df['timestamp'][start:stop], linepattern)\n",
        "  axs[0].set_title(f'systemId={systemId}:: series type')\n",
        "\n",
        "  axs[1].plot( np.array(df['timestamp'][start:stop]), linepattern)\n",
        "  axs[1].set_title(f'systemId={systemId}: array type')\n",
        "  fig.set_size_inches(12.5, 7.5)\n",
        "  del df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0rNnGd9S3Cy"
      },
      "source": [
        "plot_all_timestamp(df, ['all'], 0, 1, '+')\n",
        "plot_all_timestamp(df, ['all'], 0, 0.1, '+')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4criErlDVgWW"
      },
      "source": [
        "plot_all_timestamp(df, ['sys1'], 0, 1, '+')\n",
        "plot_all_timestamp(df, ['sys1'], 0, 0.1, '+')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZU4c1oDa2e2"
      },
      "source": [
        "systemId_selected = 'sys2'\n",
        "y = ['cpu_utilization']\n",
        "x = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops',\n",
        "       'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "plot_multx_y(df, systemId_selected, x ,y,'+', 0, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjyp35qibC1Z"
      },
      "source": [
        "systemId_selected = 'sys2'\n",
        "y = ['datetime']\n",
        "x = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops',\n",
        "       'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "plot_multx_y(df, systemId_selected, x ,y,'+', 0, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVh8vQeYp3Mx"
      },
      "source": [
        "def plot_diffby_datetime(df, systemId, x, periods, linepattern):\n",
        "  if type(periods) != int:\n",
        "    raise ValueError(f'periods {periods} must be a single integer!')\n",
        "\n",
        "  df_temp = df[df['systemId']==systemId]\n",
        "  t = df_temp['datetime']\n",
        "    \n",
        "  len_x = len(x)\n",
        "  num_rows = math.ceil(math.sqrt(len_x))\n",
        "  num_cols = math.ceil(len_x/num_rows)\n",
        "  print(f'{len_x} figures, {num_rows} rows, {num_cols} columns')\n",
        "\n",
        "  x_index = 0\n",
        "  for row in range(num_rows):\n",
        "    for col in range(num_cols):\n",
        "      x_array = df_temp[x[x_index]].diff(periods)\n",
        "      if x_index == (len_x):\n",
        "        break\n",
        "\n",
        "      plt.subplot(num_rows, num_cols, x_index+1)\n",
        "      plt.plot( t[periods:], x_array[periods:], linepattern)\n",
        "      plt.ylabel(f'{x[x_index]}')\n",
        "      plt.grid(True)\n",
        "      x_index = x_index + 1  \n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.suptitle(f'systemId={systemId}, time diff {periods}, x axis is df[datetime]')\n",
        "  plt.tight_layout()\n",
        "  fig.subplots_adjust(top=0.88)\n",
        "  fig.set_size_inches(12.5, 3*num_rows)\n",
        "  plt.show()\n",
        "  del df_temp\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju8rxEUXbNh3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mZhM-xNz1K3"
      },
      "source": [
        "def boxplot_diffby(df, systemId, x, periods):\n",
        "  if type(periods) != int:\n",
        "    raise ValueError(f'periods {periods} must be a single integer!')\n",
        "\n",
        "  df_temp = df[df['systemId']==systemId]\n",
        "    \n",
        "  len_x = len(x)\n",
        "  num_rows = math.ceil(math.sqrt(len_x))\n",
        "  num_cols = math.ceil(len_x/num_rows)\n",
        "  print(f'{len_x} figures, {num_rows} rows, {num_cols} columns')\n",
        "\n",
        "  x_index = 0\n",
        "  for row in range(num_rows):\n",
        "    for col in range(num_cols):\n",
        "      x_array = df_temp[x[x_index]].diff(periods)\n",
        "      if x_index == (len_x):\n",
        "        break\n",
        "\n",
        "      plt.subplot(num_rows, num_cols, x_index+1)\n",
        "      plt.boxplot( x_array[periods:])\n",
        "      plt.xlabel(f'{x[x_index]}')\n",
        "      plt.grid(True)\n",
        "      x_index = x_index + 1  \n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.suptitle(f'systemId={systemId}, time diff {periods}, x axis is df[datetime]')\n",
        "  plt.tight_layout()\n",
        "  fig.subplots_adjust(top=0.88)\n",
        "  fig.set_size_inches(12.5, 3*num_rows)\n",
        "  plt.show()\n",
        "  del df_temp\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSMEL6CBtHHK"
      },
      "source": [
        "periods = 1\n",
        "systemId_selected = 'sys2'\n",
        "linepattern = '+'\n",
        "x = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops',\n",
        "       'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "plot_diffby_datetime(df, systemId_selected, x, periods, linepattern)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6PpUpgIzD5N"
      },
      "source": [
        "periods = 1\n",
        "systemId_selected = 'sys2'\n",
        "linepattern = '+'\n",
        "x = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops',\n",
        "       'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "plot_diffby_datetime(df, systemId_selected, x, periods, linepattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-8aj7xE0SX4"
      },
      "source": [
        "periods = 1\n",
        "systemId = 'sys2'\n",
        "x = ['read_cache_miss', 'write_cache_miss', 'read_iops', 'write_iops',\n",
        "       'read_throughput', 'write_throughput', 'read_iosz','write_iosz', 'datetime']\n",
        "boxplot_diffby(df, systemId, x, periods)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnhrFWxjvV3c"
      },
      "source": [
        "df.columns\n",
        "#plot_x_y(df, systemId, x, y, start_fraction, stop_fraction)\n",
        "plot_x_y(df, 'sys1', 'datetime','cpu_utilization',0.2, 0.3, '+')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZX4owfqYqhC"
      },
      "source": [
        "df.columns\n",
        "y = ['cpu_utilization']\n",
        "print(len(y))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg-ETow7YnRf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3QZF7rl9B5W"
      },
      "source": [
        "plot_x_y(df, 'sys1', 'timestamp','cpu_utilization',)\n",
        "plot_all_timestamp(df)\n",
        "plot_sys1_timestamp(df)\n",
        "plot_part_sys1_timestamp(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVHCUaKdxDRw"
      },
      "source": [
        "plot_x_y(df, 'sys1', 'timestamp','cpu_utilization',)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HcP9kuU58A1"
      },
      "source": [
        "plot_all_timestamp(df)\n",
        "plot_sys1_timestamp(df)\n",
        "plot_part_sys1_timestamp(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC6r1OGBfdwW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkrTqqSxxH7T"
      },
      "source": [
        "plot_x_y(df, 'sys1', 'timestamp','cpu_utilization',)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wacBOaBOqdyw"
      },
      "source": [
        "plot_all_timestamp(df)\n",
        "plot_sys1_timestamp(df)\n",
        "plot_part_sys1_timestamp(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjH4osTLmG20"
      },
      "source": [
        "df['timestamp'].hist()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0hR8smNm9bN"
      },
      "source": [
        "df['timestamp'].plot(kind='box')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ma4blmJfPSM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.values\n",
        "X_train, X_test = train_test_split(X, test_size=TEST_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke48eRpwff-n"
      },
      "source": [
        "print(f\"df.shape={df.shape}, type(X)={type(X)}, X.shape={X.shape}\") \n",
        "print(f\"X_train.shape={X_train.shape}, X_test.shape={X_test.shape}\")\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh-aVY29XAO0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SeGiqSHyz72"
      },
      "source": [
        "df.hist()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18.5, 8.5)\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OesKKX99zmFy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZdJu9d4DK8L"
      },
      "source": [
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R04J4kXxwLXy"
      },
      "source": [
        "# StandardScaler Gaussian like features\n",
        "# LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# defiine columns_to_scale: exclude categorical columns\n",
        "columns_to_scale = list(df.columns)\n",
        "columns_to_scale.remove('systemId')\n",
        "columns_to_scale.remove('model_type')\n",
        "print(f'columns_to_scale={columns_to_scale}')\n",
        "\n",
        "autoscaler = StandardScaler()\n",
        "df[columns_to_scale] = autoscaler.fit_transform(df[columns_to_scale])\n",
        "\n",
        "# add labelencoder to systemId and model_type\n",
        "label_encoder1 = LabelEncoder()\n",
        "df['systemId'] = label_encoder1.fit_transform(df['systemId'])\n",
        "label_encoder2 = LabelEncoder()\n",
        "df['model_type'] = label_encoder2.fit_transform(df['model_type'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-tX0p_vCqCr"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a57elgsnyjK-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF5qqPmeErjG"
      },
      "source": [
        "## Q2. Group on systemId and one week's duration and then calculate following custom metrics\n",
        "- std/median on columns read_iops, read_cache_miss\n",
        "- rolling mean on columns write_throughput and write_iosz\n",
        "- exponential moving average on column write_cache_miss and write_iops\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbo5DppFI8Z5"
      },
      "source": [
        "features = [['read_iops', 'read_cache_miss'],\n",
        "            ['write_throughput', 'write_iosz'],\n",
        "            ['write_cache_miss', 'write_iops']]\n",
        "\n",
        "for k in range(len(features)):\n",
        "  print(features[k])\n",
        "  for x in features[k]:\n",
        "    print(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGTrcA4qzwpl"
      },
      "source": [
        "dftemp = df[['systemId', 'read_iops', 'read_cache_miss']]\n",
        "df_grouped = dftemp.groupby('systemId')\n",
        "\n",
        "ct = 0\n",
        "for group_name, df_group in df_grouped:\n",
        "  ct = ct + 1\n",
        "  if ct>2: \n",
        "    break\n",
        "  \n",
        "  df_group = df_group.sort_index()\n",
        "\n",
        "  print(f'\\n\\n ****** {group_name} , shape={df_group.shape}****') \n",
        "  print(df_group.head())\n",
        "\n",
        "  stat = df_group.resample('W', origin='start').sum()\n",
        "  print(f'\\n ****** {group_name} sum, shape={stat.shape}****') \n",
        "  print(stat)\n",
        "\n",
        "  stat = df_group.resample('W', origin='start').median()\n",
        "  print(f'\\n ****** {group_name} median, shape={stat.shape}****') \n",
        "  print(stat)\n",
        "    \n",
        "  stat = df_group.resample('W', origin='start').std()\n",
        "  print(f'\\n ****** {group_name} std, shape={stat.shape}****') \n",
        "  print(stat)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X95XEn3P-NBO"
      },
      "source": [
        "# normalization\n",
        "# defiine columns_to_scale\n",
        "df_plot = df.copy()\n",
        "\n",
        "columns_to_scale = list(df_plot.columns)\n",
        "columns_to_scale.remove('systemId')\n",
        "columns_to_scale.remove('model_type')\n",
        "print(columns_to_scale)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeCApQCy_FfT"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "autoscaler = StandardScaler()\n",
        "df_plot[columns_to_scale] = autoscaler.fit_transform(df_plot[columns_to_scale])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHkU25Ue_rzP"
      },
      "source": [
        "df_plot.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1dscO5lqEYv"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# add labelencoder to systemId and model_type\n",
        "label_encoder1 = LabelEncoder()\n",
        "df_plot['systemId'] = label_encoder1.fit_transform(df_plot['systemId'])\n",
        "label_encoder2 = LabelEncoder()\n",
        "df_plot['model_type'] = label_encoder2.fit_transform(df_plot['model_type'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbdtFZOv21OA"
      },
      "source": [
        "df_plot.head()\n",
        "columns_to_scale = df_plot.columns\n",
        "print(columns_to_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb5g-ThshpIg"
      },
      "source": [
        "plot_features = df_plot[columns_to_scale]\n",
        "plot_features.index = df_plot['timestamp']\n",
        "_ = plot_features.plot(subplots=True)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18.5, 10.5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_9uStkFBSk4"
      },
      "source": [
        "samples = int(24*60/5)\n",
        "plot_features = df_plot[columns_to_scale][:samples]\n",
        "plot_features.index = df_plot['timestamp'][:samples]\n",
        "_ = plot_features.plot(subplots=True)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18.5, 10.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_mF9RDClDYa"
      },
      "source": [
        "# sample period is 5 min\n",
        "sample_period = 5\n",
        "plot_len = int(24*60/5)\n",
        "plot_features = df[columns_to_scale][:plot_len]\n",
        "plot_features.index = df_temp_sysId['timestamp'][:plot_len]\n",
        "_ = plot_features.plot(subplots=True)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18.5, 10.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz4m1j8LrneQ"
      },
      "source": [
        "\n",
        "boxplot = df_temp_sysId.boxplot(column=columns_to_scale)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18.5, 10.5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq-oR6Synoq9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQm1OUQ6wDkt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLLwCg1lo2qO"
      },
      "source": [
        "sample_period = 5\n",
        "plot_len = int(2*24*60/5) # plot 2 day\n",
        "\n",
        "plt.plot(np.array(df_temp_sysId['Day sin'])[:plot_len], '+')\n",
        "plt.plot(np.array(df_temp_sysId['Day cos'])[:plot_len], '+')\n",
        "plt.xlabel('Time [h]')\n",
        "plt.title('Time of day signal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SlxA04JpkXD"
      },
      "source": [
        "plt.plot(np.array(df_temp_sysId['Year sin'])[:plot_len], '+')\n",
        "plt.plot(np.array(df_temp_sysId['Year cos'])[:plot_len], '+')\n",
        "plt.xlabel('Time [h]')\n",
        "plt.title('Time of Year signal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmM33BT-oV9B"
      },
      "source": [
        "df_temp_sysId.describe().T\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V74KW2qUp1Bm"
      },
      "source": [
        "# fft plot TODO: for time ordered and single systemId \n",
        "import tensorflow as tf\n",
        "fft = tf.signal.rfft(df_temp_sysId['cpu_utilization'])\n",
        "f_per_dataset = np.arange(0, len(fft))\n",
        "\n",
        "n_samples_h = len(df_temp_sysId['cpu_utilization'])\n",
        "hours_per_year = 24*365.2524\n",
        "years_per_dataset = n_samples_h/(hours_per_year)\n",
        "\n",
        "f_per_year = f_per_dataset/years_per_dataset\n",
        "plt.step(f_per_year, np.abs(fft))\n",
        "plt.xscale('log')\n",
        "#plt.ylim(0, 400000)\n",
        "plt.xlim([0.1, max(plt.xlim())])\n",
        "plt.xticks([1, 365.2524], labels=['1/Year', '1/day'])\n",
        "_ = plt.xlabel('Frequency (log scale)')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBkUWwHglsuq"
      },
      "source": [
        "cols_std =  ['read_iops', 'read_cache_miss']\n",
        "cols_rolling_mean = ['write_throughput', 'write_iosz']\n",
        "cols_exp_mvavg = ['write_cache_miss', 'write_iops']\n",
        "\n",
        "group_by_systemId = df_temp.groupby(\"systemId\") #, axis=\"columns\"\n",
        "group = group_by_systemId.get_group('sys1')\n",
        "group = group[['timestamp', 'write_throughput', 'write_iosz']]\n",
        "\n",
        "print(f'len ={len(group)}')\n",
        "group.head(8)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v2tIIpH30oO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpwL0lf1304n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMvlqIcDwVoC"
      },
      "source": [
        "group = group.sort_values(by='timestamp')\n",
        "group.head(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRDklfCQwcGf"
      },
      "source": [
        "tmp = group.rolling(2).mean()\n",
        "tmp.head(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tkAAvJ2wKDP"
      },
      "source": [
        "group['rmean_write_throughput'] = tmp['write_throughput']\n",
        "group['rmean_write_iosz'] = tmp['write_iosz']\n",
        "group.head(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVK4DMG6E5DQ"
      },
      "source": [
        "\n",
        "if 0:\n",
        "  summary = []\n",
        "  for name, group in group_by_systemId:\n",
        "    summary.append(\n",
        "      {\n",
        "        name : \n",
        "          {\n",
        "            'length': len(group),\n",
        "            'min': group['read_iops'].min(),\n",
        "            'max': group['read_iops'].max(),\n",
        "            'mean': group['read_iops'].mean(),\n",
        "            'std': group['read_iops'].std(),\n",
        "            'median': group['read_iops'].median()\n",
        "          }\n",
        "      }\n",
        "    )\n",
        "  if 0:\n",
        "    print(f'name={name}, len = {len(group)}\\n')\n",
        "    print('systemId uniques=', np.unique(group['systemId']))\n",
        "    #print(f'group={group.head(2)}\\n')\n",
        "    print(f'summary={summary}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpcmd8lhQwDh"
      },
      "source": [
        "## Q3. Generate a random distribution of samples from data such that each day should contain 12 continous samples and start of the sample should be random with that day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyzNxQmp3OkJ"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBnxojpJ3j8R"
      },
      "source": [
        "# note timestamp is converted to sin cos of day hour so not used here\n",
        "df_X = df[['systemId', 'model_type']].copy()\n",
        "\n",
        "print(f'df_X: {df_X.columns}')\n",
        "print(f'df: {df.columns}')\n",
        "\n",
        "print(f'y: {df_Y}')\n",
        "print(df['cpu_utilization'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyMd1Yi66lAc"
      },
      "source": [
        "# one-hot-encoder of systemId and model_type\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "X = df_X.values\n",
        "X = X.astype(str)\n",
        "\n",
        "print(f'X={X[:3, :]}')\n",
        "print(f'Y={Y[:3]}')\n",
        "print(f'X shape={X.shape}')\n",
        "print(f'Y shape={Y.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsThehZX7zeB"
      },
      "source": [
        "# encode string input values as integers\n",
        "encoded_x = None\n",
        "for i in range(0, X.shape[1]):\n",
        "\tlabel_encoder = LabelEncoder()\n",
        "\tfeature = label_encoder.fit_transform(X[:,i])\n",
        "\tfeature = feature.reshape(X.shape[0], 1)\n",
        "\tonehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "\tfeature = onehot_encoder.fit_transform(feature)\n",
        "\tprint(f'i={i}, shape={feature.shape}')\n",
        "\tif encoded_x is None:\n",
        "\t\tencoded_x = feature\n",
        "\telse:\n",
        "\t\tencoded_x = np.concatenate((encoded_x, feature), axis=1)\n",
        "  \n",
        "print(\"X shape: : \", encoded_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qOasz4I9HQ7"
      },
      "source": [
        "df_X = df.copy()\n",
        "features_added_X = ['timestamp', 'cpu_utilization', 'systemId', 'model_type']\n",
        "for i in features_added_X:\n",
        "  del df_X[i]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSBTdVSU_Wj4"
      },
      "source": [
        "print(df_X.columns)\n",
        "print(df_X.shape)\n",
        "print(encoded_x.shape)\n",
        "df_X.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGQ2QDUA-UQC"
      },
      "source": [
        "# Now encoded_x  contains the (27 for systemId) + (3 for model) = 30 categorical columns \n",
        "# Here, concatenate X's non-categorical 12 columns df_X with encoded_x to get a total of 42 columns\n",
        "X = np.concatenate((encoded_x, df_X.values), axis=1)\n",
        "print(\"X shape: : \", X.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEpxO2G7EBmY"
      },
      "source": [
        "Y = df['cpu_utilization'].values\n",
        "print(\"Y shape: : \", Y.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kX575JMFcdg"
      },
      "source": [
        "## Q4. Fit a linear regression to this data with y as \"cpu_utilization\" column. Comment on the fit of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y62yuBBoFkLN"
      },
      "source": [
        "CV_FOLDS = 2\n",
        "# Regression\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# define base model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "  \n",
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=2, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits = CV_FOLDS)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zREitEanNuH"
      },
      "source": [
        "## Q5. Create a column where cpu_utilization < 20 is 0 and cpu_utilization >= 20 as 1. Using this newly created column build a logistic regression. Commment on the evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twy9oe8xnNRM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bjufSXFnND8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Bd9EmonM2_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWPq13WoFkwb"
      },
      "source": [
        "## Q6. Fit a simple decision tree regressor to this data. Comment about the fit of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm-XHcuoFzD9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISHSD3FTQV2H"
      },
      "source": [
        "## Q7. Fit a Random forest regressor. Compare this with simple dicision tree. If Random forest is better then why"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq5mWNf3QkUD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_oc59AaQl_2"
      },
      "source": [
        "## Q8. How do improve the accuracy of Random forest regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPz1m2MCQs5C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el_i1OMb9Gmh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1BGrdNA8gSE"
      },
      "source": [
        "## Q9. Cluster the input variables using KMeans and GMM.\n",
        "       \n",
        "1.   Draw the contour plots\n",
        "2.   Explain the hyper-parameters you choose and why?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auoYiJQXTNNW"
      },
      "source": [
        "df.plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}