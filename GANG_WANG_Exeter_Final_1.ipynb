{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOioN6n4OAnfJIcL/R5rlWO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrankGangWang/AppliedML_Python_Coursera/blob/master/GANG_WANG_Exeter_Final_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction methods:\n",
        "1. spatial autocorrelation of alist of lags per sample;\n",
        "2. histogram of '1-mer' per sample = 0th markov model;\n",
        "\n",
        "Key observations:\n",
        "1. MAD (Median absolute deviation) method for outlier detection based on MSE distance to mean gives wider spread;\n",
        "2. cosine similarity is worse for outlier detection as its narrowly distributed around 1.0;\n",
        "\n",
        "New ideas:\n",
        "1. use a outlier detection algorithm to find an set of outliers set_out, with a loose threshold;\n",
        "2. define a clean set as s_clean = set_original - set_out;\n",
        "3. use the s_clean set to design autoencoder;\n",
        "4. use whole set to generate error=original - reconstructed\n",
        "5. detect outliers based on error;\n",
        "\n",
        "TODO:\n",
        "1. SOM = Self Organizing Maps, K-Means Clustering\n",
        "2. LDA Latent Dirichlet Allocation\n",
        "2. cross-validation to unsupervised outlier detection is benefical\n",
        "https://arxiv.org/pdf/0909.3052.pdf\n",
        "Here is article which explains how cross-validation is a good tool for unsupervised learning http://udini.proquest.com/view/cross-validation-for-unsupervised-pqid:1904931481/ and the full text is available here http://arxiv.org/pdf/0909.3052.pdf\n",
        "\n",
        "https:///www.researchgate.net/post/Which_are_the_methods_to_validate_an_unsupervised_machine_learning_algorithm\n",
        "\n",
        "\n",
        "SOM:  as SOM is an unsupervised learning technique, there is no target variable and hence, we do not even calculate the loss function and therefore there is no backward propagation process also needed for SOMs.\n",
        "\n",
        "\n",
        "functions:\n",
        "1. func_gen_features\n",
        "\n",
        "\n",
        "There are 3 general approaches to encoding sequence data:\n",
        "\n",
        "1.   Ordinal encoding DNA Sequence\n",
        "2.   One-hot encoding DNA Sequence\n",
        "3.   DNA sequence as a “language”, known as k-mer counting\n",
        "\n",
        "\n",
        "Ref:\n",
        "\n",
        "https://www-users.york.ac.uk/~vjh5/myPapers/Hodge+Austin_OutlierDetection_AIRE381.pdf\n"
      ],
      "metadata": {
        "id": "zivjTsUMPB8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/exter_job_test.txt'\n",
        "kmers_size = 3\n",
        "#featrue_method ='1mers'\n",
        "featrue_method ='kmers'\n",
        "Use_Tf_Idf = False\n",
        "THRESHOLD = 3\n",
        "\n",
        "# parameters for TfidfVectorizer\n",
        "TFIDF_MAX_DF = 1.0\n",
        "TFIDF_MIN_DF = 1\n"
      ],
      "metadata": {
        "id": "DqAl90tKS6HK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#generate 1-mers based features  \n"
      ],
      "metadata": {
        "id": "YG07FruBC3B8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBhzIuZkm4Zp"
      },
      "outputs": [],
      "source": [
        "# generate 1-mers based features;\n",
        "# extract the unique chars 'chars_list' from all the samples\n",
        "chars_list = set()\n",
        "count_chars = 0\n",
        "\n",
        "def func_count_unique_chars(line, counter):\n",
        "  \"\"\"func to get unique chars in 'line' based on current 'counter'.\n",
        "  Note: set counter = set() for the first line;\n",
        "  \"\"\"\n",
        "  for c in line:\n",
        "    counter.add(c)\n",
        "  return (counter)\n",
        "  #sorted(set) returns a list\n",
        "\n",
        "f = open(file_path, \"r\")\n",
        "for number_of_lines, line in enumerate(f):\n",
        "  line = line.rstrip('\\n')\n",
        "  count_chars += len(line)\n",
        "  chars_list = func_count_unique_chars(line, chars_list)\n",
        "  if number_of_lines<5:\n",
        "    print(f'Line {number_of_lines} has\\t{len(line)} chars, count_chars ={count_chars},\\\n",
        "            the last 5 chars ={line[-5:]}, uniques={sorted(chars_list)}')\n",
        "f.close()\n",
        "number_of_lines = number_of_lines + 1\n",
        "chars_list = sorted(chars_list)\n",
        "print('*** Summary: number of lines =\\t', number_of_lines)\n",
        "print('number of chars =\\t', count_chars)\n",
        "print('unique chars =', chars_list)\n",
        "\n",
        "# 1-mers feature: generate features as the frequency of each unique chars per sample;\n",
        "# equivalent to 0th order markov chain model;\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def func_count_chars_ordered(line, chars):\n",
        "  \"\"\"func to count number of strings in string 'line' based on current 'counter'.\n",
        "  Note: set counter = {} for the first line;\n",
        "  \"\"\"\n",
        "  counter = np.zeros((len(chars, )))\n",
        "  #print(counter, line, chars)\n",
        "  for id, c in enumerate(chars):\n",
        "    #print(f'id={id}, ch={chars[id]}')\n",
        "    counter[id] = line.count(chars[id])\n",
        "  return counter\n",
        "\n",
        "def func_1mers_feature(file_path, number_of_lines, chars_list):\n",
        "  df = np.zeros((number_of_lines, len(chars_list)))\n",
        "  #print(chars_list, df.shape)\n",
        "\n",
        "  f = open(file_path, \"r\")\n",
        "  for c, line in enumerate(f):\n",
        "    if c<0:\n",
        "      print(f'***num of chars in line {c} is {len(line), {line[-5:]}}')\n",
        "    line = line.rstrip('\\n')\n",
        "    df[c] = func_count_chars_ordered(line, chars_list)\n",
        "  f.close()\n",
        "\n",
        "  df = pd.DataFrame(df, columns=chars_list)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_hist_features(X):\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  fig, axs = plt.subplots(len(X.columns), 2, figsize=(10, 2*len(X.columns)), facecolor='w', edgecolor='k')\n",
        "  fig.subplots_adjust(hspace = .5, wspace=.001)\n",
        "\n",
        "  for idx, col in enumerate(X.columns):\n",
        "      ax = axs[idx, 0]\n",
        "      ax.hist(X[col])\n",
        "      ax.set_title(col)\n",
        "      ax = axs[idx, 1]\n",
        "      ax.hist(np.log(X[col]))\n",
        "      ax.set_title(col)"
      ],
      "metadata": {
        "id": "yDbQzghcw8m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = func_1mers_feature(file_path, number_of_lines, chars_list)\n",
        "print(chars_list, type(X), X.shape)\n"
      ],
      "metadata": {
        "id": "QlVsTMD8AFmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_hist_features(X)"
      ],
      "metadata": {
        "id": "EbuJW0IjAdbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.datasets import make_blobs, make_moons\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "from sklearn.linear_model import SGDOneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "matplotlib.rcParams[\"contour.negative_linestyle\"] = \"solid\"\n",
        "\n",
        "# define outlier/anomaly detection methods to be compared.\n",
        "# the SGDOneClassSVM must be used in a pipeline with a kernel approximation\n",
        "# to give similar results to the OneClassSVM\n",
        "outliers_fraction = 0.15\n",
        "\n",
        "anomaly_algorithms = [\n",
        "    (\n",
        "        \"Robust covariance\",\n",
        "        EllipticEnvelope(contamination=outliers_fraction, random_state=42),\n",
        "    ),\n",
        "    (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.1)),\n",
        "    (\n",
        "        \"One-Class SVM (SGD)\",\n",
        "        make_pipeline(\n",
        "            Nystroem(gamma=0.1, random_state=42, n_components=150),\n",
        "            SGDOneClassSVM(\n",
        "                nu=outliers_fraction,\n",
        "                shuffle=True,\n",
        "                fit_intercept=True,\n",
        "                random_state=42,\n",
        "                tol=1e-6,\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        "    (\n",
        "        \"Isolation Forest\",\n",
        "        IsolationForest(contamination=outliers_fraction, random_state=42),\n",
        "    ),\n",
        "    (\n",
        "        \"Local Outlier Factor\",\n",
        "        LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction),\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "4HjCtCouzkvs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k-mers: generate features based on k-mers and CountVectorizer per sample;\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "def func_Kmers(seq, size=2):\n",
        "  kmers = [seq[x:x+size].lower() for x in range(len(seq) - size + 1)]\n",
        "  return ' '.join(kmers)\n",
        "\n",
        "def func_kmers_feature(file_path, Use_Tf_Idf=False, kmers_size=2):\n",
        "  #convert our training data sequences into short overlapping k-mers of length 6.\n",
        "  #Lets do that for each sample of data we have using our Kmers_funct function.\n",
        "  if kmers_size==1:\n",
        "    print('kmers_size must be larger than 1!')\n",
        "    return;\n",
        "  df = pd.read_table(file_path, header=None, names=['sequence'])\n",
        "  #print(f'Inside func_kmers_feature: df has columns: {df.columns}')\n",
        "  #df[f'{kmers_size}-mers'] = df.apply(lambda x: func_Kmers(x[df.columns[0]], kmers_size), axis=1)\n",
        "  df[f'{kmers_size}-mers'] = df[df.columns[0]].apply(lambda x: func_Kmers(x, kmers_size))#, axis=1)\n",
        "\n",
        "  df = df.drop(df.columns[0], axis=1)\n",
        "  #print(f'Inside func_kmers_feature: df has columns: {df.columns}')\n",
        "\n",
        "  X = list(df[df.columns[0]])\n",
        "  #for item in range(len(X)):\n",
        "  #  X[item] = ' '.join(X[item])\n",
        "\n",
        "  #Creating the Bag of Words model:\n",
        "  if Use_Tf_Idf:\n",
        "    #max_dffloat or int, default=1.0\n",
        "    #min_dffloat or int, default=1\n",
        "    cv = TfidfVectorizer(\n",
        "      max_df=TFIDF_MAX_DF,\n",
        "      min_df=TFIDF_MIN_DF,\n",
        "      stop_words=None,\n",
        "    )\n",
        "  else:\n",
        "    cv = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "  #TODO: ngram_range needs to be hyperparameterized\n",
        "  X = cv.fit_transform(X)\n",
        "  print(f'Inside func_kmers_feature: cv len is {len(cv.vocabulary_)}, X shape is {X.shape}')\n",
        "  return X\n",
        "\n",
        "# generic feature generation function\n",
        "def func_gen_features(featrue_method, Use_Tf_Idf=False, kmers_size=1, file_path=file_path, number_of_lines=number_of_lines, chars_list=chars_list):\n",
        "  if featrue_method =='1mers':\n",
        "    print(f'The featrue generation method used: {featrue_method}')\n",
        "    df = func_1mers_feature(file_path, number_of_lines, chars_list)\n",
        "    #print(df.head(3))\n",
        "    return df.to_numpy()\n",
        "  else:\n",
        "    print(f'The featrue generation method used: {featrue_method}, kmers_size={kmers_size}')\n",
        "    X = func_kmers_feature(file_path, Use_Tf_Idf, kmers_size)\n",
        "    X = X.toarray()\n",
        "    feature_vector_size = X.shape[1]\n",
        "    print(f'feature_vector_size = {feature_vector_size}, X shape is {X.shape}, X type is {type(X)}')\n",
        "    return X"
      ],
      "metadata": {
        "id": "GBKCrT0JYhwS"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featrue_method ='kmers'\n",
        "kmers_size = 3\n",
        "Use_Tf_Idf = True\n",
        "X = func_gen_features(featrue_method, Use_Tf_Idf, kmers_size)\n",
        "#if featrue_method == 'kmers':\n",
        "#  X = X.toarray()\n",
        "print(type(X), X.shape, X[:2, ])"
      ],
      "metadata": {
        "id": "l6oNQXm_1apq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42574706-0e7e-4a8d-c3a4-193486362736"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The featrue generation method used: kmers, kmers_size=3\n",
            "Inside func_kmers_feature: cv len is 2246, X shape is (300, 2246)\n",
            "feature_vector_size = 2246, X shape is (300, 2246), X type is <class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'> (300, 2246) [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare given classifiers under given settings\n",
        "def train_multi(X, anomaly_algorithms, plot_show=False, add_outliers=False):\n",
        "  x_scores = {}\n",
        "  # Add outliers\n",
        "  if add_outliers:\n",
        "    n_outliers = 10\n",
        "    rng = np.random.RandomState(42)\n",
        "    X = np.concatenate([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)\n",
        "\n",
        "  if plot_show:\n",
        "    xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\n",
        "    plt.figure(figsize=(len(anomaly_algorithms)*3, 3))\n",
        "    plt.subplots_adjust(\n",
        "        left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01)\n",
        "    plot_num = 1\n",
        "\n",
        "  for name, algorithm in anomaly_algorithms:\n",
        "    t0 = time.time()\n",
        "    algorithm.fit(X)\n",
        "    t1 = time.time()\n",
        "    if plot_show:\n",
        "      plt.subplot(1, len(anomaly_algorithms), plot_num)\n",
        "      plt.title(name, size=18)\n",
        "\n",
        "    # fit the data and tag outliers\n",
        "    if name == \"Local Outlier Factor\":\n",
        "        y_pred = algorithm.fit_predict(X)\n",
        "        x_scores[name] = algorithm.negative_outlier_factor_\n",
        "\n",
        "    else:\n",
        "        y_pred = algorithm.fit(X).predict(X)\n",
        "        x_scores[name] = algorithm.score_samples(X)\n",
        "\n",
        "    if plot_show:\n",
        "      # plot the levels lines and the points\n",
        "      if name != \"Local Outlier Factor\":  # LOF does not implement predict\n",
        "          Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "          Z = Z.reshape(xx.shape)\n",
        "          plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\"black\")\n",
        "\n",
        "      colors = np.array([\"#377eb8\", \"#ff7f00\"])\n",
        "      plt.scatter(X[:, 0], X[:, 1], s=10)#, color=colors[(y_pred + 1) // 2])\n",
        "      plt.xlim(-7, 7)\n",
        "      plt.ylim(-7, 7)\n",
        "      plt.xticks(())\n",
        "      plt.yticks(())\n",
        "      plt.text(0.99, 0.01, (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
        "          transform=plt.gca().transAxes, size=15,\n",
        "          horizontalalignment=\"right\",)\n",
        "      plot_num += 1\n",
        "  if plot_show:\n",
        "    plt.show()\n",
        "  return x_scores"
      ],
      "metadata": {
        "id": "SauVWgYD1BNN"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get features ready for detector\n",
        "def get_features(featrue_method='kmers', Use_Tf_Idf=False, kmers_size=2, pca_n_components=2, plot_pca=False):\n",
        "  # generate datasets, PCA, outlier detection\n",
        "  import matplotlib.pyplot as plt\n",
        "  from sklearn.decomposition import PCA\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  from sklearn.pipeline import Pipeline\n",
        "\n",
        "  X = func_gen_features(featrue_method, Use_Tf_Idf, kmers_size)\n",
        "\n",
        "  n_samples = X.shape[0]\n",
        "  n_outliers = int(outliers_fraction * n_samples)\n",
        "  n_inliers = n_samples - n_outliers\n",
        "\n",
        "  print(n_samples, type(X), len(X))\n",
        "  #plot_hist_features(X)\n",
        "\n",
        "  pipeline = Pipeline([('scaling', StandardScaler()),\n",
        "                       ('pca', PCA(n_components=pca_n_components))])\n",
        "\n",
        "  #X_pca = pca.fit(X).transform(X)\n",
        "  X_pca = pipeline.fit(X).transform(X) #fit_transform(data)\n",
        "\n",
        "  # Percentage of variance explained for each components\n",
        "  print(\n",
        "      \"explained variance ratio (first two components): %s\"\n",
        "      % str(pipeline['pca'].explained_variance_ratio_)\n",
        "  )\n",
        "  print(X.shape, X_pca.shape)\n",
        "  if plot_pca:\n",
        "    plt.figure()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    colors = [\"navy\", \"turquoise\"]\n",
        "    plt.scatter(\n",
        "        X_pca[:, 0], X_pca[:, 1], color='r', alpha=0.8, lw=2)\n",
        "    plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
        "    plt.title(\"PCA of dataset\")\n",
        "    plt.show()\n",
        "  return X_pca"
      ],
      "metadata": {
        "id": "kU-HlUzw-lmH"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_outliers_idx(x_scores, percentile_threshold=5, print_info=False):\n",
        "  idx_all = {}\n",
        "  for key in x_scores.keys():\n",
        "    xt = np.percentile(x_scores[key], percentile_threshold)\n",
        "    idx = np.where(x_scores[key] < xt)\n",
        "    idx_all[key] = idx\n",
        "    if print_info:\n",
        "      print(f'The {key} algorithm detects the indexes of outliers as: \\n  {list(idx[0])}')\n",
        "  return idx_all\n"
      ],
      "metadata": {
        "id": "0Hec14-cPtRf"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_commons(outliers_idx_all):\n",
        "  # common element extraction form N lists\n",
        "  # using reduce() + lambda + set()\n",
        "  from functools import reduce\n",
        "\n",
        "  test_list = [list(x[0]) for x in list(outliers_idx_all.values())]\n",
        "  print (\"The original list is : \" + str(test_list))\n",
        "  # common element extraction form N lists\n",
        "  # using reduce() + lambda + set()\n",
        "  res = list(reduce(lambda i, j: i & j, (set(x) for x in test_list)))\n",
        "  res = sorted(res)\n",
        "  print (\"The common elements from N lists : \" + str(res))\n",
        "  return res"
      ],
      "metadata": {
        "id": "AECPXOf7l4SY"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_scores = train_multi(X_pca, anomaly_algorithms, False)"
      ],
      "metadata": {
        "id": "_hhk2vc6HNYI"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_common_outliers(percentile_threshold, plot_pca, plot_algorithms_results, print_info):\n",
        "  X_pca = get_features(featrue_method, True, kmers_size, pca_n_components, plot_pca)\n",
        "  x_scores = train_multi(X_pca, anomaly_algorithms, plot_algorithms_results)\n",
        "  outliers_idx_all = get_outliers_idx(x_scores, percentile_threshold, print_info)\n",
        "  num_figs = len(x_scores.keys())\n",
        "  if 0:\n",
        "    plt.figure(figsize=(num_figs*3, num_figs*3))\n",
        "    for k, key in enumerate(x_scores.keys()):\n",
        "      print(key, len(x_scores[key]))\n",
        "      plt.subplot(num_figs, 1, k+1)\n",
        "      plt.title(key, size=18)\n",
        "      plt.plot(x_scores[key])\n",
        "      plt.grid()\n",
        "  common_outliers = find_commons(outliers_idx_all)\n"
      ],
      "metadata": {
        "id": "EMOlQCKrQYhg"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top function call\n",
        "# featrue_method ='kmers' AND percentile_threshold=15\n",
        "# featrue_method ='kmers' AND percentile_threshold=15\n",
        "Use_Tf_Idf = False\n",
        "featrue_method ='1mers'\n",
        "kmers_size = 2\n",
        "percentile_threshold = 15\n",
        "pca_n_components = 2\n",
        "\n",
        "plot_pca = False\n",
        "plot_algorithms_results = False\n",
        "print_single_algorithm = False\n",
        "\n",
        "common_outliers = get_common_outliers(percentile_threshold, plot_pca, plot_algorithms_results, print_single_algorithm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK3A-yO1KzlW",
        "outputId": "5ca2a70f-97c9-4eb0-ab5a-f3c6209ca995"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The featrue generation method used: 1mers\n",
            "300 <class 'numpy.ndarray'> 300\n",
            "explained variance ratio (first two components): [0.39613671 0.20235765]\n",
            "(300, 5) (300, 2)\n",
            "The original list is : [[4, 6, 7, 13, 24, 40, 44, 50, 51, 54, 56, 92, 98, 113, 130, 137, 141, 142, 147, 148, 149, 151, 152, 153, 154, 155, 158, 163, 164, 165, 170, 176, 189, 192, 199, 206, 215, 216, 223, 225, 257, 276, 278, 291, 292], [4, 11, 13, 21, 25, 50, 54, 56, 81, 98, 135, 137, 141, 147, 148, 152, 154, 158, 161, 163, 164, 165, 167, 170, 176, 187, 202, 221, 223, 230, 258, 261, 262, 265, 266, 269, 275, 276, 278, 283, 287, 291, 292], [4, 11, 13, 21, 25, 50, 53, 54, 56, 81, 98, 135, 137, 141, 142, 147, 148, 152, 154, 158, 161, 163, 164, 165, 167, 170, 176, 187, 202, 221, 223, 230, 258, 261, 262, 265, 266, 269, 275, 276, 278, 283, 287, 291, 292], [4, 7, 11, 13, 21, 24, 25, 50, 53, 54, 56, 60, 98, 137, 140, 141, 144, 147, 148, 151, 152, 154, 155, 158, 161, 163, 164, 165, 170, 176, 187, 202, 221, 223, 258, 261, 262, 265, 266, 269, 275, 278, 283, 287, 291], [4, 11, 13, 16, 20, 25, 44, 50, 51, 52, 56, 65, 98, 129, 135, 137, 141, 142, 144, 148, 149, 154, 157, 158, 160, 162, 164, 165, 178, 180, 189, 199, 206, 209, 214, 216, 217, 245, 253, 258, 261, 276, 283, 291, 292]]\n",
            "The common elements from N lists : [4, 13, 50, 56, 98, 137, 141, 148, 154, 158, 164, 165, 291]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top function call\n",
        "# featrue_method ='kmers' AND percentile_threshold=15\n",
        "# featrue_method ='kmers' AND percentile_threshold=15\n",
        "Use_Tf_Idf = False\n",
        "featrue_method ='kmers'\n",
        "kmers_size = 2\n",
        "percentile_threshold = 15\n",
        "pca_n_components = 2\n",
        "\n",
        "plot_pca = False\n",
        "plot_algorithms_results = False\n",
        "print_single_algorithm = False\n",
        "\n",
        "common_outliers = get_common_outliers(percentile_threshold, plot_pca, plot_algorithms_results, print_single_algorithm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNbvu_Z8PC2Y",
        "outputId": "6a8cba6d-6bdf-4e76-abe4-ee9670e76b36"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The featrue generation method used: kmers, kmers_size=2\n",
            "Inside func_kmers_feature: cv len is 25, X shape is (300, 25)\n",
            "feature_vector_size = 25, X shape is (300, 25), X type is <class 'numpy.ndarray'>\n",
            "300 <class 'numpy.ndarray'> 300\n",
            "explained variance ratio (first two components): [0.1056142 0.0857707]\n",
            "(300, 25) (300, 2)\n",
            "The original list is : [[10, 33, 100, 133, 134, 135, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 164, 165, 175, 183, 189, 197, 200, 202, 206, 210, 213, 227, 240, 241, 282, 297], [7, 27, 33, 42, 50, 54, 59, 60, 70, 78, 93, 100, 116, 133, 134, 135, 139, 140, 143, 148, 149, 152, 153, 154, 157, 159, 161, 162, 163, 164, 165, 175, 183, 197, 202, 204, 213, 227, 239, 240, 241, 243, 275, 295, 297], [7, 27, 33, 42, 50, 54, 59, 70, 78, 93, 100, 116, 133, 134, 135, 139, 140, 143, 148, 149, 152, 153, 154, 157, 159, 160, 161, 162, 163, 164, 165, 175, 183, 197, 202, 204, 213, 227, 239, 240, 241, 243, 275, 295, 297], [27, 33, 42, 54, 59, 78, 100, 116, 133, 134, 135, 139, 140, 141, 143, 145, 146, 148, 149, 151, 152, 153, 154, 157, 159, 160, 161, 162, 163, 164, 165, 175, 183, 197, 202, 204, 210, 213, 227, 239, 240, 241, 243, 275, 297], [27, 100, 133, 134, 135, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 164, 165, 175, 183, 189, 200, 202, 204, 206, 210, 213, 227, 239, 240, 241, 243, 297]]\n",
            "The common elements from N lists : [100, 133, 134, 135, 139, 140, 143, 148, 149, 152, 153, 154, 157, 159, 161, 162, 163, 164, 165, 175, 183, 202, 213, 227, 240, 241, 297]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#summary results\n",
        "#1-mers\n",
        "The common elements from N lists : [4, 13, 50, 56, 98, 137, 141, 148, 154, 158, 164, 165, 291]\n",
        "\n",
        "#2-mers\n",
        "The common elements from N lists : [100, 133, 134, 135, 139, 140, 143, 148, 149, 152, 153, 154, 157, 159, 161, 162, 163, 164, 165, 175, 183, 202, 213, 227, 240, 241, 297]\n",
        "\n",
        "#shared:\n",
        "148, 154, 164, 165\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4gzcZO1LPTz6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wdUrogsPCyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anomaly_algorithms\n",
        "x_scores.keys()"
      ],
      "metadata": {
        "id": "q8okmFvioZPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d6fb23-c67a-440e-97a8-7b5308ff9225"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Robust covariance', 'One-Class SVM', 'One-Class SVM (SGD)', 'Isolation Forest', 'Local Outlier Factor'])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_idx_all"
      ],
      "metadata": {
        "id": "OOETyr9I8DYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dacb9d6-0679-4b3c-ba5b-65b9f0db67d5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Robust covariance': (array([133, 134, 135, 136, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
              "         148, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162,\n",
              "         163, 164, 165, 168]),),\n",
              " 'One-Class SVM': (array([ 21,  38,  42,  45,  51,  52,  57,  59,  73,  85, 113, 136, 137,\n",
              "         138, 139, 141, 142, 146, 154, 156, 157, 162, 165, 168, 206, 236,\n",
              "         258, 264, 280, 285]),),\n",
              " 'One-Class SVM (SGD)': (array([ 21,  38,  45,  51,  52,  57,  59,  73,  85, 113, 136, 137, 138,\n",
              "         139, 141, 142, 146, 154, 156, 157, 158, 162, 165, 168, 206, 224,\n",
              "         236, 258, 264, 285]),),\n",
              " 'Isolation Forest': (array([ 21,  45,  51,  52,  57,  73,  85, 104, 113, 135, 136, 137, 139,\n",
              "         141, 142, 146, 148, 154, 156, 158, 161, 162, 165, 168, 203, 206,\n",
              "         224, 225, 240, 258]),),\n",
              " 'Local Outlier Factor': (array([  0,  10,  16,  21,  38,  45,  46,  51,  52,  57,  73,  80,  85,\n",
              "         101, 109, 113, 137, 165, 168, 171, 197, 206, 215, 224, 225, 236,\n",
              "         240, 258, 280, 285]),)}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [0,1]:\n",
        "  print(X_pca.shape, np.mean(X_pca[:, k], axis=0), np.std(X_pca[:, k], axis=0))"
      ],
      "metadata": {
        "id": "mTFxFNayEZeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars_list"
      ],
      "metadata": {
        "id": "vfmIPD3kEZWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ig_qEezzcMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate features based on k-mers and CountVectorizer"
      ],
      "metadata": {
        "id": "3vj44G5h0Y6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to now convert the lists of k-mers for each sample into string sentences of words that can be used to create the Bag of Words model."
      ],
      "metadata": {
        "id": "_a_fXLQGDsnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-mers: Step 2 — Training an ‘Auto-Encoder’ neural network\n",
        "\n",
        "As our process is completely unsupervised and we don’t have labeled data (as outlier/non-outlier), we will use 5-layer deep ‘Auto-encoder’ neural network to train our model. It is a special type of neural network which copies input data to output data. This process is known as ‘reconstruction’. ‘Hidden layers’ of the network does the feature extraction & decoding work. At the end of the entire process definitely, some loss gets generated and the data point which is dissimilar from others incurs more loss.\n",
        "\n",
        "Denote feature_vector_size as the input data size;\n",
        "\n",
        "Auto-encoder layer structure will look like:\n",
        "\n",
        "feature_vector_size -> 600 -> 50 -> 600 -> feature_vector_size\n",
        "\n",
        "Layer 1(Input Layer) has feature_vector_size features, \\\n",
        "Layer 2 has 600 features, \\\n",
        "Layer 3 has 50 features, \\\n",
        "Layer 4 has 600 features, \\\n",
        "Layer 5(Output Layer)has feature_vector_size features. \\\n",
        "\n",
        "Layer 1, as usual, will have all ‘Doc2Vec’ generated features from ‘Step 1’. Layer 2, 3 & 4 are hidden layers doing the actual data messaging (expansion & shrinking) and information extraction part. Actually, Layer 2 is known as ‘encoding layer’ and Layer 4 is known as ‘decoding layer’. Ultimately, the output will come out from Layer 5. ‘Autoencoders’ are trained with the same data as input & output both. So, Layer 5 output is nothing but a reconstructed version of the input with some loss. Normal data points will go through smoothly between layers, with minimal loss, but data loss for the ‘outliers’ will be more as those don’t follow the hidden data pattern.\n",
        "\n",
        "This is the graphical representation of our 5-layer ‘Auto-encoder"
      ],
      "metadata": {
        "id": "UZg-VDJWAFsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#generate features by: 1mers, kmers"
      ],
      "metadata": {
        "id": "K40UDlF9-xpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate feature and normalize it\n",
        "def gen_feature(featrue_method, Use_Tf_Idf, kmers_size):\n",
        "  X = func_gen_features(featrue_method, Use_Tf_Idf, kmers_size, file_path)\n",
        "  #Normalize data\n",
        "  from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
        "  from sklearn.pipeline import Pipeline\n",
        "\n",
        "  if featrue_method == 'kmers':\n",
        "    # configure our pipeline\n",
        "    pipeline = Pipeline([('normalizer', Normalizer()), ('scaler', MinMaxScaler())])\n",
        "\n",
        "    # get normalization parameters by fitting to the training data\n",
        "    pipeline.fit(X) #.toarray()\n",
        "\n",
        "    # transform the training and validation data with these parameters\n",
        "    X = pipeline.transform(X) #.toarray()\n",
        "    #X_validate_transformed = pipeline.transform(X_validate)\n",
        "  return X\n"
      ],
      "metadata": {
        "id": "9u0UII7JCzv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate feature and normalize it\n",
        "Use_Tf_Idf = False\n",
        "featrue_method = 'kmers'\n",
        "kmers_size = 5\n",
        "X = gen_feature(featrue_method, Use_Tf_Idf, kmers_size)"
      ],
      "metadata": {
        "id": "VvIcMUhqQid_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZEx4dEm5Aut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder_1(X, layer_features):\n",
        "  from sklearn.neural_network import MLPRegressor\n",
        "  # hyperparameters\n",
        "  auto_encoder = MLPRegressor(hidden_layer_sizes=(layer_features))\n",
        "  #auto_encoder.fit(X.toarray(), X.toarray())\n",
        "  auto_encoder.fit(X, X)\n",
        "  predicted_vectors = auto_encoder.predict(X)\n",
        "\n",
        "  print(f'X shape is {X.shape}, X type is {type(X)}')\n",
        "  print(f'predicted_vectors shape is {predicted_vectors.shape}, predicted_vectors type is {type(predicted_vectors)}')\n",
        "  print(f'auto_encoder.score = {auto_encoder.score(predicted_vectors, X)}')\n",
        "\n",
        "  return predicted_vectors\n"
      ],
      "metadata": {
        "id": "ghx6KFHcEsZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder_2(X):\n",
        "  # another AutoEncoder\n",
        "  # data dimensions // hyperparameters\n",
        "  import tensorflow as tf\n",
        "  input_dim = X.shape[1]\n",
        "  BATCH_SIZE = 256\n",
        "  EPOCHS = 12\n",
        "\n",
        "  # https://keras.io/layers/core/\n",
        "  autoencoder = tf.keras.models.Sequential([\n",
        "      # deconstruct / encode\n",
        "      tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )),\n",
        "      tf.keras.layers.Dense(16, activation='elu'),\n",
        "      tf.keras.layers.Dense(8, activation='elu'),\n",
        "      tf.keras.layers.Dense(4, activation='elu'),\n",
        "      tf.keras.layers.Dense(2, activation='elu'),\n",
        "\n",
        "      # reconstruction / decode\n",
        "      tf.keras.layers.Dense(4, activation='elu'),\n",
        "      tf.keras.layers.Dense(8, activation='elu'),\n",
        "      tf.keras.layers.Dense(16, activation='elu'),\n",
        "      tf.keras.layers.Dense(input_dim, activation='elu')\n",
        "  ])\n",
        "\n",
        "  # https://keras.io/api/models/model_training_apis/\n",
        "  autoencoder.compile(optimizer=\"adam\",\n",
        "                      loss=\"mse\",\n",
        "                      metrics=[\"acc\"])\n",
        "\n",
        "  # print an overview of our model\n",
        "  autoencoder.summary();\n",
        "\n",
        "  history = autoencoder.fit(\n",
        "      X, X,\n",
        "      shuffle=True,\n",
        "      epochs=EPOCHS,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      #callbacks=cb,\n",
        "      #validation_data=(X_validate_transformed, X_validate_transformed)\n",
        "  );\n",
        "\n",
        "  # transform the test set with the pipeline fitted to the training set\n",
        "  #X_test_transformed = pipeline.transform(X)\n",
        "\n",
        "  # pass the transformed test set through the autoencoder to get the reconstructed result\n",
        "  reconstructions = autoencoder.predict(X)\n",
        "  #Calculate the reconstruction loss for every transaction and draw a sample.\n",
        "  return reconstructions\n"
      ],
      "metadata": {
        "id": "pb9PqS3PTV7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AutoEncoder: Setting a threshold for classification\n",
        "Unsupervised\n",
        "Normally, in an unsupervised solution, this is where the story would end. We would set a threshold that limits the amount of false positives to a manageable degree, and captures the most anomalous data points.\n",
        "\n",
        "#Percentiles\n",
        "We could set this threshold by taking the top x% of the dataset and considering it anomalous.\n",
        "\n",
        "#MAD\n",
        "We could also use a modified Z-score using the Median Absolute Deviation to define outliers on our reconstruction data. Here is a good blog post on the topic by João Rodrigues, illustrating why this algorithm is more robust and scalable than the percentiles method."
      ],
      "metadata": {
        "id": "ihKLr4WDVcAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_reconstructions(autoencoder_select):\n",
        "  if autoencoder_select=='autoencoder_1':\n",
        "    layer_features = [600, 10, 600]\n",
        "    reconstructions = autoencoder_1(X, layer_features)\n",
        "  else:\n",
        "    reconstructions = autoencoder_2(X)\n",
        "\n",
        "  # calculating the mean squared error reconstruction loss per row in the numpy array\n",
        "  mse = np.mean(np.power(X - reconstructions, 2), axis=1)\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  fig, ax = plt.subplots(figsize=(6,2))\n",
        "  ax.hist(mse, bins=50, density=True, label=\"mse\", alpha=.6, color=\"green\")\n",
        "  plt.title(\"(Normalized) Distribution of the Reconstruction Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  return reconstructions\n"
      ],
      "metadata": {
        "id": "IhOGKHx06uHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mad_score(points):\n",
        "    \"\"\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm \"\"\"\n",
        "    m = np.median(points)\n",
        "    ad = np.abs(points - m)\n",
        "    mad = np.median(ad)\n",
        "\n",
        "    return 0.6745 * ad / mad\n",
        "\n",
        "def func_outlier_mad_index(X, reconstructions):\n",
        "  mse = np.mean(np.power(X - reconstructions, 2), axis=1)\n",
        "  z_scores = mad_score(mse)\n",
        "  outliers = z_scores > THRESHOLD\n",
        "  outliers_idx = list(np.where(outliers == True)[0])\n",
        "  inliers_idx = list(np.where(outliers == False)[0])\n",
        "  return outliers_idx, inliers_idx"
      ],
      "metadata": {
        "id": "wAir1ZgiUXta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructions = calc_reconstructions('autoencoder_1')\n",
        "print(f'X.shape={X.shape}, reconstructions.shape={reconstructions.shape}')\n"
      ],
      "metadata": {
        "id": "ge0tJj7aUXxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_idx, inliers_idx= func_outlier_mad_index(X, reconstructions)\n",
        "print(f'There are {len(inliers_idx)} inliers.')\n",
        "print(f'The {len(outliers_idx)} outliers idx: {outliers_idx}')\n"
      ],
      "metadata": {
        "id": "DprEQqc1_36f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ready autoencoder: [0, 69, 93, 111, 127, 145, 159, 212, 253, 255, 260, 272, 278, 292, 296]\n",
        "\n",
        "own autoencoder: [0, 21, 95, 132, 134, 138, 139, 141, 142, 143, 146, 149, 150, 151, 152, 153, 154, 155, 156, 159, 160, 161, 162, 163, 164, 165, 194, 206, 215, 274, 276]\n",
        "\n"
      ],
      "metadata": {
        "id": "PLxYjiWjdIfg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVLJfxIydIHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(5,2))\n",
        "ax.hist(mse, bins=50, density=True, label=\"mse\", alpha=.6, color=\"green\")\n",
        "plt.title(\"(Normalized) Distribution of the Reconstruction Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z_BFwGW-YSYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(6,3))\n",
        "ax.hist(mse[outliers_idx], bins=50, density=False, label=\"mse outliers\", alpha=.06, color=\"red\")\n",
        "ax.hist(mse[inliers_idx], bins=50, density=False, label=\"mse\", alpha=.6, color=\"green\")\n",
        "plt.title(\"(Normalized) Distribution of the Reconstruction Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NS7wIQthWD5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKJyXTIoWD1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l0VecYqsWDxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(auto_encoder.loss_curve_).plot()"
      ],
      "metadata": {
        "id": "45XgMgM6Lprb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here X-axis denotes no of iterations.\n",
        "\n",
        "#Step 3 — Similarity measure with output and actual work\n",
        "\n",
        "Once we get the output vectors, we can now measure the loss. ‘Loss’ is ideally given by ‘outlier factor’ expression\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XKWy1CDJEWEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, predicted_vectors.shape)\n",
        "print(type(X), type(predicted_vectors))\n",
        "print(f'X[0] = {X[0, :5]}')\n",
        "print(f'predicted_vectors[0] = {predicted_vectors[0, :5]}')"
      ],
      "metadata": {
        "id": "8KbXnfCuFcjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def key_consine_similarity(tupple):\n",
        "    return tupple[1]\n",
        "\n",
        "def get_computed_similarities(vectors, predicted_vectors, reverse=False):\n",
        "    # This returns a cosine-based similarity, the larger the returned value the\n",
        "    # original feature fector is more similar to the re-generated vector\n",
        "    # The outliers are the ones with the smaller cosine_similarities.\n",
        "\n",
        "    data_size = vectors.shape[0]\n",
        "    cosine_similarities = []\n",
        "    for i in range(data_size):\n",
        "        cosine_sim_val = (1 - cosine(vectors[i], predicted_vectors[i]))\n",
        "        cosine_similarities.append((i, cosine_sim_val))\n",
        "\n",
        "    return sorted(cosine_similarities, key=key_consine_similarity, reverse=reverse)\n",
        "\n",
        "def display_top_n(sorted_cosine_similarities, n=5):\n",
        "    for i in range(n):\n",
        "        index, consine_sim_val = sorted_cosine_similarities[i]\n",
        "        print(f'Potential outlier row index= {index},\\t Cosine Sim Val= {consine_sim_val}')\n",
        "\n",
        "#We can call the above functions in order to get top ‘k’ outliers\n",
        "top_numbers = 15\n",
        "print(f'Top {top_numbers} outliers')\n",
        "sorted_cosine_similarities = get_computed_similarities(vectors=X, predicted_vectors=predicted_vectors)\n",
        "display_top_n(sorted_cosine_similarities, top_numbers)"
      ],
      "metadata": {
        "id": "IbP-37sVD_-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_idx, inliers_idx= func_outlier_mad_index(X, predicted_vectors)\n",
        "print(len(outliers_idx), len(inliers_idx))\n",
        "print(outliers_idx)"
      ],
      "metadata": {
        "id": "qwzGJKfpcw3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_cosine_similarities[:5], sorted_cosine_similarities[-5: ]"
      ],
      "metadata": {
        "id": "s5bX_GGhJCkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(np.array(sorted_cosine_similarities)[:, 1])"
      ],
      "metadata": {
        "id": "9OWAAxKnhpml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for k in range(3):\n",
        "  idx = sorted_cosine_similarities[k][0]\n",
        "  plt.figure()\n",
        "  plt.plot(list(X[idx,:]),  'r-+')\n",
        "  plt.plot(predicted_vectors[idx], 'k-o')"
      ],
      "metadata": {
        "id": "lnLDD7qfD_ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imporve AutoEncoder based outlier detection: compute reconstruction error only using ‘normal points’\n",
        "https://www.sciencedirect.com/science/article/abs/pii/S0957417422019224\n",
        "\n",
        "Deep learning-based unsupervised techniques (autoencoder) minimize the reconstruction error using each data instance in the dataset and subsequently, data points with higher reconstruction error are treated as outlier points. However, autoencoder based model overestimates the reconstruction error for normal points whereas it is underestimated for outlier points. As a result, genuine outliers are missed by this approach.\n",
        "\n",
        "We propose two techniques to address the issue of reconstruction error stated earlier. Main idea of our techniques is to compute reconstruction error only using ‘normal points’. In the proposed techniques, we identify probable outliers utilizing the clustering approaches intelligently and subsequently, we do not include them in the minimization process of reconstruction error. We exploit recently recognized clustering approach Density Peak Clustering (DPC) to identify the probable outlier points based on density and distance to the higher density points. However, DPC has inherent drawback of setting threshold which plays important role in deciding density. Therefore, Self Organizing Map (SOM) is exploited as another clustering approach in this article. Subsequently, we conducted experiments on synthetic as well as real world datasets and the results show that the proposed technique outperforms the popular existing deep learning model like RandNet, Boosting-based Autoencoder Ensemble method (BAE), One Class Support Vector Machine (OCSVM) and density-based algorithms like LOF, LDOF, INFLO, and RDOS."
      ],
      "metadata": {
        "id": "-7N1Cf7iKX_K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcUHmOqaLSqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WtQbtQ6DLSmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SWqci1uJD_vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd.plotting.lag_plot(df.loc[:, [\"C\"]], lag=1)\n",
        "import matplotlib.pyplot as plt\n",
        "for col in df.columns:\n",
        "  #plt.figure()\n",
        "  pd.plotting.autocorrelation_plot(df.loc[:, [col]])\n",
        "plt.legend(df.columns, loc='upper right')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "INyEKGhNYpUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lag in range(1,5):\n",
        "  plt.figure()\n",
        "  pd.plotting.lag_plot(df, lag=lag)\n"
      ],
      "metadata": {
        "id": "SoVGprmBXEJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars_list"
      ],
      "metadata": {
        "id": "7YFEYJnoWxXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "7qJJTIAmhIQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "# calculate summary statistics\n",
        "data_mean, data_std = mean(df), std(df)\n",
        "# identify outliers\n",
        "cut_off = data_std * 2\n",
        "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
        "print(f'data_mean=\\n{data_mean}, data_std=\\n{data_std}')\n",
        "print(f'lower=\\n{lower}, upper=\\n{upper}')"
      ],
      "metadata": {
        "id": "qODl28qahidQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_idx, ch, df.loc[outliers_idx, ch]"
      ],
      "metadata": {
        "id": "kWTINzoqizAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify outliers\n",
        "for ch in chars_list:\n",
        "  outliers = [x for x in df[ch] if x < lower[ch] or x > upper[ch]]\n",
        "  outliers_idx = df.index[df[ch]==outliers[0]].tolist()\n",
        "  #print(ch, outliers, '\\n', outliers_idx, '\\n')\n",
        "  print(outliers_idx)\n"
      ],
      "metadata": {
        "id": "DhY1tZUgnWSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: outlier is\n",
        "269 by 'C, G' \\\n",
        "161 by 'C, D' \\\n"
      ],
      "metadata": {
        "id": "ILmPBejPjX1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot(column=['C', 'D', 'F'])\n"
      ],
      "metadata": {
        "id": "bYO2UnXImvi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot(column=['E', 'G'])"
      ],
      "metadata": {
        "id": "TvjTXry2mz9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot()"
      ],
      "metadata": {
        "id": "V6GSoKOzm1p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "#Note that neighbors.LocalOutlierFactor does not support predict, decision_function\n",
        "#and score_samples methods by default but only a fit_predict method, as this estimator\n",
        "#was originally meant to be applied for outlier detection. The scores of abnormality\n",
        "#of the training samples are accessible through the negative_outlier_factor_ attribute.\n",
        "\n",
        "n_neighbors = 150\n",
        "clf = LocalOutlierFactor(n_neighbors=n_neighbors)\n",
        "results = clf.fit_predict(df) #\n",
        "print(np.where(results==-1))\n",
        "#estimator.predict(X_test): Inliers are labeled 1, while outliers are labeled -1.\n",
        "#clf.negative_outlier_factor_\n",
        "#The decision_function method is also defined from the scoring function, in such\n",
        "#a way that negative values are outliers and non-negative ones are inliers:\n",
        "#estimator.decision_function(X_test)\n",
        "\n",
        "#negative_outlier_factor_: The opposite LOF of the training samples. The higher, the more normal.\n",
        "#Inliers tend to have a LOF score close to 1 (negative_outlier_factor_ close to -1), while outliers tend to have a larger LOF score.\n"
      ],
      "metadata": {
        "id": "QXew3phhmh2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmers_size = 2\n",
        "file_path = \"test.txt\"\n",
        "X = func_kmers_feature(file_path, kmers_size)\n",
        "feature_vector_size = X.shape[1]\n",
        "print(f'feature_vector_size = {feature_vector_size}, X shape is {X.shape}, X type is {type(X)}')"
      ],
      "metadata": {
        "id": "cP2VLqL0QlNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "n_neighbors = 2\n",
        "clf = LocalOutlierFactor(n_neighbors=n_neighbors)\n",
        "results = clf.fit_predict(X.toarray())\n",
        "outlier_idx = np.where(results==-1)\n",
        "print(f'outlier index={outlier_idx}')\n",
        "print(f'outlier results={results[outlier_idx]}')\n",
        "print(f'outlier negative_outlier_factor_={clf.negative_outlier_factor_[outlier_idx]}')\n",
        "print(f'first 10 negative_outlier_factor_={clf.negative_outlier_factor_[:10]}')"
      ],
      "metadata": {
        "id": "qvLQpe5vQQ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = func_1mers_feature(file_path, number_of_lines, chars_list)"
      ],
      "metadata": {
        "id": "V0zKvKz7QQva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "n_neighbors = 25\n",
        "clf = LocalOutlierFactor(n_neighbors=n_neighbors)\n",
        "results = clf.fit_predict(df)\n",
        "outlier_idx = np.where(results==-1)\n",
        "print(f'outlier index={outlier_idx}')\n",
        "print(f'outlier results={results[outlier_idx]}')\n",
        "print(f'outlier negative_outlier_factor_={clf.negative_outlier_factor_[outlier_idx]}')\n",
        "print(f'first 10 negative_outlier_factor_={clf.negative_outlier_factor_[:10]}')"
      ],
      "metadata": {
        "id": "kg2eQRusmNRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot()"
      ],
      "metadata": {
        "id": "H-oMXChFoxIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "outliers as indicated by out of range of mean by 2*std:\\\n",
        "Conclusion: outlier with shared columns is:\\\n",
        "161 by 'C, D' \\\n",
        "269 by 'C, G' \\\n",
        "\n",
        "C [50, 77, 161, 170, 265, 269]\\\n",
        "D [24, 63, 100, 104, 122, 133, 151, 154, 159, 161, 164, 169, 187, 202, 228, 252]\\\n",
        "E [11, 25, 135, 141, 166, 283, 287]\\\n",
        "F [13, 54, 56, 147, 152, 154, 155, 163, 176]\\\n",
        "G [94, 269]\\\n",
        "\n"
      ],
      "metadata": {
        "id": "vVMf204HnSTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "n_neighbors = 2\n",
        "clf = LocalOutlierFactor(n_neighbors=n_neighbors)\n",
        "#results = clf.fit_predict(df[['G']])\n",
        "results = clf.fit_predict(df)\n",
        "\n",
        "outlier_idx = np.where(results==-1)\n",
        "print(f'outlier index={outlier_idx}')\n",
        "print(f'outlier results={results[outlier_idx]}')\n",
        "print(f'outlier negative_outlier_factor_={clf.negative_outlier_factor_[outlier_idx]}')\n",
        "print(f'first 10 negative_outlier_factor_={clf.negative_outlier_factor_[:10]}')"
      ],
      "metadata": {
        "id": "O-zaILWG2NRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://scikit-learn.org/stable/auto_examples/neighbors/plot_lof_outlier_detection.html#sphx-glr-auto-examples-neighbors-plot-lof-outlier-detection-py\n",
        "\n",
        "Outlier detection with Local Outlier Factor (LOF)\n"
      ],
      "metadata": {
        "id": "DVmYbnlqPDS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X_inliers = 0.3 * np.random.randn(100, 2)\n",
        "X_inliers = np.r_[X_inliers + 2, X_inliers - 2]\n",
        "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
        "X = np.r_[X_inliers, X_outliers]\n",
        "\n",
        "n_outliers = len(X_outliers)\n",
        "ground_truth = np.ones(len(X), dtype=int)\n",
        "ground_truth[-n_outliers:] = -1"
      ],
      "metadata": {
        "id": "EqeZWZAWPETq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_inliers.shape, np.random.randn(100, 2).shape, X_outliers.shape, n_outliers, X.shape"
      ],
      "metadata": {
        "id": "u-OwZoqePaj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbYzTOmxPaUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
        "y_pred = clf.fit_predict(X)\n",
        "n_errors = (y_pred != ground_truth).sum()\n",
        "X_scores = clf.negative_outlier_factor_"
      ],
      "metadata": {
        "id": "tlygJ2moPN77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.legend_handler import HandlerPathCollection\n",
        "\n",
        "\n",
        "def update_legend_marker_size(handle, orig):\n",
        "    \"Customize size of the legend marker\"\n",
        "    handle.update_from(orig)\n",
        "    handle.set_sizes([20])\n",
        "\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], color=\"k\", s=3.0, label=\"Data points\")\n",
        "# plot circles with radius proportional to the outlier scores\n",
        "radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\n",
        "scatter = plt.scatter(\n",
        "    X[:, 0],\n",
        "    X[:, 1],\n",
        "    s=1000 * radius,\n",
        "    edgecolors=\"r\",\n",
        "    facecolors=\"none\",\n",
        "    label=\"Outlier scores\",\n",
        ")\n",
        "plt.axis(\"tight\")\n",
        "plt.xlim((-5, 5))\n",
        "plt.ylim((-5, 5))\n",
        "plt.xlabel(\"prediction errors: %d\" % (n_errors))\n",
        "plt.legend(\n",
        "    handler_map={scatter: HandlerPathCollection(update_func=update_legend_marker_size)}\n",
        ")\n",
        "plt.title(\"Local Outlier Factor (LOF)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jgldZkclPQUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple Anomaly Detection using Unsupervised KNN\n",
        "\n",
        "Ref: https://www.kaggle.com/code/kimchanyoung/simple-anomaly-detection-using-unsupervised-knn\n",
        "\n",
        "KNN is a supervised learning-based algorithm.\n",
        "However, using KNN's distance calculation method can also be used as an unsupervised learning method.\n",
        "\n",
        "In this work, we will use Scikit-Learn's NearestNeighbors, which we can use it for unsupervised learning\n",
        "\n",
        "sklearn.neighbors.NearestNeighbors\n",
        "class sklearn.neighbors.NearestNeighbors(*, n_neighbors=5, radius=1.0, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, n_jobs=None)\n",
        "n_neighbors : int, default=5 (Number of neighbors to use by default for kneighbors queries.)\n",
        "radius : float, default=1.0 (Range of parameter space to use by default for radius_neighbors queries.)\n",
        "algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’ (Algorithm used to compute the nearest neighbors)\n",
        "metric : str or callable, default=’minkowski’ (the distance metric to use for the tree.)\n",
        "p : int, default=2 (Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HEUu5NZzxWLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "metadata": {
        "id": "7yP-lM5AxW0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "n_neighbors = 50\n",
        "nbrs = NearestNeighbors(n_neighbors = n_neighbors)\n",
        "# fit model\n",
        "nbrs.fit(X)"
      ],
      "metadata": {
        "id": "nMLmoVmayQf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distances and indexes of k-neaighbors from model outputs\n",
        "distances, indexes = nbrs.kneighbors(X)\n",
        "print(f'distances={distances.shape}, indexes={indexes.shape}')\n",
        "distances = pd.DataFrame(distances)\n",
        "distances_mean = distances.mean(axis =1)\n",
        "\n",
        "print(f'distances={distances.shape}, distances_mean={distances_mean.shape}')\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(distances_mean)\n",
        "plt.grid()\n",
        "plt.title('mean distances')"
      ],
      "metadata": {
        "id": "zP-kVsmkyWBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Since 75th percentile is 0.79, we will set threshold into 1.0\n",
        "#Since max is 1.25, we will set threshold into 1.0\n",
        "#Set thresholds with reference to statistics.\n",
        "tmp = distances_mean.describe()\n",
        "for th in np.linspace(tmp['75%'], tmp['max']*0.95, num=5):\n",
        "  outlier_index = np.where(distances_mean > th)\n",
        "  print(th, len(list(outlier_index[0])), outlier_index)\n"
      ],
      "metadata": {
        "id": "de2UFlpOyVvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outlier_values = df[outlier_index]\n",
        "#outlier_values"
      ],
      "metadata": {
        "id": "iBo4gKE1zn_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1c3DEHBm7VRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, len(chars_list), figsize=(15, 6), facecolor='w', edgecolor='k')\n",
        "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
        "axs = axs.ravel()\n",
        "for i in range(0, len(chars_list)):\n",
        "    axs[i].plot(df[:, i], color = \"b\", marker=\"+\")\n",
        "    axs[i].plot(outlier_values[i], color='r', marker=\"o\") #s=80, facecolors='none', edgecolors='r'\n",
        "    axs[i].set_title(chars_list[i])"
      ],
      "metadata": {
        "id": "0ywwIRDp2Vei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, len(chars_list)-1, figsize=(15, 6), facecolor='w', edgecolor='k')\n",
        "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
        "axs = axs.ravel()\n",
        "for i in range(1, len(chars_list)):\n",
        "    axs[i-1].scatter(df[\"C\"], df[chars_list[i]], color = \"b\", marker=\"+\")\n",
        "    axs[i-1].scatter(outlier_values[\"C\"], outlier_values[chars_list[i]], marker=\"o\", s=80, facecolors='none', edgecolors='r')\n",
        "    axs[i-1].set_title(chars_list[i])"
      ],
      "metadata": {
        "id": "JgE12m533fe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion of outlier with single feature separately:\n",
        "269 by 'C, G' \\\n",
        "161 by 'C, D' \\"
      ],
      "metadata": {
        "id": "KpXqjv_5zU5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Isolation Forest\n",
        "One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The ensemble.IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n"
      ],
      "metadata": {
        "id": "AlZYa5Sv7wBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#max_features int or float, default=1.0\n",
        "The number of features to draw from X to train each base estimator.\n",
        "\n",
        "If int, then draw max_features features.\n",
        "\n",
        "If float, then draw max(1, int(max_features * n_features_in_)) features.\n",
        "\n",
        "Note: using a float number less than 1.0 or integer less than number of features will enable feature subsampling and leads to a longer runtime.\n"
      ],
      "metadata": {
        "id": "Ws_G0lFS8LFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "clf = IsolationForest(max_samples=100, random_state=0) #len(chars_list))\n",
        "clf.fit(X)"
      ],
      "metadata": {
        "id": "Y8ZbeQJU7vfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict if a particular sample is an outlier or not.\n",
        "results = clf.predict(X)\n",
        "outlier_idx = np.where(results==-1)\n",
        "print(f'num of outliers = {len(outlier_idx[0])}, outlier_idx={outlier_idx}')"
      ],
      "metadata": {
        "id": "gsfM5PGW_UBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.shape, len(outlier_idx[0])"
      ],
      "metadata": {
        "id": "ZiT7Xm4TBk89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Opposite of the anomaly score defined in the original paper.\n",
        "results = clf.score_samples(X)\n",
        "plt.plot(range(300), results)\n",
        "plt.plot(outlier_idx[0], results[outlier_idx[0]], 'r+')"
      ],
      "metadata": {
        "id": "Ci6A06x8A7zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(results[outlier_idx])\n"
      ],
      "metadata": {
        "id": "lm7ERkCEBK5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = IsolationForest(max_samples=100, random_state=0, max_features=2) #len(chars_list))\n",
        "clf.fit(df[['C', 'D']])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "disp = DecisionBoundaryDisplay.from_estimator(\n",
        "    clf,\n",
        "    df[['C', 'D']],\n",
        "    response_method=\"predict\",\n",
        "    alpha=0.5,\n",
        ")\n",
        "disp.ax_.scatter(df['C'], df['D'], s=20, edgecolor=\"k\")\n",
        "disp.ax_.set_title(\"Binary decision boundary \\nof IsolationForest\")\n",
        "plt.axis(\"square\")\n",
        "plt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2CNGTw0h8KuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnM7HEHh7Ws3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kqnHzX8m7XML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ordinal encoding DNA sequence data\n",
        "\n",
        "In this approach, we need to encode each nitrogen base as an ordinal value. For example “ATGC” becomes [0.25, 0.5, 0.75, 1.0]. Any other base such as “N” can be a 0.\n",
        "\n",
        "\n",
        "So let us create functions such as creating a NumPy array object from a sequence string, and a label encoder with the DNA sequence alphabet “a”, “c”, “g” and “t”, but also a character for anything else, “n”."
      ],
      "metadata": {
        "id": "h1HSDdPNDGDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One-hot encoding DNA Sequence\n",
        "\n",
        "Another approach is to use one-hot encoding to represent the DNA sequence. This is widely used in deep learning methods and lends itself well to algorithms like convolutional neural networks. In this example, “ATGC” would become [0,0,0,1], [0,0,1,0], [0,1,0,0], [1,0,0,0]. And these one-hot encoded vectors can either be concatenated or turned into 2-dimensional arrays.\n"
      ],
      "metadata": {
        "id": "1K3DHprVX3i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(np.array(['a','c','g','t','z']))\n",
        "\n",
        "def one_hot_encoder(seq_string):\n",
        "  seq_string = string_to_array(seq_string)\n",
        "  int_encoded = label_encoder.transform(seq_string)\n",
        "  print('my int_encoded', int_encoded)\n",
        "\n",
        "  onehot_encoder = OneHotEncoder(sparse=False, dtype=int)\n",
        "  int_encoded = int_encoded.reshape(len(int_encoded), 1)\n",
        "  onehot_encoded = onehot_encoder.fit_transform(int_encoded)\n",
        "  onehot_encoded = np.delete(onehot_encoded, -1, 1)\n",
        "  return onehot_encoded\n",
        "\n",
        "int_encoded = label_encoder.transform(string_to_array('ACGTZnv'))\n",
        "onehot_encoder = OneHotEncoder(sparse=False, dtype=int)\n",
        "print('int_encoded', int_encoded)\n",
        "int_encoded = int_encoded.reshape(len(int_encoded), 1)\n",
        "onehot_encoder.fit(int_encoded)\n",
        "print('onehot_encoder ', onehot_encoder.categories_)\n",
        "onehot_encoder.transform(int_encoded)"
      ],
      "metadata": {
        "id": "fMcist9GXILG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlWKZRu2NIa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "VALIDATE_SIZE = 0.2\n",
        "RANDOM_SEED = 123\n",
        "# train // validate - no labels since they're all clean anyway\n",
        "X_train, X_test = train_test_split(X,\n",
        "                                   test_size=VALIDATE_SIZE,\n",
        "                                   random_state=RANDOM_SEED)\n",
        "\n",
        "# manually splitting the labels from the test df\n",
        "#X_test, y_test = X_test.drop('label', axis=1).values, X_test.label.values"
      ],
      "metadata": {
        "id": "G1SDGykbNITr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "jE5-KuC8NiiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Woq0AmaMOOrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RqgirXmmOR_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ge5-7UPFOlbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0].toarray()[:2], X_train_transformed)\n"
      ],
      "metadata": {
        "id": "BqaWY8gPObn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.shape, type(X_train), X_train.toarray().shape\n",
        "X_train.toarray()[:2,:3]"
      ],
      "metadata": {
        "id": "skbGjq36PpkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "#g = sns.PairGrid(X_train.iloc[:,:3].sample(600, random_state=RANDOM_SEED))\n",
        "g = sns.PairGrid(X_train.toarray()[:,:3]) #.sample(600, random_state=RANDOM_SEED))\n",
        "plt.subplots_adjust(top=0.9)\n",
        "g.fig.suptitle('Before:')\n",
        "g.map_diag(sns.kdeplot)\n",
        "g.map_offdiag(sns.kdeplot);"
      ],
      "metadata": {
        "id": "Epue-Bl8POgk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}